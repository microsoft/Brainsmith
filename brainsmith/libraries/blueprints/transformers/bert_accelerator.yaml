# BERT Transformer Accelerator Blueprint
# Modern BrainSmith blueprint for BERT transformer models with attention mechanisms

name: "bert_accelerator"
description: "BERT transformer accelerator with optimized folding for attention mechanisms"
category: "transformers"
version: "1.0"

# Base configuration
model_type: "bert"
target_platform: "zynq_ultrascale"
optimization_level: "advanced"

# Configurable parameters for design space exploration
parameters:
  # BERT model architecture parameters
  bert_config:
    description: "BERT model architecture configuration"
    hidden_size:
      range: [256, 384, 512, 768]
      default: 384
      description: "Hidden layer dimension"
    num_layers:
      range: [1, 3, 6, 12]
      default: 3
      description: "Number of transformer layers"
    num_attention_heads:
      range: [8, 12, 16]
      default: 12
      description: "Number of attention heads"
    intermediate_size:
      range: [512, 1536, 3072]
      default: 1536
      description: "Feed-forward intermediate size"
    sequence_length:
      range: [64, 128, 256, 512]
      default: 128
      description: "Maximum sequence length"
    
  # FINN folding configuration derived from JSON analysis
  folding_factors:
    description: "FINN folding parameters for BERT operations"
    # Main MVAU operations (6 per layer)
    mvau_pe:
      range: [4, 8, 16, 32]
      default: 8
      description: "Processing elements for MVAU layers"
    mvau_simd:
      range: [12, 24, 48]
      default: 12
      description: "SIMD width for MVAU layers"
    
    # Feed-forward layer MVAUs (layers 4-5 get 2x parallelism)
    ff_pe_multiplier:
      range: [1, 2]
      default: 2
      description: "PE multiplier for feed-forward layers"
    ff_simd_multiplier:
      range: [1, 2]
      default: 2
      description: "SIMD multiplier for feed-forward layers"
    
    # Auxiliary operations
    aux_pe:
      range: [1, 2, 4]
      default: 1
      description: "PE for auxiliary operations (DuplicateStreams, Add, Mul)"
    aux_simd:
      range: [1, 2, 4]
      default: 1
      description: "SIMD for auxiliary operations (Shuffle, Softmax, LayerNorm)"
    
    # DynMVU configuration
    dynmvu_pe:
      range: [4, 8, 16]
      default: 8
      description: "PE for dynamic matrix-vector units"
    dynmvu_simd_divisor:
      range: [3, 4]
      default: 4
      description: "SIMD divisor for DynMVU (SIMD = mvau_simd / divisor)"
  
  # Memory and resource configuration
  memory_config:
    description: "Memory architecture settings"
    mvau_mem_mode:
      values: ["internal_default", "internal_decoupled", "external"]
      default: "internal_decoupled"
      description: "Memory mode for MVAU operations"
    dynmvu_mem_mode:
      values: ["external"]
      default: "external"
      description: "Memory mode for DynMVU operations"
    ram_style:
      values: ["auto", "block", "distributed"]
      default: "auto"
      description: "RAM implementation style"
    runtime_writeable_weights:
      values: [0, 1]
      default: 0
      description: "Enable runtime weight updates"
  
  # Quantization settings
  quantization:
    description: "Quantization configuration"
    weight_width:
      range: [4, 8]
      default: 8
      description: "Weight quantization bit width"
    activation_width:
      range: [4, 8]
      default: 8
      description: "Activation quantization bit width"
  
  # Performance optimization targets
  optimization:
    description: "Performance optimization settings"
    target_fps:
      range: [1000, 3000, 5000]
      default: 3000
      description: "Target throughput in inferences per second"
    clock_period_ns:
      range: [3.33, 5.0, 10.0]
      default: 5.0
      description: "Target clock period in nanoseconds"

# Fixed platform configuration
fpga_part: "xczu7ev-ffvc1156-2-e"
board: "V80"
enable_zynq_ps: true
verification_steps: ["cppsim", "rtlsim"]

# Expected model characteristics
expected_model:
  input_shape: [1, "sequence_length"]
  architecture: "bert_base"
  layers:
    - type: "embedding"
      name: "embeddings"
      description: "Token and position embeddings"
    - type: "transformer_block"
      name: "encoder_layers"
      count: "num_layers"
      components:
        - "multi_head_attention"
        - "feed_forward"
        - "layer_norm"
        - "residual_connections"
    - type: "pooler"
      name: "pooler"
      description: "Sequence pooling layer"

# Performance targets
targets:
  throughput:
    direction: "maximize"
    target: "target_fps"
    description: "Maximize inference throughput"
  power_budget_w: 15.0
  accuracy_drop_max: 0.05
  latency_ms_max: 20.0

# Optimization constraints
constraints:
  min_throughput_fps: 500
  max_power_w: 25.0
  max_latency_ms: 50.0
  resource_utilization_max:
    lut: 0.8
    dsp: 0.7
    bram: 0.6
    uram: 0.5

# FINN-specific folding mapping (derived from gen_initial_folding.py logic)
folding_mapping:
  description: "Automatic folding configuration generation based on BERT layer structure"
  mvau_pattern:
    per_layer: 6
    indices: [0, 1, 2, 3, 4, 5]
    special_layers: [4, 5]  # Feed-forward layers get 2x parallelism
  auxiliary_patterns:
    duplicate_streams: 3
    shuffle: 4
    thresholding: 9
    dynmvu: 2
    elementwise_add: 2
    elementwise_mul: 5
    softmax: 1
    layer_norm: 2