version: "3.0"
name: "Explicit Backends Test Blueprint"
description: "Tests explicit backend specification and validation"

hw_compiler:
  kernels:
    # Explicit backend specifications
    - ["MatMul", ["MatMulRTL", "MatMulHLS"]]
    - ["LayerNorm", ["LayerNormHLS"]]
    - ["Softmax", ["SoftmaxRTL", "SoftmaxHLS", "SoftmaxDSP"]]
    
    # Optional with explicit backends
    - ["~Attention", ["AttentionCUDA", "AttentionTriton"]]
    
    # Mutually exclusive group with explicit backends
    - [
        ["Conv2D", ["Conv2DRTL", "Conv2DHLS"]],
        ["DepthwiseConv2D", ["DepthwiseConv2DDSP"]],
        ["GroupedConv2D", ["GroupedConv2DHLS"]],
        ~  # Skip option
      ]
  
  transforms:
    pre_proc:
      - "PrepareModel"
    cleanup:
      - "RemoveIdentityOps"
      - "GiveReadableTensorNames"
    topology_opt:
      - "ConvertQONNXtoFINN"
      - "InferDataLayouts"
    kernel_opt:
      - "OptimizeKernelConfig"
    dataflow_opt:
      - "OptimizeDataflow"
    post_proc:
      - "FinalizeModel"
  
  build_steps:
    - "ConvertToHW"
    - "PrepareIP"
    - "GenerateDriver"
    - "PackageDesign"

processing:
  preprocessing:
    - name: "quantization"
      options:
        - {enabled: true, bits: 8}
        - {enabled: true, bits: 4}
        - {enabled: false}

search:
  strategy: "bayesian"
  constraints:
    - metric: "lut_utilization"
      operator: "<="
      value: 0.85
    - metric: "bram_utilization"
      operator: "<="
      value: 0.9
    - metric: "latency_cycles"
      operator: "<="
      value: 1000
  max_evaluations: 50
  timeout_minutes: 120
  parallel_builds: 4

global:
  output_stage: "ip"
  working_directory: "./explicit_backend_builds"
  cache_results: true
  save_artifacts: true
  log_level: "DEBUG"