{#-
AutoHWCustomOp Subclass Template

This template generates AutoHWCustomOp subclasses that:
1. Create simplified KernelDefinition with interface definitions
2. Generate explicit get_nodeattr_types() method for FINN integration
3. Inherit all FINN method implementations from AutoHWCustomOp
4. Only implement kernel-specific runtime extraction methods

Key features:
- Explicit, human-readable node attribute definitions
- No runtime CodegenBinding dependencies
- Clean separation of concerns
- Compile-time parameter binding resolution
-#}
############################################################################
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT License.
#
# Auto-generated HWCustomOp for {{ kernel_name }}
# Generated from: {{ source_file }}
# Generation timestamp: {{ generation_timestamp }}
#
# This HWCustomOp uses the modern AutoHWCustomOp base class with explicit
# parameter definitions and no runtime CodegenBinding dependencies.
############################################################################

from typing import List, Dict, Tuple, Any
import numpy as np
from qonnx.core.datatype import DataType

from brainsmith.core.finn.auto_hw_custom_op import AutoHWCustomOp
from brainsmith.core.dataflow import (
    KernelDefinition,
    InputDefinition,
    OutputDefinition,
    RelationType
)
from brainsmith.core.dataflow.qonnx_types import DatatypeConstraintGroup


class {{ class_name }}(AutoHWCustomOp):
    """
    Auto-generated HWCustomOp for {{ kernel_name }} kernel.
    
    Generated from RTL: {{ source_file }}
    Uses AutoHWCustomOp for automatic FINN method implementation.
    """
    
    def __init__(self, onnx_node, **kwargs):
        """Initialize {{ class_name }} with KernelDefinition."""
        kernel_def = self._create_kernel_definition()
        super().__init__(onnx_node, kernel_def, **kwargs)
        
        # Set kernel-specific attributes
        self.kernel_name = "{{ kernel_name }}"
        self.rtl_source = "{{ source_file }}"
    
    def get_nodeattr_types(self):
        """Define interface datatypes and BDIM/SDIM parameters from SHAPE pragmas."""
        # Get parent attributes first (includes exec_mode, backend, etc.)
        attrs = super().get_nodeattr_types()
        
        # Add kernel-specific attributes
        attrs.update({
            # Interface datatype attributes (required by FINN)
            {% for attr in explicit_datatype_attrs %}
            "{{ attr.name }}": {{ attr.attr_spec | replace('null', 'None') | replace('true', 'True') | replace('false', 'False') }},
            {% endfor %}
            
            # BDIM/SDIM parameters from SHAPE pragmas
            {% for param in shape_nodeattrs %}
            "{{ param.name }}": ('i', True, 0),  # {{ param.source_comment }}
            {% endfor %}
            
            {% if has_weights %}
            "ram_style": ('s', False, 'auto', {'auto', 'block', 'distributed', 'ultra'}),
            {% endif %}
        })
        
        return attrs
    
    def _create_kernel_definition(self) -> KernelDefinition:
        """Create simplified KernelDefinition with interface definitions only."""
        kernel_def = KernelDefinition(name="{{ kernel_name }}")
        
        # Add input definitions
        {% for interface in input_interfaces %}
        input_def = InputDefinition(
            name="{{ interface.name }}",
            datatype_constraints=[
                {% for constraint in interface.datatype_constraints %}
                DatatypeConstraintGroup(
                    base_type="{{ constraint.base_type }}",
                    min_width={{ constraint.min_width }},
                    max_width={{ constraint.max_width }}
                ),
                {% endfor %}
            ],
            {% if interface.bdim_shape %}
            block_tiling={{ interface.bdim_shape | tojson }},
            {% endif %}
            {% if interface.sdim_shape %}
            stream_tiling={{ interface.sdim_shape | tojson }}
            {% endif %}
        )
        kernel_def.add_input(input_def)
        {% endfor %}
        
        # Add weight input definitions
        {% for interface in weight_interfaces %}
        weight_def = InputDefinition(
            name="{{ interface.name }}",
            datatype_constraints=[
                {% for constraint in interface.datatype_constraints %}
                DatatypeConstraintGroup(
                    base_type="{{ constraint.base_type }}",
                    min_width={{ constraint.min_width }},
                    max_width={{ constraint.max_width }}
                ),
                {% endfor %}
            ],
            {% if interface.bdim_shape %}
            block_tiling={{ interface.bdim_shape | tojson }},
            {% endif %}
            {% if interface.sdim_shape %}
            stream_tiling={{ interface.sdim_shape | tojson }},
            {% endif %}
            is_weight=True
        )
        kernel_def.add_input(weight_def)
        {% endfor %}
        
        # Add output definitions
        {% for interface in output_interfaces %}
        output_def = OutputDefinition(
            name="{{ interface.name }}",
            datatype_constraints=[
                {% for constraint in interface.datatype_constraints %}
                DatatypeConstraintGroup(
                    base_type="{{ constraint.base_type }}",
                    min_width={{ constraint.min_width }},
                    max_width={{ constraint.max_width }}
                ),
                {% endfor %}
            ],
            {% if interface.bdim_shape %}
            block_tiling={{ interface.bdim_shape | tojson }}
            {% endif %}
        )
        kernel_def.add_output(output_def)
        {% endfor %}
        
        # Add relationships
        {% for rel in relationships %}
        kernel_def.add_relationship(
            source_name="{{ rel.source_interface }}",
            target_name="{{ rel.target_interface }}",
            relationship_type=RelationType.{{ rel.relation.name }},
            {% if rel.source_dim is not none %}source_dim={{ rel.source_dim }},{% endif %}
            {% if rel.target_dim is not none %}target_dim={{ rel.target_dim }},{% endif %}
            {% if rel.factor is not none %}factor={{ rel.factor }},{% endif %}
            {% if rel.dependency_type %}dependency_type="{{ rel.dependency_type }}"{% endif %}
        )
        {% endfor %}
        return kernel_def
    
    {% if verification_required %}
    def verify_node(self) -> List[str]:
        """Verify kernel-specific constraints."""
        messages = super().verify_node()
        
        # Verify all required parameters are present
        {% for param in required_attributes %}
        if self.get_nodeattr("{{ param }}") is None:
            messages.append(f"âœ— Required parameter '{{ param }}' not specified")
        {% endfor %}
        
        # Additional {{ kernel_name }}-specific verification
        # TODO: Add kernel-specific constraint checks
        
        return messages
    {% endif %}
    
    def execute_node(self, context, graph):
        """
        Execute the hardware kernel in simulation.
        
        TODO: Implement this method for your specific kernel.
        This should handle both 'cppsim' and 'rtlsim' execution modes.
        
        For reference implementation, see:
        deps/finn/src/finn/custom_op/fpgadataflow/rtl/thresholding_rtl.py
        """
        raise NotImplementedError(
            f"execute_node() not implemented for {self.__class__.__name__}. "
            "Please implement this method to support simulation."
        )
    
    def bram_estimation(self):
        """
        Estimate BRAM usage for this kernel.
        
        TODO: Implement based on your kernel's memory requirements.
        Return the number of BRAM blocks needed.
        
        For kernels without memory requirements, return 0.
        For kernels with weights/parameters, calculate based on:
        - Weight tensor dimensions
        - Parallelism factors (PE)
        - Memory packing efficiency
        """
        raise NotImplementedError(
            f"bram_estimation() not implemented for {self.__class__.__name__}. "
            "Please implement this method to provide resource estimates."
        )
    
    def uram_estimation(self):
        """
        Estimate URAM usage for this kernel.
        
        TODO: Implement based on your kernel's memory requirements.
        Return the number of URAM blocks needed.
        
        For kernels without memory requirements, return 0.
        For kernels with large weight tensors, consider URAM usage.
        """
        raise NotImplementedError(
            f"uram_estimation() not implemented for {self.__class__.__name__}. "
            "Please implement this method to provide resource estimates."
        )
    
    def lut_estimation(self):
        """
        Estimate LUT usage for this kernel.
        
        TODO: Implement based on your kernel's logic requirements.
        Return the number of LUTs needed.
        
        Consider:
        - Computational complexity
        - Data path width
        - Control logic overhead
        """
        raise NotImplementedError(
            f"lut_estimation() not implemented for {self.__class__.__name__}. "
            "Please implement this method to provide resource estimates."
        )


# Convenience function for FINN integration
def make_{{ kernel_name }}_node(inputs, outputs, **node_attrs):
    """
    Create {{ class_name }} ONNX node.
    
    Interface datatype attributes (required):
    {% for interface_attr in explicit_datatype_attrs %}
    - {{ interface_attr.name }}: str (required)
    {% endfor %}
    
    BDIM/SDIM parameters from SHAPE pragmas:
    {% for param in shape_nodeattrs %}
    - {{ param.name }}: int (required)  # {{ param.source_comment }}
    {% endfor %}
    
    {% if has_weights %}
    Hardware optimization hints:
    - ram_style: str = 'auto'
    {% endif %}
    """
    import onnx.helper
    
    # Verify required interface datatypes are specified
    {% for interface_attr in explicit_datatype_attrs %}
    if "{{ interface_attr.name }}" not in node_attrs:
        raise ValueError("Required attribute '{{ interface_attr.name }}' not specified")
    {% endfor %}
    
    return onnx.helper.make_node(
        "{{ class_name }}",
        inputs=inputs,
        outputs=outputs,
        domain="finn.custom_op.fpgadataflow",
        **node_attrs
    )