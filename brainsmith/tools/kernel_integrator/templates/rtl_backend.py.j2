{#-
RTL Backend Template with Explicit Parameter Resolution

Generates minimal RTL Backend subclasses with explicit parameter
assignments generated at compile-time from CodegenBinding.
-#}
# Auto-generated by Brainsmith Kernel Integrator
# Generated from: {{ source_file }}
# Date: {{ generation_timestamp }}

from brainsmith.core.finn.auto_rtl_backend import AutoRTLBackend
from {{ kernel_name }}_hw_custom_op import {{ class_name }}
from qonnx.core.datatype import DataType


class {{ kernel_name }}_rtl({{ class_name }}, AutoRTLBackend):
    """RTL backend for {{ kernel_name }} operation.
    
    Auto-generated from SystemVerilog RTL analysis.
    Uses explicit parameter resolution generated at compile-time.
    {% if operation_description %}
    
    {{ operation_description }}
    {% endif %}
    """
    
    def __init__(self, onnx_node, **kwargs):
        super().__init__(onnx_node, **kwargs)
    
    @property
    def finn_rtllib_module(self) -> str:
        """Return finn-rtllib module name for this operation."""
        return "{{ finn_rtllib_module }}"
    
    def get_nodeattr_types(self):
        """Define all node attributes including RTL-specific parameters."""
        # Get interface datatype attributes from HWCustomOp parent
        my_attrs = super().get_nodeattr_types()
        
        # Add RTL-specific algorithm parameters
        rtl_attrs = {
            {% for param_name, attr_spec in rtl_specific_nodeattrs.items() %}
            "{{ param_name }}": {{ attr_spec | replace('null', 'None') | replace('true', 'True') | replace('false', 'False') }},
            {% endfor %}
        }
        my_attrs.update(rtl_attrs)
        
        # Add HDL generation attributes
        my_attrs.update({
            "gen_top_module": ("s", False, ""),
            "ipgen_path": ("s", False, ""),
            "ip_path": ("s", False, ""),
        })
        
        return my_attrs
    
    def prepare_codegen_rtl_values(self, model):
        """Explicit parameter resolution for RTL template generation."""
        code_gen_dict = {}
        
        # Basic module information
        code_gen_dict["$MODULE_NAME_AXI_WRAPPER$"] = [self.get_verilog_top_module_name()]
        code_gen_dict["$TOP_MODULE$"] = code_gen_dict["$MODULE_NAME_AXI_WRAPPER$"]
        
        # Standard stream width variables
        code_gen_dict["$IBITS$"] = [str(self.get_instream_width())]
        code_gen_dict["$OBITS$"] = [str(self.get_outstream_width())]
        
        # Explicit parameter assignments (generated from CodegenBinding at compile-time)
        {% for assignment in explicit_parameter_assignments %}
        code_gen_dict["{{ assignment.template_var }}"] = [{{ assignment.assignment }}]  # {{ assignment.comment }}
        {% endfor %}
        
        return code_gen_dict
    
    def get_supporting_rtl_files(self) -> list:
        """Get list of supporting RTL files to include."""
        return [
            {% for file_name in supporting_rtl_files %}
            "{{ file_name }}",
            {% endfor %}
        ]
    
    def generate_hdl(self, model, fpgapart, clk):
        """Generate HDL from pre-generated wrapper template."""
        import os
        import shutil
        
        # Get code generation directory
        code_gen_dir = self.get_nodeattr("code_gen_dir_ipgen")
        os.makedirs(code_gen_dir, exist_ok=True)
        
        # Save top module name
        topname = self.get_verilog_top_module_name()
        self.set_nodeattr("gen_top_module", topname)
        
        # Get template variables
        code_gen_dict = self.prepare_codegen_rtl_values(model)
        
        # Add missing stream width variables
        code_gen_dict["$INPUT_STREAM_WIDTH$"] = [str(self.get_instream_width())]
        code_gen_dict["$OUTPUT_STREAM_WIDTH$"] = [str(self.get_outstream_width())]
        
        # Find the pre-generated wrapper template
        module_dir = os.path.dirname(os.path.abspath(__file__))
        wrapper_name = "{{ kernel_name }}_wrapper.v"
        wrapper_path = os.path.join(module_dir, wrapper_name)
        
        if os.path.exists(wrapper_path):
            # Read wrapper template
            with open(wrapper_path, "r") as f:
                template_content = f.read()
            
            # Apply template substitution
            for placeholder, values in code_gen_dict.items():
                value = values[0] if isinstance(values, list) and values else str(values)
                template_content = template_content.replace(placeholder, value)
            
            # Write processed wrapper
            output_path = os.path.join(code_gen_dir, f"{topname}.v")
            with open(output_path, "w") as f:
                f.write(template_content)
        else:
            raise FileNotFoundError(
                f"Wrapper template not found at {wrapper_path}. "
                "Ensure the wrapper file is in the same directory as this RTL backend."
            )
        
        # Copy supporting RTL files if specified
        for rtl_file in self.get_supporting_rtl_files():
            if os.path.exists(rtl_file):
                shutil.copy(rtl_file, code_gen_dir)
            else:
                print(f"Warning: Supporting RTL file not found: {rtl_file}")
        
        # Set paths for downstream tools
        self.set_nodeattr("ipgen_path", code_gen_dir)
        self.set_nodeattr("ip_path", code_gen_dir)
    
    {% if has_weights %}
    def make_weight_file(self, weights, weight_file_mode, weight_file_name):
        """
        Generate weight initialization file for this kernel.
        
        TODO: Implement this method if your kernel uses weights/parameters.
        This should write weights in the format expected by your RTL.
        
        Args:
            weights: numpy array with weight values
            weight_file_mode: 'decoupled' or 'const' 
            weight_file_name: path where weight file should be written
            
        For reference implementation, see:
        deps/finn/src/finn/custom_op/fpgadataflow/rtl/thresholding_rtl.py
        """
        raise NotImplementedError(
            f"make_weight_file() not implemented for {self.__class__.__name__}. "
            "Please implement this method if your kernel uses weights."
        )
    {% endif %}