{#-
RTL Backend Template V2 - Direct KernelMetadata Access

Generates minimal RTL Backend subclasses with explicit parameter
assignments using direct metadata access.
-#}
# Auto-generated by Brainsmith Kernel Integrator
# Generated from: {{ kernel_metadata.source_file }}
# Date: {{ generation_timestamp }}

from brainsmith.core.finn.auto_rtl_backend import AutoRTLBackend
from {{ kernel_metadata.name }}_hw_custom_op import {{ kernel_metadata.class_name }}
from qonnx.core.datatype import DataType


class {{ kernel_metadata.name }}_rtl({{ kernel_metadata.class_name }}, AutoRTLBackend):
    """RTL backend for {{ kernel_metadata.name }} operation.
    
    Auto-generated from SystemVerilog RTL analysis.
    Uses explicit parameter resolution generated at compile-time.
    """
    
    def __init__(self, onnx_node, **kwargs):
        super().__init__(onnx_node, **kwargs)
    
    @property
    def finn_rtllib_module(self) -> str:
        """Return finn-rtllib module name for this operation."""
        return "{{ kernel_metadata.module_name }}"
    
    def get_nodeattr_types(self):
        """Define all node attributes including RTL-specific parameters."""
        # Get interface datatype attributes from HWCustomOp parent
        my_attrs = super().get_nodeattr_types()
        
        # Add RTL-specific algorithm parameters
        rtl_attrs = {
            {% for param in kernel_metadata.parameters %}
            {% if param.category.value == 'algorithm' and param.is_exposed %}
            {% if param.name.endswith('_PATH') %}
            "{{ param.name }}": ('s', False, ''),
            {% else %}
            "{{ param.name }}": ('i', True, {{ param.default_value or 0 }}),
            {% endif %}
            {% endif %}
            {% endfor %}
        }
        my_attrs.update(rtl_attrs)
        
        # Add HDL generation attributes
        my_attrs.update({
            "gen_top_module": ("s", False, ""),
            "gen_top_module_path": ("s", False, ""),
            {% if kernel_metadata.weight_interfaces %}
            "code_gen_dir_cppsim": ("s", False, ""),
            "executable_path": ("s", False, ""),
            {% endif %}
        })
        
        return my_attrs
    
    def get_template_params(self):
        """
        Generate template parameters for RTL wrapper generation.
        
        Uses explicit parameter mappings generated at compile-time
        from CodegenBinding analysis.
        """
        # Get base template parameters from parent
        code_gen_dict = super().get_template_params()
        
        # Explicit parameter assignments (generated from CodegenBinding at compile-time)
        {% for assignment in explicit_parameter_assignments %}
        code_gen_dict["{{ assignment.template_var }}"] = [{{ assignment.assignment }}]  # {{ assignment.comment }}
        {% endfor %}
        
        return code_gen_dict
    
    def get_supporting_rtl_files(self) -> list:
        """Get list of supporting RTL files to include."""
        return [
        ]
    
    def generate_hdl(self, model, fpgapart, clk):
        """Generate HDL from pre-generated wrapper template."""
        import os
        import shutil
        
        # Get paths from parent class
        code_gen_dir = self.get_nodeattr("code_gen_dir_ipgen")
        wrapper_filename = self.get_verilog_wrapper_filename()
        
        # Check if pre-generated wrapper exists
        if os.path.exists(wrapper_filename):
            # Use existing wrapper
            shutil.copy(wrapper_filename, code_gen_dir)
        else:
            # Generate using parent class method
            super().generate_hdl(model, fpgapart, clk)
    
    def get_verilog_wrapper_template_filename(self):
        """Return path to pre-generated Verilog wrapper template."""
        # This could be customized to point to generated wrappers
        return "{{ kernel_metadata.name }}_wrapper.v"
    
    {% if kernel_metadata.weight_interfaces %}
    def get_weight_storage_type(self):
        """Define weight storage preference for this kernel."""
        ram_style = self.get_nodeattr("ram_style")
        if ram_style == "block":
            return "BRAM"
        elif ram_style == "distributed":
            return "LUTRAM"
        else:
            return "AUTO"
    {% endif %}


# RTL Backend Reference
"""
{{ kernel_metadata.name }} RTL Backend Specification:

RTL Module: {{ kernel_metadata.module_name }}

RTL-Specific Parameters:
{% for param in kernel_metadata.parameters %}
{% if param.category.value == 'algorithm' and param.is_exposed %}
- {{ param.name }}: ('i', True, {{ param.default_value or 0 }})
{% endif %}
{% endfor %}
{% for param in kernel_metadata.parameters %}
{% if param.name.endswith('_PATH') and param.is_exposed %}
- {{ param.name }}: ('s', False, '')
{% endif %}
{% endfor %}

Code Generation:
- Uses explicit parameter assignments from compile-time analysis
- All BDIM/SDIM/DataType parameters resolved through KernelModel
- Algorithm parameters exposed as node attributes

"""