{#-
RTL Backend Template V2 - Direct KernelMetadata Access

Generates minimal RTL Backend subclasses with explicit parameter
assignments using direct metadata access.
-#}
# Auto-generated by Brainsmith Kernel Integrator
# Generated from: {{ source_file }}
# Date: {{ generation_timestamp }}

from brainsmith.core.finn.auto_rtl_backend import AutoRTLBackend
from {{ kernel_name }}_hw_custom_op import {{ class_name }}
from qonnx.core.datatype import DataType


class {{ kernel_name }}_rtl({{ class_name }}, AutoRTLBackend):
    """RTL backend for {{ kernel_name }} operation.
    
    Auto-generated from SystemVerilog RTL analysis.
    Uses explicit parameter resolution generated at compile-time.
    {% if operation_description %}
    
    {{ operation_description }}
    {% endif %}
    """
    
    def __init__(self, onnx_node, **kwargs):
        super().__init__(onnx_node, **kwargs)
    
    @property
    def finn_rtllib_module(self) -> str:
        """Return finn-rtllib module name for this operation."""
        return "{{ finn_rtllib_module }}"
    
    def get_nodeattr_types(self):
        """Define all node attributes including RTL-specific parameters."""
        # Get interface datatype attributes from HWCustomOp parent
        my_attrs = super().get_nodeattr_types()
        
        # Add RTL-specific algorithm parameters
        rtl_attrs = {
            {% for param_name, attr_spec in rtl_specific_nodeattrs.items() %}
            "{{ param_name }}": {{ attr_spec | tojson | replace('null', 'None') | replace('true', 'True') | replace('false', 'False') }},
            {% endfor %}
        }
        my_attrs.update(rtl_attrs)
        
        # Add HDL generation attributes
        my_attrs.update({
            "gen_top_module": ("s", False, ""),
            "gen_top_module_path": ("s", False, ""),
            {% if weight_interfaces %}
            "code_gen_dir_cppsim": ("s", False, ""),
            "executable_path": ("s", False, ""),
            {% endif %}
        })
        
        return my_attrs
    
    def generate_hdl(self, model, fpgapart, clk):
        """Generate HDL implementation for {{ kernel_name }}."""
        # Base implementation provided by AutoRTLBackend
        return super().generate_hdl(model, fpgapart, clk)
    
    def get_template_params(self):
        """
        Generate template parameters for RTL wrapper generation.
        
        Uses explicit parameter mappings generated at compile-time
        from CodegenBinding analysis.
        """
        # Get base template parameters from parent
        code_gen_dict = super().get_template_params()
        
        # Explicit parameter assignments (generated from CodegenBinding at compile-time)
        {% for assignment in explicit_parameter_assignments %}
        code_gen_dict["{{ assignment.template_var }}"] = [{{ assignment.assignment }}]  # {{ assignment.comment }}
        {% endfor %}
        
        return code_gen_dict
    
    def get_supporting_rtl_files(self) -> list:
        """Get list of supporting RTL files to include."""
        return [
            {% for file_name in supporting_rtl_files %}
            "{{ file_name }}",
            {% endfor %}
        ]
    
    def generate_hdl(self, model, fpgapart, clk):
        """Generate HDL from pre-generated wrapper template."""
        import os
        import shutil
        
        # Get paths from parent class
        code_gen_dir = self.get_nodeattr("code_gen_dir_ipgen")
        wrapper_filename = self.get_verilog_wrapper_filename()
        
        # Check if pre-generated wrapper exists
        if os.path.exists(wrapper_filename):
            # Use existing wrapper
            shutil.copy(wrapper_filename, code_gen_dir)
        else:
            # Generate using parent class method
            super().generate_hdl(model, fpgapart, clk)
    
    def get_verilog_wrapper_template_filename(self):
        """Return path to pre-generated Verilog wrapper template."""
        # This could be customized to point to generated wrappers
        return "{{ kernel_name }}_wrapper.v"
    
    {% if weight_interfaces %}
    def get_weight_storage_type(self):
        """Define weight storage preference for this kernel."""
        ram_style = self.get_nodeattr("ram_style")
        if ram_style == "block":
            return "BRAM"
        elif ram_style == "distributed":
            return "LUTRAM"
        else:
            return "AUTO"
    {% endif %}


# RTL Backend Reference
"""
{{ kernel_name }} RTL Backend Specification:

RTL Module: {{ finn_rtllib_module }}

RTL-Specific Parameters:
{% for param_name, attr_spec in rtl_specific_nodeattrs.items() %}
- {{ param_name }}: {{ attr_spec }}
{% endfor %}

Code Generation:
- Uses explicit parameter assignments from compile-time analysis
- All BDIM/SDIM/DataType parameters resolved through KernelModel
- Algorithm parameters exposed as node attributes

{% if supporting_rtl_files %}
Supporting RTL Files:
{% for file in supporting_rtl_files %}
- {{ file }}
{% endfor %}
{% endif %}
"""