{#-
Infer Transformation Template

Generates a transformation for converting ONNX nodes to AutoHWCustomOp.
Follows the concise pattern of other kernel integrator templates.
-#}
############################################################################
# Auto-generated Infer transformation for {{ kernel_metadata.name }}
# Generated from: {{ kernel_metadata.source_file }}
############################################################################

from typing import Dict, Any, List, Optional
import numpy as np
from onnx import NodeProto
from qonnx.core.modelwrapper import ModelWrapper
from qonnx.core.datatype import DataType
from qonnx.util.basic import get_by_name
from qonnx.custom_op.registry import getCustomOp

from brainsmith.transforms.core.infer_auto_hw_custom_op import InferAutoHWCustomOp


class Infer{{ kernel_metadata.class_name }}(InferAutoHWCustomOp):
    """
    Convert ONNX nodes to {{ kernel_metadata.class_name }} AutoHWCustomOp.
    
    Generated from RTL: {{ kernel_metadata.source_file }}
    """
    
    def __init__(self, target_domain: str = "rtl"):
        """Initialize with target domain ('hls' or 'rtl')."""
        super().__init__(target_domain)
    
    def get_auto_hw_custom_op_name(self) -> str:
        """Return the AutoHWCustomOp class name."""
        return "{{ kernel_metadata.class_name }}"
    
    def get_domain_base(self) -> str:
        """Return domain base for the kernel."""
        # TODO: Update to match your package structure
        return "brainsmith.kernels.{{ kernel_metadata.file_name }}"
    
    # NOTE: You may want to override get_target_domain() to return just the base domain
    # without the .rtl/.hls suffix if using SpecializeLayers transformation:
    # def get_target_domain(self) -> str:
    #     """Override to return base domain without specialization suffix."""
    #     return self.get_domain_base()
    
    ############################################################################
    # ======================= MANUALLY IMPLEMENT FUNCTIONS BELOW ===============
    # Implement node matching, validation, and attribute extraction logic here
    ############################################################################
    
    def matches_node(self, model: ModelWrapper, node: NodeProto) -> bool:
        """
        Check if this node should be converted to {{ kernel_metadata.class_name }}.
        
        TODO: Implement ONNX node matching logic.
        Common patterns: op_type matching, attribute checking, custom op detection.
        """
        raise NotImplementedError(
            f"matches_node() not implemented for {self.__class__.__name__}. "
            "Implement to match ONNX nodes (e.g., return node.op_type == 'YourOpType')"
        )
    
    def can_convert_node(self, model: ModelWrapper, node: NodeProto) -> bool:
        """
        Additional validation before conversion.
        
        TODO: Add validation beyond basic matching if needed.
        Common checks: datatypes, tensor shapes, static weights, configurations.
        """
        # Check for integer datatypes on all inputs/outputs
        if not self.validate_integer_datatypes(model, node):
            return False
        
        {% if kernel_metadata.has_weights %}
        # Verify weight initializers exist
        weight_indices = []  # TODO: Set indices for weight inputs
        for idx in weight_indices:
            if idx < len(node.input) and model.get_initializer(node.input[idx]) is None:
                return False
        {% endif %}
        
        # TODO: Add additional validation as needed
        return True
    
    def extract_node_attributes(self, model: ModelWrapper, node: NodeProto) -> Dict[str, Any]:
        """
        Extract attributes from ONNX node for {{ kernel_metadata.class_name }}.
        
        TODO: Map ONNX attributes to kernel parameters.
        """
        attrs = {}
        
        # Extract algorithm parameters
        {% for param in kernel_metadata.parameters %}
        # {{ param.name }}{% if param.rtl_type %} ({{ param.rtl_type }}){% endif %}{% if param.kernel_value %} = {{ param.kernel_value }}{% endif %}
        {% if param.name == "DEPTH_TRIGGER_URAM" %}
        attrs["{{ param.nodeattr_name }}"] = 1024  # TODO: Adjust based on your memory requirements
        {% elif param.name == "DEPTH_TRIGGER_BRAM" %}
        attrs["{{ param.nodeattr_name }}"] = 256  # TODO: Adjust based on your memory requirements
        {% elif param.name == "DEEP_PIPELINE" %}
        attrs["{{ param.nodeattr_name }}"] = 1  # Enable deep pipelining by default
        {% elif param.rtl_type and "FPARG" in param.name %}
        attrs["{{ param.nodeattr_name }}"] = 0  # Default fractional bits for integer types
        {% elif param.rtl_type and param.rtl_type.startswith("bit") %}
        attrs["{{ param.nodeattr_name }}"] = 0  # Default for bit type
        {% elif param.rtl_type and ("int" in param.rtl_type or param.rtl_type == "integer") %}
        attrs["{{ param.nodeattr_name }}"] = 0  # Default for integer type
        {% elif param.rtl_type == "string" or param.name.endswith("_PATH") %}
        attrs["{{ param.nodeattr_name }}"] = ""  # Default for string/path type
        {% else %}
        attrs["{{ param.nodeattr_name }}"] = None  # TODO: Extract from node
        {% endif %}
        {% endfor %}
        
        {% set has_shape_params = namespace(value=false) %}
        {% for interface in kernel_metadata.stream_interfaces %}
        {% if interface.bdim_shape or interface.sdim_shape %}
        {% set has_shape_params.value = true %}
        {% endif %}
        {% endfor %}
        {% if has_shape_params.value %}
        # Shape parameters for tiling and parallelism
        {% for interface in kernel_metadata.stream_interfaces %}
        {% if interface.bdim_shape %}
        # {{ interface.name }} block dimensions: {{ interface.bdim_shape }}
        {% for shape_param in interface.bdim_shape %}
        attrs["{{ shape_param }}"] = 1  # TODO: Set based on requirements
        {% endfor %}
        {% endif %}
        {% if interface.sdim_shape %}
        # {{ interface.name }} stream dimensions: {{ interface.sdim_shape }}
        {% for shape_param in interface.sdim_shape %}
        attrs["{{ shape_param }}"] = 1  # TODO: Set parallelism factor
        {% endfor %}
        {% endif %}
        {% endfor %}
        {% endif %}
        
        # TODO: Extract additional attributes as needed
        # Common patterns:
        # - Extract from existing ONNX node: getCustomOp(node).get_nodeattr("attr_name")
        # - Tensor shapes: model.get_tensor_shape(tensor_name)
        # - NumChannels/ActVal for FINN compatibility
        # - Configuration interface width parameters
        
        return attrs
    
    def create_basic_finn_attributes(self, model: ModelWrapper, node: NodeProto) -> Dict[str, Any]:
        """
        Create FINN-specific attributes with interface datatype mappings.
        
        TODO: Map tensor indices to kernel interfaces based on your ONNX node structure.
        """
        attrs = {"backend": "fpgadataflow"}
        
        # Standard FINN datatype attributes
        # TODO: Update tensor indices based on your ONNX node structure
        if node.input:
            attrs["inputDataType"] = model.get_tensor_datatype(node.input[0]).name
        {% if kernel_metadata.has_weights %}
        if len(node.input) > 1:
            attrs["weightDataType"] = model.get_tensor_datatype(node.input[1]).name
            # Additional weight-specific datatypes if needed
            # attrs["thresholdDataType"] = model.get_tensor_datatype(node.input[1]).name
        {% endif %}
        if node.output:
            attrs["outputDataType"] = model.get_tensor_datatype(node.output[0]).name
        
        {% if kernel_metadata.config %}
        attrs["runtime_writeable_weights"] = True
        {% endif %}
        
        return attrs
    
    def convert_to_auto_hw_custom_op(self, model: ModelWrapper, node: NodeProto) -> NodeProto:
        """
        Convert node to AutoHWCustomOp.
        
        Override for special conversion logic (e.g., absorbing adjacent nodes).
        """
        # TODO: Add special conversion logic if needed
        return super().convert_to_auto_hw_custom_op(model, node)


############################################################################
# Kernel Metadata Reference
############################################################################
# Interfaces:
{% for interface in kernel_metadata.interfaces -%}
# - {{ interface.name }}: {{ interface.interface_type.name }}{% if interface.is_weight %} (weight){% endif %}
{% endfor %}
#
# Parameters:
{% for param in kernel_metadata.parameters -%}
# - {{ param.nodeattr_name }}: {{ param.rtl_type or 'parameter' }}{% if param.resolved_default %} = {{ param.resolved_default }}{% endif %}
{% endfor %}
{% if kernel_metadata.linked_parameters %}
#
# Linked Parameters:
{% for param in kernel_metadata.linked_parameters -%}
# - {{ param.name }}: {{ param.kernel_value or 'linked' }}
{% endfor %}
{% endif %}