"""
Enhanced Configuration framework for the Hardware Kernel Generator with Dataflow Integration.

This module extends the existing configuration framework with comprehensive
support for the Interface-Wise Dataflow Modeling system.
"""

from dataclasses import dataclass, field, asdict
from typing import Dict, Any, Optional, List, Union, Set
from pathlib import Path
import json
from enum import Enum

from .errors import ConfigurationError, ValidationError


class GeneratorType(Enum):
    """Supported generator types with dataflow integration."""
    HW_CUSTOM_OP = "hw_custom_op"
    RTL_BACKEND = "rtl_backend"
    TEST_SUITE = "test_suite"
    DOCUMENTATION = "documentation"
    
    # New dataflow-aware generators
    AUTO_HW_CUSTOM_OP = "auto_hw_custom_op"
    AUTO_RTL_BACKEND = "auto_rtl_backend"


class ValidationLevel(Enum):
    """Validation strictness levels."""
    STRICT = "strict"
    MODERATE = "moderate"
    RELAXED = "relaxed"


class DataflowMode(Enum):
    """Dataflow integration modes."""
    DISABLED = "disabled"
    HYBRID = "hybrid"         # Mix legacy and dataflow approaches
    DATAFLOW_ONLY = "dataflow_only"  # Use only dataflow modeling


@dataclass
class DataflowConfig:
    """Configuration for dataflow modeling integration."""
    
    # Dataflow integration mode
    mode: DataflowMode = DataflowMode.HYBRID
    
    # ONNX integration
    onnx_metadata: Optional[Dict[str, Any]] = None
    enable_onnx_layout_inference: bool = True
    default_onnx_layout: str = "NCHW"
    
    # Parallelism optimization
    enable_parallelism_optimization: bool = True
    parallelism_bounds_calculation: bool = True
    resource_estimation_enabled: bool = True
    
    # Tensor chunking
    enable_tensor_chunking: bool = True
    default_chunking_strategy: str = "broadcast"  # broadcast, divide, explicit
    
    # Validation
    validate_mathematical_constraints: bool = True
    validate_datatype_constraints: bool = True
    validate_tensor_shapes: bool = True
    
    # Performance optimization
    enable_interface_caching: bool = True
    enable_model_caching: bool = True
    cache_size_limit: int = 100
    
    # Resource constraints for optimization
    resource_constraints: Dict[str, Any] = field(default_factory=lambda: {
        "max_dsp": None,
        "max_bram": None, 
        "max_lut": None,
        "target_throughput": None,
        "target_latency": None
    })
    
    # Generator selection
    prefer_autogenerated_base_classes: bool = True
    fallback_to_legacy: bool = True
    
    def __post_init__(self):
        """Validate dataflow configuration."""
        # Validate ONNX layout
        valid_layouts = ["NCHW", "NHWC", "NCL", "NLC", "NC"]
        if self.default_onnx_layout not in valid_layouts:
            raise ConfigurationError(
                f"Invalid default_onnx_layout: {self.default_onnx_layout}",
                config_section="dataflow",
                suggestion=f"Use one of: {valid_layouts}"
            )
        
        # Validate chunking strategy
        valid_strategies = ["broadcast", "divide", "explicit"]
        if self.default_chunking_strategy not in valid_strategies:
            raise ConfigurationError(
                f"Invalid chunking strategy: {self.default_chunking_strategy}",
                config_section="dataflow",
                suggestion=f"Use one of: {valid_strategies}"
            )
        
        # Validate cache size
        if self.cache_size_limit < 1 or self.cache_size_limit > 1000:
            raise ConfigurationError(
                f"Invalid cache_size_limit: {self.cache_size_limit}",
                config_section="dataflow",
                suggestion="Use cache_size_limit between 1 and 1000"
            )


@dataclass
class TemplateConfig:
    """Enhanced configuration for template handling with dataflow support."""
    
    # Template directories
    template_dirs: List[Path] = field(default_factory=lambda: [
        Path(__file__).parent / "templates"
    ])
    
    # Template caching
    enable_caching: bool = True
    cache_size: int = 100
    cache_ttl: int = 3600  # seconds
    
    # Template customization
    custom_templates: Dict[str, Path] = field(default_factory=dict)
    template_overrides: Dict[str, str] = field(default_factory=dict)
    
    # Jinja2 environment settings
    trim_blocks: bool = True
    lstrip_blocks: bool = True
    keep_trailing_newline: bool = True
    auto_reload: bool = False
    strict_undefined: bool = True
    
    # Dataflow-specific templates
    dataflow_template_dirs: List[Path] = field(default_factory=lambda: [
        Path(__file__).parent / "templates" / "dataflow"
    ])
    
    # Template selection strategy
    prefer_dataflow_templates: bool = True
    template_selection_strategy: str = "auto"  # auto, dataflow, legacy, custom
    
    def __post_init__(self):
        """Validate template configuration."""
        # Convert string paths to Path objects
        self.template_dirs = [Path(d) for d in self.template_dirs]
        self.dataflow_template_dirs = [Path(d) for d in self.dataflow_template_dirs]
        
        # Validate template directories (create if they don't exist for defaults)
        for template_dir in self.template_dirs:
            if not template_dir.exists() and template_dir.name != "templates":
                raise ConfigurationError(
                    f"Template directory does not exist: {template_dir}",
                    config_section="template",
                    suggestion="Ensure all template directories exist before configuration"
                )
        
        # Validate dataflow template directories
        for template_dir in self.dataflow_template_dirs:
            if not template_dir.exists() and template_dir.name != "dataflow":
                raise ConfigurationError(
                    f"Dataflow template directory does not exist: {template_dir}",
                    config_section="template",
                    suggestion="Ensure dataflow template directories exist"
                )
        
        # Validate custom templates exist
        for name, path in self.custom_templates.items():
            template_path = Path(path)
            if not template_path.exists():
                raise ConfigurationError(
                    f"Custom template '{name}' not found: {template_path}",
                    config_section="template",
                    suggestion=f"Ensure custom template file exists: {template_path}"
                )
        
        # Validate template selection strategy
        valid_strategies = ["auto", "dataflow", "legacy", "custom"]
        if self.template_selection_strategy not in valid_strategies:
            raise ConfigurationError(
                f"Invalid template_selection_strategy: {self.template_selection_strategy}",
                config_section="template",
                suggestion=f"Use one of: {valid_strategies}"
            )


@dataclass
class GenerationConfig:
    """Enhanced configuration for code generation with dataflow support."""
    
    # Output settings
    output_dir: Path = field(default_factory=lambda: Path("./generated"))
    overwrite_existing: bool = False
    create_directories: bool = True
    
    # Generation options
    include_debug_info: bool = False
    include_documentation: bool = True
    include_type_hints: bool = True
    
    # Dataflow-specific generation
    generate_dataflow_model: bool = True
    generate_base_class_inheritance: bool = True
    minimize_template_code: bool = True
    
    # Code generation targets
    enabled_generators: Set[str] = field(default_factory=lambda: {
        "hwcustomop", "rtlbackend", "test", "documentation"
    })
    
    # Code formatting
    indent_size: int = 4
    use_tabs: bool = False
    max_line_length: int = 88
    
    # Generation filters
    skip_empty_methods: bool = True
    skip_unused_imports: bool = True
    skip_deprecated_methods: bool = True
    
    # Dataflow optimization
    optimize_for_dataflow: bool = True
    use_autogenerated_base_classes: bool = True
    generate_enhanced_validation: bool = True
    
    def __post_init__(self):
        """Validate generation configuration."""
        self.output_dir = Path(self.output_dir)
        
        # Validate output directory can be created
        if not self.output_dir.exists() and not self.create_directories:
            raise ConfigurationError(
                f"Output directory does not exist: {self.output_dir}",
                config_section="generation",
                suggestion="Set create_directories=True or create the directory manually"
            )
        
        # Validate formatting options
        if self.indent_size < 1 or self.indent_size > 8:
            raise ConfigurationError(
                f"Invalid indent_size: {self.indent_size}",
                config_section="generation",
                suggestion="Use indent_size between 1 and 8"
            )
        
        # Validate enabled generators
        valid_generators = {"hwcustomop", "rtlbackend", "test", "documentation", "wrapper"}
        invalid_generators = self.enabled_generators - valid_generators
        if invalid_generators:
            raise ConfigurationError(
                f"Invalid generators: {invalid_generators}",
                config_section="generation",
                suggestion=f"Use only: {valid_generators}"
            )


@dataclass
class AnalysisConfig:
    """Enhanced configuration for RTL analysis with dataflow integration."""
    
    # Analysis options
    analyze_interfaces: bool = True
    analyze_dependencies: bool = True
    analyze_timing: bool = False
    
    # Dataflow analysis
    analyze_dataflow_interfaces: bool = True
    infer_tensor_dimensions: bool = True
    validate_pragma_compatibility: bool = True
    
    # Interface analysis
    interface_patterns: List[str] = field(default_factory=lambda: [
        r"input\s+.*",
        r"output\s+.*", 
        r"inout\s+.*"
    ])
    
    # Dataflow interface detection
    axi_stream_patterns: List[str] = field(default_factory=lambda: [
        r".*_tdata",
        r".*_tvalid",
        r".*_tready",
        r".*_tlast"
    ])
    
    axi_lite_patterns: List[str] = field(default_factory=lambda: [
        r".*_awaddr",
        r".*_awvalid", 
        r".*_awready",
        r".*_wdata"
    ])
    
    # Dependency analysis
    dependency_patterns: List[str] = field(default_factory=lambda: [
        r"import\s+.*",
        r"include\s+.*",
        r"`include\s+.*"
    ])
    
    # Analysis depth
    max_depth: int = 5
    follow_includes: bool = True
    
    # Dataflow-specific analysis
    enable_rtl_to_dataflow_conversion: bool = True
    validate_conversion_results: bool = True
    
    def __post_init__(self):
        """Validate analysis configuration."""
        if self.max_depth < 1 or self.max_depth > 20:
            raise ConfigurationError(
                f"Invalid max_depth: {self.max_depth}",
                config_section="analysis",
                suggestion="Use max_depth between 1 and 20"
            )


@dataclass
class ValidationConfig:
    """Enhanced configuration for validation with dataflow support."""
    
    # Validation levels
    level: ValidationLevel = ValidationLevel.MODERATE
    
    # Validation options
    validate_syntax: bool = True
    validate_semantics: bool = True
    validate_compatibility: bool = True
    
    # Dataflow validation
    validate_dataflow_model: bool = True
    validate_interface_constraints: bool = True
    validate_tensor_chunking: bool = True
    validate_parallelism_bounds: bool = True
    
    # Error handling
    fail_on_warnings: bool = False
    max_errors: int = 10
    continue_on_validation_failure: bool = False
    
    # Validation rules
    required_fields: List[str] = field(default_factory=list)
    forbidden_patterns: List[str] = field(default_factory=list)
    
    # Dataflow-specific validation rules
    required_interfaces: List[str] = field(default_factory=lambda: ["input", "output"])
    validate_dimensional_consistency: bool = True
    
    def __post_init__(self):
        """Validate validation configuration."""
        if self.max_errors < 1 or self.max_errors > 100:
            raise ConfigurationError(
                f"Invalid max_errors: {self.max_errors}",
                config_section="validation",
                suggestion="Use max_errors between 1 and 100"
            )


@dataclass 
class PipelineConfig:
    """Enhanced main configuration for the HWKG pipeline with dataflow integration."""
    
    # Sub-configurations
    template: TemplateConfig = field(default_factory=TemplateConfig)
    generation: GenerationConfig = field(default_factory=GenerationConfig)
    analysis: AnalysisConfig = field(default_factory=AnalysisConfig)
    validation: ValidationConfig = field(default_factory=ValidationConfig)
    dataflow: DataflowConfig = field(default_factory=DataflowConfig)
    
    # Pipeline options
    generator_type: GeneratorType = GeneratorType.AUTO_HW_CUSTOM_OP
    enable_caching: bool = True
    parallel_processing: bool = False
    
    # Input/Output files
    rtl_file_path: Optional[Path] = None
    compiler_data_path: Optional[Path] = None
    output_dir: Optional[Path] = None
    custom_doc_path: Optional[Path] = None
    
    # Logging and debugging
    verbose: bool = False
    debug: bool = False
    log_level: str = "INFO"
    
    # Compatibility and migration
    maintain_backward_compatibility: bool = True
    legacy_mode: bool = False
    migration_mode: bool = False  # Gradual migration from legacy to dataflow
    
    # Performance and optimization
    optimization_enabled: bool = True
    error_recovery_enabled: bool = True
    continue_on_generator_failure: bool = True
    
    @classmethod
    def from_args(cls, args: Dict[str, Any]) -> 'PipelineConfig':
        """Create configuration from command line arguments or dictionary."""
        config = cls()
        
        # Enhanced argument mapping including dataflow options
        arg_mapping = {
            # Basic arguments
            'output_dir': ('generation', 'output_dir'),
            'template_dir': ('template', 'template_dirs'),
            'overwrite': ('generation', 'overwrite_existing'),
            'debug': ('debug',),
            'verbose': ('verbose',),
            'generator_type': ('generator_type',),
            
            # Input files
            'rtl_file_path': ('rtl_file_path',),
            'compiler_data_path': ('compiler_data_path',),
            'custom_doc_path': ('custom_doc_path',),
            
            # Dataflow options
            'dataflow_mode': ('dataflow', 'mode'),
            'enable_dataflow': ('dataflow', 'mode'),  # Simple enable/disable
            'onnx_metadata': ('dataflow', 'onnx_metadata'),
            'enable_parallelism_optimization': ('dataflow', 'enable_parallelism_optimization'),
            'resource_constraints': ('dataflow', 'resource_constraints'),
            
            # Generation options
            'enabled_generators': ('generation', 'enabled_generators'),
            'minimize_code': ('generation', 'minimize_template_code'),
        }
        
        for arg_name, config_path in arg_mapping.items():
            if arg_name in args:
                value = args[arg_name]
                
                # Special handling for specific arguments
                if arg_name == 'template_dir':
                    # Convert single template dir to list
                    if isinstance(value, (str, Path)):
                        value = [value]
                elif arg_name == 'generator_type':
                    # Convert string to enum
                    if isinstance(value, str):
                        value = GeneratorType(value)
                elif arg_name == 'dataflow_mode':
                    # Convert string to enum
                    if isinstance(value, str):
                        value = DataflowMode(value)
                elif arg_name == 'enable_dataflow':
                    # Convert boolean to dataflow mode
                    value = DataflowMode.DATAFLOW_ONLY if value else DataflowMode.DISABLED
                    config_path = ('dataflow', 'mode')
                elif arg_name == 'enabled_generators':
                    # Convert to set
                    if isinstance(value, (list, tuple)):
                        value = set(value)
                    elif isinstance(value, str):
                        value = {value}
                elif arg_name in ['rtl_file_path', 'compiler_data_path', 'custom_doc_path', 'output_dir']:
                    # Convert to Path
                    if value is not None:
                        value = Path(value)
                
                # Set the configuration value
                if len(config_path) == 1:
                    setattr(config, config_path[0], value)
                else:
                    sub_config = getattr(config, config_path[0])
                    setattr(sub_config, config_path[1], value)
                    
                    # Re-run __post_init__ for sub-config to apply validations
                    if hasattr(sub_config, '__post_init__'):
                        sub_config.__post_init__()
        
        # Sync output_dir between generation config and main config
        if config.output_dir is not None:
            config.generation.output_dir = config.output_dir
        elif config.generation.output_dir != Path("./generated"):
            config.output_dir = config.generation.output_dir
        
        return config
    
    @classmethod
    def from_defaults(cls, generator_type: GeneratorType = GeneratorType.AUTO_HW_CUSTOM_OP) -> 'PipelineConfig':
        """Create configuration with sensible defaults for a generator type."""
        config = cls(generator_type=generator_type)
        
        # Generator-specific defaults
        if generator_type in [GeneratorType.HW_CUSTOM_OP, GeneratorType.AUTO_HW_CUSTOM_OP]:
            config.generation.include_debug_info = True
            config.analysis.analyze_timing = False
            config.validation.level = ValidationLevel.MODERATE
            config.dataflow.mode = DataflowMode.HYBRID if generator_type == GeneratorType.HW_CUSTOM_OP else DataflowMode.DATAFLOW_ONLY
            
        elif generator_type in [GeneratorType.RTL_BACKEND, GeneratorType.AUTO_RTL_BACKEND]:
            config.generation.include_debug_info = False
            config.analysis.analyze_timing = True
            config.validation.level = ValidationLevel.STRICT
            config.dataflow.mode = DataflowMode.HYBRID if generator_type == GeneratorType.RTL_BACKEND else DataflowMode.DATAFLOW_ONLY
            
        elif generator_type == GeneratorType.TEST_SUITE:
            config.generation.include_debug_info = True
            config.generation.enabled_generators = {"test"}
            config.validation.level = ValidationLevel.STRICT
            config.dataflow.mode = DataflowMode.HYBRID
            
        elif generator_type == GeneratorType.DOCUMENTATION:
            config.generation.include_documentation = True
            config.generation.enabled_generators = {"documentation"}
            config.validation.level = ValidationLevel.MODERATE
            config.dataflow.mode = DataflowMode.DATAFLOW_ONLY
        
        return config
    
    @classmethod
    def from_file(cls, config_file: Union[str, Path]) -> 'PipelineConfig':
        """Load configuration from JSON file."""
        config_path = Path(config_file)
        
        if not config_path.exists():
            raise ConfigurationError(
                f"Configuration file not found: {config_path}",
                config_section="file",
                suggestion=f"Create configuration file or check path: {config_path}"
            )
        
        try:
            with open(config_path, 'r') as f:
                data = json.load(f)
            
            return cls.from_dict(data)
        except json.JSONDecodeError as e:
            raise ConfigurationError(
                f"Invalid JSON in configuration file: {e}",
                config_section="file",
                suggestion="Check JSON syntax in configuration file"
            )
        except Exception as e:
            raise ConfigurationError(
                f"Error loading configuration file: {e}",
                config_section="file",
                suggestion="Ensure file is readable and contains valid configuration"
            )
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'PipelineConfig':
        """Create configuration from dictionary."""
        # Extract sub-configuration data
        template_data = data.pop('template', {})
        generation_data = data.pop('generation', {})
        analysis_data = data.pop('analysis', {})
        validation_data = data.pop('validation', {})
        dataflow_data = data.pop('dataflow', {})
        
        # Handle Path conversions
        for path_field in ['rtl_file_path', 'compiler_data_path', 'output_dir', 'custom_doc_path']:
            if path_field in data and data[path_field] is not None:
                data[path_field] = Path(data[path_field])
        
        # Handle enabled_generators conversion from list to set
        if 'enabled_generators' in generation_data and isinstance(generation_data['enabled_generators'], list):
            generation_data['enabled_generators'] = set(generation_data['enabled_generators'])
        
        # Create sub-configurations
        template_config = TemplateConfig(**template_data)
        generation_config = GenerationConfig(**generation_data)
        analysis_config = AnalysisConfig(**analysis_data)
        validation_config = ValidationConfig(**validation_data)
        dataflow_config = DataflowConfig(**dataflow_data)
        
        # Handle enum conversions
        if 'generator_type' in data:
            if isinstance(data['generator_type'], str):
                data['generator_type'] = GeneratorType(data['generator_type'])
        
        if 'level' in validation_data:
            if isinstance(validation_data['level'], str):
                validation_config.level = ValidationLevel(validation_data['level'])
        
        if 'mode' in dataflow_data:
            if isinstance(dataflow_data['mode'], str):
                dataflow_config.mode = DataflowMode(dataflow_data['mode'])
        
        # Create main configuration
        config = cls(
            template=template_config,
            generation=generation_config,
            analysis=analysis_config,
            validation=validation_config,
            dataflow=dataflow_config,
            **data
        )
        
        return config
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert configuration to dictionary."""
        data = asdict(self)
        
        # Convert enums to strings
        data['generator_type'] = self.generator_type.value
        data['validation']['level'] = self.validation.level.value
        data['dataflow']['mode'] = self.dataflow.mode.value
        
        # Convert Path objects to strings
        for path_field in ['rtl_file_path', 'compiler_data_path', 'output_dir', 'custom_doc_path']:
            if data.get(path_field) is not None:
                data[path_field] = str(data[path_field])
        
        data['generation']['output_dir'] = str(self.generation.output_dir)
        data['template']['template_dirs'] = [str(d) for d in self.template.template_dirs]
        data['template']['dataflow_template_dirs'] = [str(d) for d in self.template.dataflow_template_dirs]
        data['template']['custom_templates'] = {
            k: str(v) for k, v in self.template.custom_templates.items()
        }
        
        # Convert sets to lists for JSON serialization
        data['generation']['enabled_generators'] = list(self.generation.enabled_generators)
        
        return data
    
    def to_file(self, config_file: Union[str, Path]) -> None:
        """Save configuration to JSON file."""
        config_path = Path(config_file)
        
        # Create directory if it doesn't exist
        config_path.parent.mkdir(parents=True, exist_ok=True)
        
        try:
            with open(config_path, 'w') as f:
                json.dump(self.to_dict(), f, indent=2)
        except Exception as e:
            raise ConfigurationError(
                f"Error saving configuration file: {e}",
                config_section="file",
                suggestion="Ensure directory is writable"
            )
    
    def validate(self) -> None:
        """Validate the entire configuration."""
        # Validate sub-configurations (done in __post_init__)
        # Additional cross-configuration validation
        
        # Ensure template directories are set if using custom templates
        if self.template.custom_templates and not self.template.template_dirs:
            raise ValidationError(
                "Custom templates specified but no template directories configured",
                validation_type="configuration",
                suggestion="Add template directories to template.template_dirs"
            )
        
        # Ensure output directory is writable
        if self.generation.output_dir.exists():
            if not self.generation.output_dir.is_dir():
                raise ValidationError(
                    f"Output path is not a directory: {self.generation.output_dir}",
                    validation_type="configuration",
                    suggestion="Use a directory path for output_dir"
                )
        
        # Validate generator-specific requirements
        if self.generator_type in [GeneratorType.RTL_BACKEND, GeneratorType.AUTO_RTL_BACKEND]:
            if not self.analysis.analyze_interfaces:
                raise ValidationError(
                    "RTL backend requires interface analysis",
                    validation_type="configuration",
                    suggestion="Set analysis.analyze_interfaces=True for RTL backend"
                )
        
        # Validate dataflow configuration consistency
        if self.dataflow.mode == DataflowMode.DATAFLOW_ONLY:
            if self.generator_type in [GeneratorType.HW_CUSTOM_OP, GeneratorType.RTL_BACKEND]:
                raise ValidationError(
                    f"Dataflow-only mode incompatible with legacy generator type: {self.generator_type}",
                    validation_type="configuration",
                    suggestion=f"Use AUTO_{self.generator_type.value.upper()} or change dataflow mode"
                )
        
        # Validate file paths if provided
        if self.rtl_file_path and not self.rtl_file_path.exists():
            raise ValidationError(
                f"RTL file not found: {self.rtl_file_path}",
                validation_type="configuration",
                suggestion="Ensure RTL file exists or remove from configuration"
            )
        
        if self.compiler_data_path and not self.compiler_data_path.exists():
            raise ValidationError(
                f"Compiler data file not found: {self.compiler_data_path}",
                validation_type="configuration", 
                suggestion="Ensure compiler data file exists or remove from configuration"
            )
    
    def get_effective_config(self) -> 'PipelineConfig':
        """Get configuration with all defaults resolved and validation applied."""
        # Create a copy to avoid modifying the original
        config = PipelineConfig.from_dict(self.to_dict())
        
        # Apply validation
        config.validate()
        
        return config
    
    def is_dataflow_enabled(self) -> bool:
        """Check if dataflow modeling is enabled."""
        return self.dataflow.mode != DataflowMode.DISABLED
    
    def should_use_legacy_generators(self) -> bool:
        """Check if legacy generators should be used."""
        return (self.dataflow.mode == DataflowMode.DISABLED or 
                self.legacy_mode or
                self.generator_type in [GeneratorType.HW_CUSTOM_OP, GeneratorType.RTL_BACKEND])
    
    def get_template_dirs(self) -> List[Path]:
        """Get effective template directories based on configuration."""
        dirs = list(self.template.template_dirs)
        
        if self.is_dataflow_enabled() and self.template.prefer_dataflow_templates:
            # Prepend dataflow template directories for higher priority
            dirs = list(self.template.dataflow_template_dirs) + dirs
        
        return dirs


def create_default_config(generator_type: GeneratorType = GeneratorType.AUTO_HW_CUSTOM_OP) -> PipelineConfig:
    """Create a default configuration for common use cases with dataflow support."""
    return PipelineConfig.from_defaults(generator_type)


def create_dataflow_config(
    rtl_file: Union[str, Path],
    compiler_data: Union[str, Path],
    output_dir: Union[str, Path],
    onnx_metadata: Optional[Dict] = None
) -> PipelineConfig:
    """Create a dataflow-optimized configuration for common use cases."""
    config = PipelineConfig.from_defaults(GeneratorType.AUTO_HW_CUSTOM_OP)
    
    # Set file paths
    config.rtl_file_path = Path(rtl_file)
    config.compiler_data_path = Path(compiler_data)
    config.output_dir = Path(output_dir)
    config.generation.output_dir = Path(output_dir)
    
    # Optimize for dataflow
    config.dataflow.mode = DataflowMode.DATAFLOW_ONLY
    config.dataflow.onnx_metadata = onnx_metadata
    config.dataflow.enable_parallelism_optimization = True
    config.generation.use_autogenerated_base_classes = True
    config.generation.minimize_template_code = True
    
    return config


def load_config(config_source: Union[str, Path, Dict[str, Any], None] = None) -> PipelineConfig:
    """Load configuration from various sources with dataflow support."""
    if config_source is None:
        return create_default_config()
    elif isinstance(config_source, (str, Path)):
        return PipelineConfig.from_file(config_source)
    elif isinstance(config_source, dict):
        return PipelineConfig.from_dict(config_source)
    else:
        raise ConfigurationError(
            f"Invalid configuration source type: {type(config_source)}",
            config_section="loading",
            suggestion="Use file path, dictionary, or None for defaults"
        )