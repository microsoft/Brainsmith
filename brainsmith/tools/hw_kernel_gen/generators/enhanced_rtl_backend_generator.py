"""
Enhanced RTL Backend Generator.

This module provides an enhanced RTL Backend generator that integrates with
the new Week 3 orchestration architecture and dataflow modeling system.
"""

import time
from typing import Dict, Any, Optional, List, Union
from pathlib import Path

from ..enhanced_config import PipelineConfig, GeneratorType, DataflowMode
from ..enhanced_generator_base import GeneratorBase, GenerationResult, GeneratedArtifact
from ..enhanced_data_structures import RTLModule
from ..enhanced_template_context import EnhancedTemplateContextBuilder, RTLBackendContext
from ..enhanced_template_manager import EnhancedTemplateManager
from ..errors import CodeGenerationError, ValidationError, TemplateError
from ..orchestration.generator_factory import GeneratorCapability


class EnhancedRTLBackendGenerator(GeneratorBase):
    """
    Enhanced RTL Backend generator with dataflow integration.
    
    This generator creates optimized RTL Backend implementations that integrate
    with the AutoRTLBackend base class and provide FINN integration capabilities.
    """
    
    def __init__(self, config: PipelineConfig):
        super().__init__(config)
        self.template_manager = EnhancedTemplateManager(config.template)
        self.context_builder = EnhancedTemplateContextBuilder(config)
        
        # Generator metadata
        self.capabilities = {
            GeneratorCapability.RTL_BACKEND,
            GeneratorCapability.DATAFLOW_INTEGRATION,
            GeneratorCapability.VALIDATION,
            GeneratorCapability.OPTIMIZATION
        }
        
        # Performance tracking
        self._generation_count = 0
        self._total_generation_time = 0.0
        self._optimization_count = 0
    
    def get_template_name(self) -> str:
        """Get the primary template name for this generator."""
        if self.config.generation.use_autogenerated_base_classes:
            return "rtl_backend_slim.py.j2"
        else:
            return "rtl_backend.py.j2"
    
    def get_artifact_type(self) -> str:
        """Get the artifact type produced by this generator."""
        return "rtlbackend"
    
    def get_capabilities(self) -> set:
        """Get generator capabilities."""
        return self.capabilities
    
    def validate_inputs(self, inputs: Dict[str, Any]) -> List[str]:
        """Validate generator inputs."""
        errors = []
        
        # Check for required inputs
        if "rtl_module" not in inputs:
            errors.append("Missing required input: 'rtl_module'")
        
        # Validate RTL module
        if "rtl_module" in inputs:
            rtl_module = inputs["rtl_module"]
            if not isinstance(rtl_module, RTLModule):
                errors.append(f"Invalid rtl_module type: {type(rtl_module)}")
            elif not rtl_module.interfaces:
                errors.append("RTL module has no interfaces")
            elif not any(iface.interface_type in ["axi_stream", "axi_lite"] for iface in rtl_module.interfaces):
                errors.append("RTL module lacks required AXI interfaces for RTL backend")
        
        # Validate configuration
        if "config" in inputs:
            config = inputs["config"]
            if not isinstance(config, PipelineConfig):
                errors.append(f"Invalid config type: {type(config)}")
        
        return errors
    
    def generate(self, inputs: Dict[str, Any]) -> GenerationResult:
        """Generate RTL Backend implementation."""
        start_time = time.time()
        result = GenerationResult(success=True)
        
        try:
            # Validate inputs
            validation_errors = self.validate_inputs(inputs)
            if validation_errors:
                for error in validation_errors:
                    result.add_error(error)
                return result
            
            # Extract and prepare inputs
            rtl_module = inputs["rtl_module"]
            config = inputs.get("config", self.config)
            
            # Build template context
            context = self._build_template_context(rtl_module, inputs)
            
            # Generate primary RTL backend artifact
            backend_artifact = self._generate_rtl_backend_artifact(context, inputs)
            result.add_artifact(backend_artifact)
            
            # Generate additional artifacts if requested
            if self.config.generation.include_documentation:
                doc_artifact = self._generate_documentation_artifact(context, inputs)
                if doc_artifact:
                    result.add_artifact(doc_artifact)
            
            # Generate wrapper artifacts if needed
            if inputs.get("generate_wrapper", True):
                wrapper_artifact = self._generate_wrapper_artifact(context, inputs)
                if wrapper_artifact:
                    result.add_artifact(wrapper_artifact)
            
            # Add generation metadata
            generation_time = time.time() - start_time
            self._update_generation_metrics(generation_time, True)
            
            result.metadata.update({
                "generator": "enhanced_rtl_backend",
                "generation_time": generation_time,
                "template_used": self.get_template_name(),
                "dataflow_enabled": config.dataflow.mode != DataflowMode.DISABLED,
                "slim_mode": config.generation.use_autogenerated_base_classes,
                "optimization_applied": self._optimization_count > 0,
                "capabilities": [cap.value for cap in self.capabilities]
            })
            
            return result
            
        except Exception as e:
            generation_time = time.time() - start_time
            self._update_generation_metrics(generation_time, False)
            
            result.add_error(f"RTL Backend generation failed: {e}")
            result.success = False
            return result
    
    def _build_template_context(self, rtl_module: RTLModule, inputs: Dict[str, Any]) -> RTLBackendContext:
        """Build template context for RTL Backend generation."""
        try:
            # Use enhanced context builder
            context = self.context_builder.build_rtlbackend_context(
                rtl_module=rtl_module,
                config=self.config,
                analysis_results=inputs.get("analysis_results"),
                performance_metadata=inputs.get("performance_metadata"),
                custom_metadata=inputs.get("custom_metadata", {})
            )
            
            # Add RTL backend-specific context
            context.custom_data.update({
                "backend_class_name": inputs.get("backend_class_name", f"{rtl_module.name}RTLBackend"),
                "include_performance_monitoring": inputs.get("include_performance_monitoring", True),
                "enable_resource_estimation": self.config.dataflow.resource_estimation_enabled,
                "enable_optimization": self.config.optimization_enabled,
                "target_fpga": inputs.get("target_fpga", "generic"),
                "clock_frequency": inputs.get("clock_frequency", 100),  # MHz
                "memory_mode": inputs.get("memory_mode", "external")
            })
            
            # Apply optimizations if enabled
            if self.config.optimization_enabled:
                context = self._apply_optimizations(context, inputs)
            
            return context
            
        except Exception as e:
            raise TemplateError(f"Failed to build RTL backend template context: {e}")
    
    def _apply_optimizations(self, context: RTLBackendContext, inputs: Dict[str, Any]) -> RTLBackendContext:
        """Apply RTL backend optimizations."""
        try:
            # Interface optimization
            optimized_interfaces = self._optimize_interfaces(context.rtl_module.interfaces)
            context.custom_data["optimized_interfaces"] = optimized_interfaces
            
            # Resource optimization
            if self.config.dataflow.resource_estimation_enabled:
                resource_estimates = self._estimate_resources(context.rtl_module, inputs)
                context.custom_data["resource_estimates"] = resource_estimates
            
            # Performance optimization
            if context.custom_data.get("include_performance_monitoring", True):
                performance_config = self._generate_performance_config(context.rtl_module)
                context.custom_data["performance_config"] = performance_config
            
            self._optimization_count += 1
            return context
            
        except Exception as e:
            # Don't fail generation for optimization errors
            context.custom_data["optimization_errors"] = [str(e)]
            return context
    
    def _optimize_interfaces(self, interfaces: List[Any]) -> Dict[str, Any]:
        """Optimize interface configurations for RTL backend."""
        optimized = {
            "stream_interfaces": [],
            "control_interfaces": [],
            "memory_interfaces": [],
            "optimization_applied": []
        }
        
        for interface in interfaces:
            if hasattr(interface, 'interface_type'):
                if interface.interface_type == "axi_stream":
                    # Optimize AXI Stream interfaces
                    opt_config = {
                        "name": interface.name,
                        "data_width": self._get_interface_data_width(interface),
                        "enable_tlast": self._interface_has_signal(interface, "tlast"),
                        "enable_tkeep": self._interface_has_signal(interface, "tkeep"),
                        "buffering": "pipeline"  # Default optimization
                    }
                    optimized["stream_interfaces"].append(opt_config)
                    optimized["optimization_applied"].append("stream_buffering")
                
                elif interface.interface_type == "axi_lite":
                    # Optimize AXI Lite interfaces
                    opt_config = {
                        "name": interface.name,
                        "address_width": 32,  # Standard
                        "data_width": 32,     # Standard
                        "enable_cache": True
                    }
                    optimized["control_interfaces"].append(opt_config)
                    optimized["optimization_applied"].append("control_caching")
        
        return optimized
    
    def _get_interface_data_width(self, interface: Any) -> int:
        """Get data width from interface signals."""
        if hasattr(interface, 'signals'):
            for signal in interface.signals:
                if hasattr(signal, 'interface_role') and signal.interface_role == "tdata":
                    return getattr(signal, 'width', 32)
        return 32  # Default
    
    def _interface_has_signal(self, interface: Any, role: str) -> bool:
        """Check if interface has signal with specific role."""
        if hasattr(interface, 'signals'):
            for signal in interface.signals:
                if hasattr(signal, 'interface_role') and signal.interface_role == role:
                    return True
        return False
    
    def _estimate_resources(self, rtl_module: RTLModule, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Estimate FPGA resource requirements."""
        # Simplified resource estimation
        interface_count = len(rtl_module.interfaces)
        parameter_count = len(rtl_module.parameters)
        
        # Basic estimation heuristics
        estimated_luts = interface_count * 100 + parameter_count * 10
        estimated_ffs = interface_count * 150 + parameter_count * 15
        estimated_dsps = max(1, interface_count // 4)  # Assume some DSP usage
        estimated_brams = max(1, interface_count // 2)  # Assume some memory usage
        
        return {
            "luts": estimated_luts,
            "flip_flops": estimated_ffs,
            "dsps": estimated_dsps,
            "brams": estimated_brams,
            "estimation_method": "heuristic",
            "confidence": "low"
        }
    
    def _generate_performance_config(self, rtl_module: RTLModule) -> Dict[str, Any]:
        """Generate performance monitoring configuration."""
        return {
            "enable_cycle_counting": True,
            "enable_throughput_monitoring": True,
            "enable_latency_measurement": True,
            "enable_resource_utilization": True,
            "performance_counters": [
                "input_transactions",
                "output_transactions", 
                "processing_cycles",
                "idle_cycles"
            ]
        }
    
    def _generate_rtl_backend_artifact(
        self, 
        context: RTLBackendContext, 
        inputs: Dict[str, Any]
    ) -> GeneratedArtifact:
        """Generate the primary RTL Backend artifact."""
        try:
            # Render template
            template_name = self.get_template_name()
            content = self.template_manager.render_template(
                template_name=template_name,
                context=context.to_dict(),
                generator_type=GeneratorType.AUTO_RTL_BACKEND
            )
            
            # Determine output file name
            backend_class_name = context.custom_data.get("backend_class_name", f"{context.rtl_module.name}RTLBackend")
            file_name = f"{backend_class_name}.py"
            
            # Create artifact
            artifact = GeneratedArtifact(
                file_name=file_name,
                content=content,
                artifact_type=self.get_artifact_type(),
                metadata={
                    "backend_class_name": backend_class_name,
                    "module_name": context.rtl_module.name,
                    "template_name": template_name,
                    "interface_count": len(context.rtl_module.interfaces),
                    "optimization_applied": self._optimization_count > 0,
                    "dataflow_enabled": context.config_metadata.get("dataflow_enabled", False),
                    "slim_mode": context.custom_data.get("slim_mode", False),
                    "generation_timestamp": time.time()
                }
            )
            
            # Validate artifact if enabled
            if self.config.generation.generate_enhanced_validation:
                self._validate_artifact(artifact, context)
            
            return artifact
            
        except TemplateError:
            raise
        except Exception as e:
            raise CodeGenerationError(
                f"Failed to generate RTL Backend artifact: {e}",
                generator_type="enhanced_rtl_backend",
                template_name=template_name
            )
    
    def _generate_documentation_artifact(
        self, 
        context: RTLBackendContext, 
        inputs: Dict[str, Any]
    ) -> Optional[GeneratedArtifact]:
        """Generate documentation artifact for RTL Backend."""
        try:
            # Check if documentation template exists
            doc_template = "rtl_backend_doc.md.j2"
            if not self.template_manager.template_exists(doc_template):
                return None
            
            # Render documentation
            content = self.template_manager.render_template(
                template_name=doc_template,
                context=context.to_dict(),
                generator_type=GeneratorType.AUTO_RTL_BACKEND
            )
            
            file_name = f"{context.rtl_module.name}_RTLBackend_README.md"
            
            return GeneratedArtifact(
                file_name=file_name,
                content=content,
                artifact_type="documentation",
                metadata={
                    "doc_type": "rtl_backend_readme",
                    "module_name": context.rtl_module.name,
                    "generation_timestamp": time.time()
                }
            )
            
        except Exception:
            # Don't fail generation for documentation errors
            return None
    
    def _generate_wrapper_artifact(
        self, 
        context: RTLBackendContext, 
        inputs: Dict[str, Any]
    ) -> Optional[GeneratedArtifact]:
        """Generate Verilog wrapper artifact if needed."""
        try:
            # Check if wrapper template exists
            wrapper_template = "rtl_wrapper.v.j2"
            if not self.template_manager.template_exists(wrapper_template):
                return None
            
            # Render wrapper
            content = self.template_manager.render_template(
                template_name=wrapper_template,
                context=context.to_dict(),
                generator_type=GeneratorType.AUTO_RTL_BACKEND
            )
            
            file_name = f"{context.rtl_module.name}_wrapper.v"
            
            return GeneratedArtifact(
                file_name=file_name,
                content=content,
                artifact_type="wrapper",
                metadata={
                    "wrapper_type": "verilog",
                    "module_name": context.rtl_module.name,
                    "generation_timestamp": time.time()
                }
            )
            
        except Exception:
            # Don't fail generation for wrapper errors
            return None
    
    def _validate_artifact(self, artifact: GeneratedArtifact, context: RTLBackendContext) -> None:
        """Validate generated RTL Backend artifact."""
        try:
            # Basic content validation
            if not artifact.content.strip():
                raise ValidationError("Generated artifact is empty")
            
            # Check for required Python/RTL Backend elements
            required_elements = ["class", "def", "import", "RTLBackend"]
            for element in required_elements:
                if element not in artifact.content:
                    artifact.metadata["warnings"] = artifact.metadata.get("warnings", [])
                    artifact.metadata["warnings"].append(f"Missing expected element: {element}")
            
            # Validate backend class name
            backend_class_name = context.custom_data.get("backend_class_name")
            if backend_class_name and f"class {backend_class_name}" not in artifact.content:
                artifact.metadata["warnings"] = artifact.metadata.get("warnings", [])
                artifact.metadata["warnings"].append(f"Expected backend class '{backend_class_name}' not found")
            
            # Check for AXI interface handling
            axi_interfaces = [iface for iface in context.rtl_module.interfaces 
                            if hasattr(iface, 'interface_type') and 'axi' in iface.interface_type]
            if axi_interfaces and "axi" not in artifact.content.lower():
                artifact.metadata["warnings"] = artifact.metadata.get("warnings", [])
                artifact.metadata["warnings"].append("AXI interfaces present but no AXI handling found in code")
            
        except Exception as e:
            # Don't fail generation for validation errors
            artifact.metadata["validation_errors"] = [str(e)]
    
    def _update_generation_metrics(self, generation_time: float, success: bool) -> None:
        """Update generation performance metrics."""
        self._generation_count += 1
        self._total_generation_time += generation_time
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """Get generator performance metrics."""
        avg_time = (
            self._total_generation_time / self._generation_count 
            if self._generation_count > 0 else 0.0
        )
        
        return {
            "generation_count": self._generation_count,
            "total_generation_time": self._total_generation_time,
            "average_generation_time": avg_time,
            "optimization_count": self._optimization_count,
            "optimization_rate": (
                self._optimization_count / self._generation_count 
                if self._generation_count > 0 else 0.0
            )
        }
    
    def get_supported_templates(self) -> List[str]:
        """Get list of supported templates."""
        return [
            "rtl_backend_slim.py.j2",
            "rtl_backend.py.j2",
            "rtl_backend_dataflow.py.j2",
            "rtl_backend_doc.md.j2",
            "rtl_wrapper.v.j2"
        ]
    
    def supports_dataflow_mode(self) -> bool:
        """Check if generator supports dataflow mode."""
        return GeneratorCapability.DATAFLOW_INTEGRATION in self.capabilities
    
    def get_configuration_requirements(self) -> Dict[str, Any]:
        """Get configuration requirements for this generator."""
        return {
            "required_config_sections": ["template", "generation", "dataflow"],
            "required_templates": ["rtl_backend.py.j2", "rtl_backend_slim.py.j2"],
            "required_capabilities": [cap.value for cap in self.capabilities],
            "optional_inputs": [
                "backend_class_name", "target_fpga", "clock_frequency",
                "memory_mode", "performance_metadata", "analysis_results"
            ],
            "output_artifacts": [self.get_artifact_type(), "documentation", "wrapper"]
        }


# Factory function for backward compatibility
def create_enhanced_rtl_backend_generator(config: PipelineConfig) -> EnhancedRTLBackendGenerator:
    """Create enhanced RTL Backend generator."""
    return EnhancedRTLBackendGenerator(config)


# Legacy compatibility wrapper
def generate_enhanced_rtlbackend(
    rtl_module: RTLModule,
    config: PipelineConfig,
    backend_class_name: Optional[str] = None,
    **kwargs
) -> GenerationResult:
    """
    Legacy-compatible function interface for RTL Backend generation.
    """
    generator = EnhancedRTLBackendGenerator(config)
    
    inputs = {
        "rtl_module": rtl_module,
        "config": config,
        "backend_class_name": backend_class_name,
        **kwargs
    }
    
    return generator.generate(inputs)