{#-
RTLBackend Template for HWKG

Generates minimal RTLBackend subclasses that inherit from AutoRTLBackend
and provide operation-specific customizations.
-#}
# Auto-generated by Brainsmith Hardware Kernel Generator
# Generated from: {{ source_file }}
# Date: {{ generation_timestamp }}

{% if base_kernel_class %}
from finn.custom_op.fpgadataflow.{{ base_kernel_class.lower() }} import {{ base_kernel_class }}
{% endif %}
from brainsmith.dataflow.core.auto_rtl_backend import AutoRTLBackend
{% if datatype_derivation_methods %}
from qonnx.core.datatype import DataType
{% endif %}
{% if complexity_level == "medium" and has_implementation_styles %}
from brainsmith.dataflow.core.auto_rtl_backend import ImplementationStyleMixin
{% endif %}
{% if complexity_level == "high" and has_advanced_memory %}
from brainsmith.dataflow.core.auto_rtl_backend import AdvancedMemoryMixin
{% endif %}
{% if has_dynamic_config %}
from brainsmith.dataflow.core.auto_rtl_backend import DynamicConfigMixin
{% endif %}


class {{ kernel_name }}_rtl({% if base_kernel_class %}{{ base_kernel_class }}, {% endif %}{% if complexity_level == "medium" and has_implementation_styles %}ImplementationStyleMixin, {% endif %}{% if complexity_level == "high" and has_advanced_memory %}AdvancedMemoryMixin, {% endif %}{% if has_dynamic_config %}DynamicConfigMixin, {% endif %}AutoRTLBackend):
    """RTL backend for {{ kernel_name }} operation.
    
    Auto-generated from SystemVerilog RTL analysis.
    {% if operation_description %}
    
    {{ operation_description }}
    {% endif %}
    """
    
    def __init__(self, onnx_node, **kwargs):
        super().__init__(onnx_node, **kwargs)
    
    @property
    def finn_rtllib_module(self) -> str:
        """Return finn-rtllib module name for this operation."""
        return "{{ finn_rtllib_module }}"
    
    def get_nodeattr_types(self):
        """Get node attribute types for RTLBackend (algorithm parameters only)."""
        my_attrs = {}
        
        # Only expose algorithm parameters (exposed parameters only)
{% for param in parameter_definitions %}
    {% if param.name in exposed_parameters %}
        {% if param.required %}
        # Algorithm parameter - required
        my_attrs["{{ param.name }}"] = ("i", True, {{ param.default_value if param.default_value is not none else "None" }})
        {% else %}
        # Algorithm parameter - optional with default
        my_attrs["{{ param.name }}"] = ("i", False, {{ param.default_value if param.default_value is not none else "1" }})
        {% endif %}
    {% endif %}
{% endfor %}
        
        {% if has_config_interface %}
        # Standardized AXI-Lite configuration support
        my_attrs["axilite_config"] = ("i", False, 1, {0, 1})
        {% endif %}
        
        # Merge with parent class attributes
{% if base_kernel_class %}
        my_attrs.update({{ base_kernel_class }}.get_nodeattr_types(self))
{% endif %}
        my_attrs.update(AutoRTLBackend.get_nodeattr_types(self))
        
        return my_attrs
    
    def prepare_codegen_rtl_values(self, model):
        """Prepare template variables for RTL code generation.
        
        All dictionary values produced in this function are to replace
        their key value(s) in the RTL template files following FINN's pattern.
        """
        code_gen_dict = {}
        
        # Basic module information
        code_gen_dict["$MODULE_NAME_AXI_WRAPPER$"] = [self.get_verilog_top_module_name()]
        code_gen_dict["$TOP_MODULE$"] = code_gen_dict["$MODULE_NAME_AXI_WRAPPER$"]
        
        # Standard stream width variables
        code_gen_dict["$IBITS$"] = [str(self.get_instream_width())]
        code_gen_dict["$OBITS$"] = [str(self.get_outstream_width())]
        
        # Algorithm parameters from node attributes
{% for param in parameter_definitions %}
    {% if param.name in exposed_parameters %}
        {%- set nodeattr_name = parameter_aliases.get(param.name, param.name) %}
        code_gen_dict["${{ param.name.upper() }}$"] = [str(self.get_nodeattr("{{ nodeattr_name }}"))]{% if nodeattr_name != param.name %}  # Aliased from {{ nodeattr_name }}{% endif %}

    {% elif param.name in derived_parameters %}
        # Derived parameter - computed from expression
        code_gen_dict["${{ param.name.upper() }}$"] = [str({{ derived_parameters[param.name] }})]

    {% endif %}
{% endfor %}
        
        # BDIM/SDIM parameter linkage (from interface metadata)
        interface_metadata = self.get_interface_metadata()
        for interface in interface_metadata:
            # BDIM parameter linkage
            if hasattr(interface, 'bdim_param') and interface.bdim_param:
                bdim_value = self.get_nodeattr(interface.bdim_param) if interface.bdim_param in {{ exposed_parameters }} else None
                if bdim_value is not None:
                    code_gen_dict[f"${interface.bdim_param.upper()}$"] = [str(bdim_value)]
            
            # SDIM parameter linkage  
            if hasattr(interface, 'sdim_param') and interface.sdim_param:
                sdim_value = self.get_nodeattr(interface.sdim_param) if interface.sdim_param in {{ exposed_parameters }} else None
                if sdim_value is not None:
                    code_gen_dict[f"${interface.sdim_param.upper()}$"] = [str(sdim_value)]
        
        # Datatype parameter assignments (generated at template time)
        from qonnx.core.datatype import DataType
{% for assignment in datatype_parameter_assignments %}
        # {{ assignment.comment }}
        code_gen_dict["{{ assignment.template_var }}"] = [{{ assignment.source }}]
{% endfor %}
        
        # Stream width parameters from DataflowModel
        dataflow_model = self.get_dataflow_model()
        if dataflow_model:
            for iface_name, iface in dataflow_model.interfaces.items():
                # Calculate stream width for this interface
                stream_width = iface.calculate_stream_width()
                # Generate template variable using original interface name (uppercase)
                template_var = f"${iface.name.upper()}_STREAM_WIDTH$"
                code_gen_dict[template_var] = [str(stream_width)]
        
        return code_gen_dict
    
    def get_supporting_rtl_files(self) -> list:
        """Get list of supporting RTL files to include."""
        return [
{% for file_name in supporting_rtl_files %}
            "{{ file_name }}",
{% endfor %}
        ]
{% if has_custom_execution %}
    
    def execute_node(self, context, graph):
        """Custom execution handling for {{ kernel_name }}."""
        mode = self.get_nodeattr("exec_mode")
        if mode == "cppsim":
{% if base_kernel_class %}
            {{ base_kernel_class }}.execute_node(self, context, graph)
{% else %}
            # Custom cppsim implementation would go here
            super().execute_node(context, graph)
{% endif %}
        elif mode == "rtlsim":
            # Custom rtlsim implementation for {{ kernel_name }}
            code_gen_dir = self.get_nodeattr("code_gen_dir_ipgen")
            
            # Input processing
            node = self.onnx_node
            for in_ind, inputs in enumerate(node.input):
                # Operation-specific input processing
                pass
            
            # RTL simulation
            sim = self.get_rtlsim()
            self.reset_rtlsim(sim)
            
            # Operation-specific simulation logic would go here
            
            self.close_rtlsim(sim)
        else:
            raise Exception(f"Invalid exec_mode '{mode}'. Must be 'cppsim' or 'rtlsim'")
{% endif %}
    
    def lut_estimation(self) -> int:
        """Estimate LUT usage for {{ kernel_name }}."""
        # TODO: Implement operation-specific LUT estimation
        # For now, use base class conservative estimate
        return super().lut_estimation()
    
    def bram_estimation(self) -> int:
        """Estimate BRAM usage for {{ kernel_name }}."""
        # TODO: Implement operation-specific BRAM estimation
        # For now, use base class estimate
        return super().bram_estimation()
    
    def dsp_estimation(self, fpgapart) -> int:
        """Estimate DSP usage for {{ kernel_name }}."""
        # TODO: Implement operation-specific DSP estimation
        # For now, use base class estimate
        return super().dsp_estimation(fpgapart)
    
    {% if has_config_interface %}
    def get_verilog_top_module_intf_names(self):
        """Override to add AXI-Lite interface when configuration is enabled."""
        intf_names = super().get_verilog_top_module_intf_names()
        if self.get_nodeattr("axilite_config") == 1:
            intf_names["axilite"] = ["{{ config_interface_name }}"]
        return intf_names
    {% endif %}
{% if has_implementation_styles %}
    
    def select_impl_style(self):
        """Select optimal implementation style for {{ kernel_name }}."""
        # Operation-specific implementation style selection logic
        current_style = self.get_nodeattr("impl_style")
        
        # Add operation-specific selection criteria here
        # Example: depth-based selection for FIFOs
        {% if kernel_name.lower().startswith('fifo') %}
        depth = self.get_nodeattr("depth")
        if depth > 1024:
            return "vivado"  # Use Vivado IP for large FIFOs
        {% endif %}
        
        return current_style
{% endif %}
{% if has_dynamic_config %}
    
    def get_dynamic_config(self):
        """Get dynamic configuration data for {{ kernel_name }}."""
        config = {}
        
        # Add operation-specific dynamic configuration
        # Example: runtime weight updates for convolution operations
        
        return config
{% endif %}