{#-
Phase 2 HWCustomOp Template with Runtime Parameter Extraction

This template generates AutoHWCustomOp subclasses that:
1. Extract runtime parameters from ONNX nodes
2. Define static interface metadata with validated symbolic BDIM
3. Create proper node attribute definitions
4. Integrate seamlessly with FINN

Key features:
- All RTL parameters become ONNX node attributes
- Runtime parameter extraction in __init__
- Static interface metadata with symbolic BDIM shapes
- Proper whitelist handling for default values
-#}
############################################################################
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT License.
#
# Auto-generated HWCustomOp for {{ kernel_name }}
# Generated from: {{ source_file }}
# Generation timestamp: {{ generation_timestamp }}
#
# PHASE 2: FINN INTEGRATION
# This HWCustomOp follows FINN's standard pattern with simple constructor
# and static interface metadata. Parameters are accessed via get_nodeattr().
############################################################################

from typing import List, Dict, Tuple, Any
import numpy as np
from qonnx.core.datatype import DataType

from brainsmith.dataflow.core import AutoHWCustomOp
from brainsmith.dataflow.core.interface_metadata import InterfaceMetadata
from brainsmith.dataflow.core.qonnx_types import DatatypeConstraintGroup
from brainsmith.dataflow.core.interface_types import InterfaceType
from brainsmith.dataflow.core.block_chunking import BlockChunkingStrategy


class {{ class_name }}(AutoHWCustomOp):
    """
    Auto-generated HWCustomOp for {{ kernel_name }} kernel.
    
    Generated from RTL: {{ source_file }}
    Follows FINN's standard HWCustomOp pattern with static interface metadata.
    
    RTL parameters are defined in get_nodeattr_types().
    """
    
    def __init__(self, onnx_node, **kwargs):
        """Initialize {{ class_name }} following FINN's standard pattern."""
        super().__init__(onnx_node, **kwargs)
        
        # Set kernel-specific attributes
        self.kernel_name = "{{ kernel_name }}"
        self.rtl_source = "{{ source_file }}"
    
    @staticmethod
    def get_interface_metadata() -> List[InterfaceMetadata]:
        """
        Return static interface metadata with validated symbolic BDIM shapes.
        
        All BDIM parameters have been validated during template generation
        to ensure they reference valid module parameters.
        """
        return [
            {% for interface in interface_metadata %}
            InterfaceMetadata(
                name="{{ interface.name }}",
                interface_type=InterfaceType.{{ interface.interface_type.name }},
                datatype_constraints=[
                    {% for constraint in interface.datatype_constraints %}
                    DatatypeConstraintGroup(
                        base_type="{{ constraint.base_type }}",
                        min_width={{ constraint.min_width }},
                        max_width={{ constraint.max_width }}
                    ),
                    {% endfor %}
                ],
                {% if interface.chunking_strategy %}
                chunking_strategy=BlockChunkingStrategy(
                    block_shape={{ interface.chunking_strategy.block_shape | repr }},  # Validated symbolic shape
                    rindex={{ interface.chunking_strategy.rindex }}
                )
                {% else %}
                chunking_strategy=None
                {% endif %}
            ),
            {% endfor %}
        ]
    
    def get_nodeattr_types(self) -> Dict[str, Tuple[str, bool, Any]]:
        """
        Define ONNX node attributes for all RTL parameters.
        
        Parameters with whitelisted defaults are optional, all others are required.
        """
        # Start with parent class attributes
        my_attrs = {}
        
        # RTL parameters as node attributes (only exposed parameters)
        {% for param in parameter_definitions %}
        {% if param.name in exposed_parameters %}
        {% if param.is_whitelisted and param.default_value %}
        my_attrs["{{ param.name }}"] = ("i", False, {{ param.default_value }})  # Optional with default
        {% else %}
        my_attrs["{{ param.name }}"] = ("i", True, None)  # Required parameter
        {% endif %}
        {% endif %}
        {% endfor %}
        
        # Interface datatype attributes (high-level datatype specification)
        {% for interface_attr in interface_datatype_attributes %}
        my_attrs["{{ interface_attr.name }}"] = {{ interface_attr.attr_spec }}  # {{ interface_attr.interface_type }} interface datatype
        {% endfor %}
        
        # Hardware-specific attributes from RTL analysis
        {% for attr_name, attr_spec in node_attributes.items() %}
        {% if attr_name not in parameter_definitions|map(attribute='name')|list %}
        my_attrs["{{ attr_name }}"] = {{ attr_spec }}
        {% endif %}
        {% endfor %}
        
        # Base HWCustomOp attributes
        my_attrs.update({
            "runtime_writeable_weights": ("i", False, 0, {0, 1}),
            "numInputVectors": ("ints", False, [1]),
        })
        
        # Update with parent class attributes (FINN base classes)
        my_attrs.update(super().get_nodeattr_types())
        return my_attrs
    
    # ===== Automatic Tensor Formatting Methods =====
    # These methods are automatically generated by the enhanced AutoHWCustomOp
    # using dataflow mathematics, eliminating the need for manual implementation
    
    {% if has_weight_interfaces %}
    def get_hw_compatible_weight_tensor(self, orig_weight_matrix):
        """
        Automatic weight tensor formatting via dataflow mathematics.
        
        This method uses the revolutionary DataflowModeling system to automatically
        generate hardware-optimized tensor layouts, achieving identical results to
        manual implementations while eliminating 200+ lines of error-prone code.
        
        Mathematical relationships:
        - WMEM = (tensor_dims[0] // block_dims[0]) * (tensor_dims[1] // block_dims[1])
        - Hardware layout: [1, wPar, WMEM, iPar]
        - PE distribution and SIMD optimization applied automatically
        """
        return super().get_hw_compatible_weight_tensor(orig_weight_matrix)
    {% endif %}
    
    {% if has_threshold_interfaces %}
    def get_hw_compatible_threshold_tensor(self, orig_thres_matrix):
        """
        Automatic threshold tensor formatting via dataflow mathematics.
        
        This method uses dataflow interface mathematics to automatically generate
        threshold tensor layouts based on output interface characteristics.
        
        Mathematical relationships:
        - TMEM = tensor_dims[0] // block_dims[0]
        - Hardware layout: [1, PE, TMEM, n_thres_steps]
        - Optimized for threshold operations
        """
        return super().get_hw_compatible_threshold_tensor(orig_thres_matrix)
    {% endif %}
    
    {% if has_memory_calculations %}
    def calc_wmem(self):
        """
        Auto-calculated WMEM from dataflow interface mathematics.
        
        Replaces manual calculation with automatic computation from
        dataflow interface relationships: num_blocks[0] * num_blocks[1]
        """
        return super().calc_wmem()
    
    def calc_tmem(self):
        """
        Auto-calculated TMEM from dataflow interface mathematics.
        
        Replaces manual calculation with automatic computation from
        dataflow interface relationships: num_blocks[0]
        """
        return super().calc_tmem()
    {% endif %}
    
    # Note: Legacy methods like calc_simd(), calc_pe(), etc. are automatically
    # derived from the dataflow model parallelism configuration
    
    # Note: Datatype methods handled by AutoHWCustomOp parent class
    # Parent class validates datatypes against constraint groups at runtime
    # Note: Shape calculation methods handled by AutoHWCustomOp parent class
    # Parent class computes shapes from DataflowModel interfaces automatically
    
    # Note: Stream width methods handled by AutoHWCustomOp parent class
    # Parent class calculates stream widths from datatypes and parallelism automatically
    
    {% if resource_estimation_required %}
    # ===== Resource Estimation Methods =====
    
    # Note: Cycle calculation and memory handling done by AutoHWCustomOp parent class
    
    {% if resource_estimation_methods.bram_estimation %}
    def bram_estimation(self) -> int:
        """Estimate BRAM usage for {{ kernel_name }}."""
        {{ resource_estimation_methods.bram_estimation|indent(8) }}
    {% endif %}
    
    {% if resource_estimation_methods.lut_estimation %}
    def lut_estimation(self) -> int:
        """Estimate LUT usage for {{ kernel_name }}."""
        {{ resource_estimation_methods.lut_estimation|indent(8) }}
    {% endif %}
    
    {% if resource_estimation_methods.dsp_estimation %}
    def dsp_estimation(self) -> int:
        """Estimate DSP usage for {{ kernel_name }}."""
        {{ resource_estimation_methods.dsp_estimation|indent(8) }}
    {% endif %}
    {% endif %}
    
    {% if verification_required %}
    def verify_node(self):
        """Verify kernel-specific constraints."""
        super().verify_node()
        
        # Verify all required parameters are present
        {% for param in required_attributes %}
        if self.get_nodeattr("{{ param }}") is None:
            raise ValueError(f"Required parameter '{{ param }}' not specified")
        {% endfor %}
        
        # Additional {{ kernel_name }}-specific verification
        # TODO: Add kernel-specific constraint checks
    {% endif %}


# Convenience function for FINN integration
def make_{{ kernel_name }}_node(inputs, outputs, **node_attrs):
    """
    Create {{ class_name }} ONNX node.
    
    Required algorithm parameters:
    {% for param in required_attributes %}
    {% if param in exposed_parameters %}
    - {{ param }}: int
    {% endif %}
    {% endfor %}
    
    Interface datatype attributes:
    {% for interface_attr in interface_datatype_attributes %}
    - {{ interface_attr.name }}: str = "{{ interface_attr.default_datatype }}"  # {{ interface_attr.interface_type }} interface datatype
    {% endfor %}
    
    Note: RTL-level parameters (width, signed, format, etc.) are automatically derived 
    from interface datatypes by the RTLBackend and should not be specified directly.
    
    Optional parameters (with defaults):
    {% for param_name, default_value in whitelisted_defaults.items() %}
    - {{ param_name }}: int = {{ default_value }}
    {% endfor %}
    """
    import onnx.helper
    
    # Validate required algorithm parameters (only exposed parameters)
    required_algorithm_params = [p for p in {{ required_attributes }} if p in {{ exposed_parameters }}]
    missing = [p for p in required_algorithm_params if p not in node_attrs]
    if missing:
        raise ValueError(f"Missing required algorithm parameters: {missing}")
    
    # Note: Interface datatype parameters are handled by RTLBackend, not node attributes
    
    return onnx.helper.make_node(
        "{{ class_name }}",
        inputs=inputs,
        outputs=outputs,
        domain="finn.custom_op.fpgadataflow",
        **node_attrs
    )