{# Jinja2 template for generating a FINN HWCustomOp Python class #}
# Auto-generated by Brainsmith HKG on {{ generation_timestamp }}
# HWCustomOp for {{ hw_kernel.name }}
# Description: {{ description }}

import os
import numpy as np
from qonnx.core.datatype import DataType
from qonnx.custom_op.general.custom_op import CustomOp # Adjust if your base class is different (e.g., HWCustomOp)
from qonnx.util.basic import roundup_to_integer_multiple

# User-defined imports (if any)
{{ user_imports }}

class {{ class_name }}(CustomOp): # Or HWCustomOp if that's the base
    """{{ description }}"""

    def __init__(self, onnx_node, **kwargs):
        super().__init__(onnx_node, **kwargs)
        # The hw_kernel_details attribute can be used to store a representation
        # of the HWKernel object for inspection or debugging purposes.
        # It's pretty-printed here for readability if included in the generated file.
        # self.hw_kernel_details = """{{ hw_kernel_repr | indent(8) }}"""
        # You can also initialize attributes directly from hw_kernel if needed:
        # self.my_kernel_param = self.get_nodeattr("my_rtl_param_name")

    def get_nodeattr_types(self):
        """Return a dictionary of attribute names and their types for this custom op."""
        my_attrs = {
            # Kernel-specific attributes derived from HWKernel.parameters
{%- if kernel_attributes %}
{%- for name, type_char, required, default_value_repr in kernel_attributes %}
            "{{ name }}": ("{{ type_char }}", {{ required }}, {{ default_value_repr }}),
{%- endfor %}
{%- endif %}
        }
        # Make sure to include attributes from the superclass
        my_attrs.update(super().get_nodeattr_types())
        return my_attrs

    # ---------------
    # Derived Methods (automatically generated based on HWKernel, can be overridden by user)
    # ---------------
{%- for method_name, method_code in derived_methods.items() %}
    {{ method_code | indent(4, first=True) }}
{%- endfor %}

    # ---------------
    # User-defined Methods (from compiler_data.py, adapted for this class)
    # ---------------
{%- for method_name, method_code in user_methods.items() %}
    {{ method_code | indent(4, first=True) }}
{%- endfor %}

    # ---------------
    # Static Methods (if any, potentially from compiler_data.py or defined by convention)
    # ---------------
{%- for method_name, method_code in static_methods.items() %}
    @staticmethod
    {{ method_code | indent(4, first=True) }}
{%- endfor %}

    # ---------------
    # Placeholder/Required CustomOp Methods (implement these based on your kernel's needs)
    # ---------------
{%- for method_name, method_details in placeholder_methods.items() %}
    def {{ method_name }}({{ method_details.signature }}):
        """{{ method_details.docstring }}"""
        {{- method_details.body | indent(4, first=True) }}
{%- endfor %}

    # ---------------
    # Core CustomOp Methods (default implementations, override if necessary)
    # ---------------
    def get_verilog_top_module_name(self):
        """Return the Verilog top module name."""
        # This usually matches the 'entity_name' attribute, which defaults to hw_kernel.name
        return self.get_nodeattr("entity_name")

    def execute_node(self, context, graph):
        """Execute this node, by calling the HLS/RTL execution flow (simulation or hardware)."""
        node = self.onnx_node

        # Basic input/output assertions based on HWKernel stream counts
        # These assume that the ONNX node inputs/outputs correspond directly to streams.
        # Adjust if your kernel has non-stream control inputs/outputs via ONNX.
        # Retrieve input tensor(s)
{%- for i in range(hw_kernel.num_input_streams) %}
        # assert DataType[inp{{i}}_data.dtype.name] == expected_dtype, f"Input {{i}} dtype mismatch: expected {expected_dtype}, got {DataType[inp{{i}}_data.dtype.name]}"
{%- endfor %}
        # Placeholder for output generation - replace with actual kernel logic and data
{%- if hw_kernel.num_output_streams == 0 and hw_kernel.num_input_streams == 0 %}
        pass
{%- endif %}
{%- for i in range(hw_kernel.num_output_streams) %}
        #    raise NotImplementedError(f"Output stream {i} (ONNX output: {node.output[{{i}}]) was not populated by execute_node.")
{%- endfor %}


    def infer_node_datatype(self, model):
{%- if hw_kernel.num_input_streams == 0 and hw_kernel.num_output_streams > 0 %}
        pass # Requires specific implementation if this case is valid
{%- endif %}


    def verify_node(self):
        pass # Ensure this method returns a list or generator of messages.

# Example of how a main block might look for testing the generated class
# This part would not be included in the final generated file by default.
# {% if add_main_block_for_testing -%}
# if __name__ == "__main__":
#     # This is a sample main block for testing the {{ class_name }} class.
#     # You would need to create a dummy ONNX node and potentially a model context.
#     
#     # Example: Create a dummy ONNX node
#     from onnx import helper
#     dummy_onnx_node = helper.make_node(
#         "{{ class_name }}", # Op type
#         inputs=["input_tensor"],
#         outputs=["output_tensor"],
#         domain="finn.custom_op.general", # Or your custom domain
#         # Add attributes matching those in get_nodeattr_types
#         backend="rtl",
#         entity_name="{{ hw_kernel.name }}",
#     {%- for name, type_char, required, default_value_repr in kernel_attributes %}
#         {{ name }}={{ default_value_repr }}, # Assuming default_value_repr is a valid Python literal
#     {%- endfor %}
#     )
# 
#     # Instantiate the custom op
#     custom_op_instance = {{ class_name }}(dummy_onnx_node)
# 
#     print(f"Successfully instantiated {{ class_name }} for kernel {{ hw_kernel.name }}")
#     print("Node attributes:")
#     for attr_name, attr_val in custom_op_instance.onnx_node.attribute.items():
#         print(f"  {attr_name}: {helper.get_attribute_value(attr_val)}")
# 
#     print("\nVerification messages:")
#     # verify_node returns a generator, so iterate over it
#     verification_successful = True
#     for msg_type, msg_args in custom_op_instance.verify_node():
#         print(f"  {msg_type}: {msg_args}")
#         if msg_type == "ERROR":
#             verification_successful = False
#     print(f"Verification successful: {verification_successful}")
# 
#     # TODO: Add calls to other methods like get_input_datatype, execute_node (with dummy context)
#     # print(f"Input datatype (stream 0): {custom_op_instance.get_input_datatype(0)}")
#     # print(f"Folded input shape (stream 0): {custom_op_instance.get_folded_input_shape(0)}")
# 
#     # Note: execute_node would require a context dictionary and a graph object.
#     # dummy_context = {"input_tensor": np.random.rand(*custom_op_instance.get_normal_input_shape(0)).astype(np.float32)}
#     # dummy_graph = None # Or a mock graph object
#     # try:
#     #     custom_op_instance.execute_node(dummy_context, dummy_graph)
#     #     print("execute_node called (actual execution depends on implementation). Output:", dummy_context.get("output_tensor"))
#     # except NotImplementedError as e:
#     #     print(f"execute_node not fully implemented: {e}")
# 
# {% endif %}