{#-
Slim HWCustomOp Template for Runtime-Configurable Hardware Components

This template generates compact HWCustomOp classes that use runtime dimension
extraction from ModelWrapper instead of static hardcoded dimensions.

CRITICAL: This template generates components for future use by the FINN compiler.
The HWCustomOp should NEVER set static num_tensors, tDim, or stream_dims values.
Instead, these dimensions are extracted at runtime when the FINN compiler
instantiates the HWCustomOp with actual tensor shapes from the ModelWrapper.
-#}
############################################################################
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT License.
#
# Auto-generated HWCustomOp for {{ kernel_name }}
# Generated from: {{ source_file }}
# Generation timestamp: {{ generation_timestamp }}
#
# RUNTIME-CONFIGURABLE HARDWARE COMPONENT
# This HWCustomOp uses runtime dimension extraction from ModelWrapper.
# NEVER set static num_tensors, tDim, or stream_dims values in generated code.
############################################################################

import numpy as np
from brainsmith.dataflow.core.auto_hw_custom_op import AutoHWCustomOp
from brainsmith.dataflow.core.interface_metadata import InterfaceMetadata, DataTypeConstraint
from brainsmith.dataflow.core.dataflow_interface import DataflowInterfaceType
from brainsmith.dataflow.core.tensor_chunking import default_chunking


class {{ class_name }}(AutoHWCustomOp):
    """
    Slim auto-generated HWCustomOp for {{ kernel_name }} kernel.
    
    Generated from RTL: {{ source_file }}
    Uses enhanced TDIM pragma integration for automatic chunking strategies.
    """
    
    def __init__(self, onnx_node, **kwargs):
        """Initialize {{ class_name }} with interface metadata and chunking strategies."""
        
        # Define interface metadata with data layout-based chunking
        # Chunking is determined automatically from ONNX tensor layout (NCHW, NHWC, NLC, etc.)
        # Only include AXI_STREAM interfaces in dataflow model (AXI_LITE will be handled separately)
        self._interface_metadata = [
            {% for interface in interfaces %}
            {% if interface.type.name == 'AXI_STREAM' %}
            InterfaceMetadata(
                name="{{ interface.name }}",
                interface_type=DataflowInterfaceType.{{ interface.dataflow_type }},
                allowed_datatypes=[
                    {% for constraint in interface.datatype_constraints %}
                    DataTypeConstraint(
                        finn_type="{{ constraint.finn_type }}",
                        bit_width={{ constraint.bit_width }},
                        signed={{ constraint.signed }}
                    ),
                    {% endfor %}
                ],
                chunking_strategy=default_chunking()  # Data layout-based chunking determined at runtime
            ),
            {% endif %}
            {% endfor %}
        ]
        
        # Initialize parent with interface metadata
        super().__init__(onnx_node, interface_metadata=self._interface_metadata, **kwargs)
        
        # Set kernel-specific attributes
        self.kernel_name = "{{ kernel_name }}"
        self.rtl_source = "{{ source_file }}"
    
    def get_nodeattr_types(self):
        """Define kernel-specific node attributes."""
        attrs = super().get_nodeattr_types()
        
        # Add RTL parameters as node attributes
        kernel_attrs = {
            {% for param in rtl_parameters %}
            "{{ param.name }}": ("i", False, {{ param.default_value or 0 }}),
            {% endfor %}
        }
        
        attrs.update(kernel_attrs)
        return attrs
    
    def determine_chunking_from_layout(self, interface_name, tensor_shape, onnx_layout):
        """
        Determine qDim and tDim from ONNX tensor layout.
        
        Based on Interface-Wise Dataflow Modeling specification:
        - [N, C]: qDim=1, tDim=C
        - [N, C, H, W]: qDim=C, tDim=H*W  
        - [N, H, W, C]: qDim=H*W, tDim=C
        - [N, L, C]: qDim=L, tDim=C
        - [N, C, L]: qDim=C, tDim=L
        - [N, L, h, d]: qDim=L, tDim=h*d
        
        For weight interfaces:
        - 1D weights: qDim=1, tDim=length
        - 2D weights: qDim=second_dim, tDim=first_dim
        """
        if onnx_layout == "[N, C]":
            N, C = tensor_shape
            return {"qDim": 1, "tDim": C, "chunk_dimension": None}
        elif onnx_layout == "[N, C, H, W]":
            N, C, H, W = tensor_shape
            return {"qDim": C, "tDim": H * W, "chunk_dimension": 1}
        elif onnx_layout == "[N, H, W, C]":
            N, H, W, C = tensor_shape
            return {"qDim": H * W, "tDim": C, "chunk_dimension": 2}
        elif onnx_layout == "[N, L, C]":
            N, L, C = tensor_shape
            return {"qDim": L, "tDim": C, "chunk_dimension": 1}
        elif onnx_layout == "[N, C, L]":
            N, C, L = tensor_shape
            return {"qDim": C, "tDim": L, "chunk_dimension": 1}
        elif onnx_layout == "[N, L, h, d]":
            N, L, h, d = tensor_shape
            return {"qDim": L, "tDim": h * d, "chunk_dimension": 1}
        else:
            # Default fallback - treat as single chunk
            return {"qDim": 1, "tDim": np.prod(tensor_shape[1:]), "chunk_dimension": None}
    
    def get_kernel_interface_specs(self):
        """Return interface specifications for kernel integration."""
        return {
            {% for interface in interfaces %}
            {% if interface.type.name == 'AXI_STREAM' %}
            "{{ interface.name }}": {
                "type": "{{ interface.dataflow_type.lower() }}",
                "interface_type": "{{ interface.type.name }}",
                "chunking_strategy": "data_layout_chunking",  # Determined from ONNX tensor layout
                "layout_detection": "automatic",  # qDim/tDim calculated from [N,C,H,W], [N,L,C], etc.
            },
            {% endif %}
            {% endfor %}
        }
    
    {% if resource_estimation_required %}
    # ===== Kernel-Specific Resource Estimation =====
    
    def bram_estimation(self) -> int:
        """Estimate BRAM usage for {{ kernel_name }}."""
        # TODO: Implement based on {{ kernel_name }} architecture
        {% if weight_interfaces_count > 0 %}
        # Weight storage + activation buffers
        return {{ weight_interfaces_count * 2 + 1 }}  # Placeholder
        {% else %}
        # Minimal usage for compute-only kernel
        return 1
        {% endif %}
    
    def lut_estimation(self) -> int:
        """Estimate LUT usage for {{ kernel_name }}."""
        # TODO: Implement based on {{ kernel_name }} logic complexity
        parallelism = self._get_current_parallelism()
        base_luts = {{ 500 if kernel_complexity == 'low' else 1500 if kernel_complexity == 'medium' else 3000 }}
        return base_luts * sum(parallelism.iPar.values())
    
    def dsp_estimation(self) -> int:
        """Estimate DSP usage for {{ kernel_name }}."""
        {% if kernel_type in ['matmul', 'conv', 'gemm'] %}
        # Arithmetic-intensive kernel likely uses DSPs
        parallelism = self._get_current_parallelism()
        return sum(parallelism.wPar.values())  # One DSP per weight parallel unit
        {% else %}
        # Non-arithmetic kernel typically doesn't use DSPs
        return 0
        {% endif %}
    {% endif %}
    
    {% if verification_required %}
    def verify_node(self):
        """Verify kernel-specific constraints."""
        super().verify_node()
        
        # Add {{ kernel_name }}-specific verification
        {% for verification in kernel_verifications %}
        # {{ verification.description }}
        {{ verification.code }}
        {% endfor %}
    {% endif %}


# Convenience function for FINN integration
def make_{{ kernel_name }}_node(inputs, outputs, **node_attrs):
    """Create {{ class_name }} ONNX node with enhanced TDIM pragma support."""
    import onnx.helper
    
    return onnx.helper.make_node(
        "{{ class_name }}",
        inputs=inputs,
        outputs=outputs,
        domain="finn.custom_op.fpgadataflow",
        **node_attrs
    )