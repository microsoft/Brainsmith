{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#brainsmith","title":"Brainsmith","text":""},{"location":"#compile-neural-networks-to-fpga-accelerators","title":"Compile Neural Networks to FPGA Accelerators","text":"<p>Brainsmith is an end-to-end compiler to transform ONNX models into dataflow accelerators for FPGAs. Through design space exploration, it evaluates hardware configurations to find the optimal configuration for your use case.</p> ONNX Model <p></p>  \u2192  Dataflow Core <p></p> <p>Automated RTL generation from ONNX models. Design space exploration to identify optimal configurations.</p> <p>Get Started View on GitHub</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li> <p> Automatic Design Space Exploration</p> <p>Navigate parallelization factors, resource allocation, and architectural choices. Explore multiple configurations to identify promising designs.</p> </li> <li> <p> Schema-Driven Kernel Development</p> <p>Define hardware semantics declaratively. Validation, design space construction, and interface generation are derived from schema definitions.</p> </li> <li> <p> Synthesizable RTL Generation</p> <p>Generate Verilog/VHDL with standard AXI-Stream interfaces. Compatible with Vivado IP Integrator workflows.</p> </li> <li> <p> Growing Kernel Library</p> <p>Built-in support for MatMul, LayerNorm, Softmax, and other common operations. Extensible architecture for adding custom kernels.</p> </li> <li> <p> Performance Estimation</p> <p>Resource estimation, cycle-accurate simulation support, and throughput analysis. Evaluate design tradeoffs before synthesis.</p> </li> <li> <p> Multi-Layer Offload</p> <p>Scale to large models with constant FPGA resources. Stream weights from external memory to process arbitrarily deep networks without increasing hardware footprint.</p> </li> </ul>"},{"location":"#basic-usage","title":"Basic Usage","text":"<p>Generate an accelerator with a single command:</p> <pre><code># Design space exploration and RTL generation\nsmith model.onnx blueprint.yaml\n\n# Output: RTL + performance estimates + resource reports\n</code></pre>"},{"location":"#example-bert-accelerator","title":"Example: BERT Accelerator","text":"<pre><code># blueprint.yaml - Define your design space\nname: \"BERT Accelerator\"\nclock_ns: 5.0  # 200MHz target\n\ndesign_space:\n  kernels:\n    - MVAU           # Matrix-vector operations\n    - LayerNorm      # Layer normalization\n    - Softmax        # Attention softmax\n\n  steps:\n    - \"streamline\"           # Graph optimization\n    - \"infer_kernels\"        # Hardware kernel mapping\n    - \"specialize_layers\"    # Backend selection\n    - \"dataflow_partition\"   # Multi-layer offload\n</code></pre> <p>Run design space exploration:</p> <pre><code>smith bert.onnx blueprint.yaml --output-dir ./results\n</code></pre> <p>Results include:</p> <ul> <li>Synthesizable RTL in <code>results/stitched_ip/</code></li> <li>Performance estimates in <code>results/report/estimate_reports.json</code></li> <li>Detailed build logs for debugging</li> </ul> <p>The example targets V80 platform using Vivado 2024.2 and is compatible with Xilinx Zynq/Ultrascale+ platforms.</p> <p>See examples/bert for full implementation</p>"},{"location":"#open-source-collaborative","title":"Open Source &amp; Collaborative","text":"<p>Brainsmith is MIT-licensed and builds upon a foundation of proven open-source tools:</p> <ul> <li>FINN - Dataflow compiler for quantized neural networks</li> <li>QONNX - Quantized ONNX representation</li> <li>Brevitas - PyTorch quantization library</li> </ul> <p>Brainsmith extends FINN with automated design space exploration, blueprint inheritance, and a schema-driven kernel system. FINN provides the low-level RTL generation and QONNX transformations.</p> <p>Developed through collaboration between Microsoft and AMD.</p> <p>License: MIT - see LICENSE</p>"},{"location":"#community-support","title":"Community &amp; Support","text":"<ul> <li>Feature Roadmap - See what's planned and in progress</li> <li>GitHub Issues - Report bugs or request features</li> <li>GitHub Discussions - Ask questions and share experiences</li> </ul>"},{"location":"404/","title":"404","text":"404 Page Not Found <p>The page you're looking for doesn't exist. It may have been moved, deleted, or the URL might be incorrect.</p>        Return Home      <p>     Need help? Visit GitHub Discussions or open an issue.   </p>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#getting-started","title":"Getting Started","text":""},{"location":"getting-started/#installation","title":"Installation","text":"<p>Prerequisites</p> <ul> <li>Ubuntu 22.04+ (primary development/testing platform)</li> <li>Vivado Design Suite 2024.2 (migration to 2025.1 in process)</li> <li>[Optional] Cmake for BERT example V80 shell integration</li> </ul> <pre><code>git clone https://github.com/microsoft/brainsmith.git ./brainsmith\ncd brainsmith\n</code></pre>"},{"location":"getting-started/#option-a-local-development-with-poetry","title":"(Option A): Local Development with Poetry","text":"<p>Prerequisites</p> <ul> <li>Python 3.11+ and Poetry</li> <li>[Optional] direnv for automatic environment activation</li> </ul> <p>Run automated setup script</p> <pre><code>./setup-venv.sh\n</code></pre> <p>Edit the project configuration file to customize project settings</p> <pre><code>vim brainsmith.yaml\n</code></pre> <p>Activate environment manually or with direnv</p> <pre><code>source .venv/bin/activate &amp;&amp; source .brainsmith/env.sh\n# If using direnv, just reload directory\ncd .\n</code></pre> <p>Query project settings to confirm your configuration is loaded correctly</p> <pre><code>brainsmith project info\n</code></pre>"},{"location":"getting-started/#option-b-docker-based-development","title":"(Option B): Docker-based Development","text":"<p>Prerequisites</p> <ul> <li>Docker configured to run without root</li> </ul> <p>Edit <code>ctl-docker.sh</code> or set environment variables to directly set brainsmith project settings</p> <pre><code>export BSMITH_XILINX_PATH=/tools/Xilinx/\nexport BSMITH_XILINX_VERSION=2024.2\n</code></pre> <p>Start container</p> <pre><code>./ctl-docker.sh start\n</code></pre> <p>Open an interactive shell to check your configuration</p> <pre><code>./ctl-docker.sh shell\nbrainsmith project info\n</code></pre> <p>Or send one-off commands to the container</p> <pre><code>./ctl-docker.sh \"brainsmith project info\"\n</code></pre>"},{"location":"getting-started/#command-line-interface","title":"Command-Line Interface","text":"<p>Brainsmith provides two CLIs:</p> <p><code>smith</code> - Streamlined CLI for creating dataflow accelerators: </p><pre><code>smith model.onnx blueprint.yaml    # Create dataflow accelerator\n</code></pre><p></p> <p><code>brainsmith</code> - Full toolkit with administrative commands: </p><pre><code>brainsmith project init                # Initialize project\nbrainsmith registry                    # List registered components\nbrainsmith setup cppsim                # Setup C++ simulation\n</code></pre><p></p> <p>All commands support <code>--help</code> for details. See CLI Reference for complete documentation.</p>"},{"location":"getting-started/#project-management","title":"Project Management","text":"<p>Brainsmith operates from a single poetry <code>venv</code> from the repository root, but you can create isolated workspaces via the project system with independent configurations, build directories, and component registries.</p>"},{"location":"getting-started/#creating-projects","title":"Creating Projects","text":"<pre><code># Activate brainsmith venv if not in an active project\nsource /path/to/brainsmith/.venv/bin/activate\n\n# Create and initialize new project directory\nbrainsmith project init ~/my-fpga-project\ncd ~/my-fpga-project\n</code></pre> <p>Edit the default generated config file</p> <pre><code>vim ~/my-fpga-project/brainsmith.yaml\n</code></pre> <p>[Optional] Enable auto-activation if using direnv</p> <pre><code>brainsmith project allow-direnv\ncd .  # Triggers direnv\n</code></pre> <p>Otherwise, refresh env after any config changes:</p> <pre><code>source .brainsmith/env.sh\n</code></pre>"},{"location":"getting-started/#configuration","title":"Configuration","text":"<p>Project settings are loaded from multiple sources with the following priority (highest to lowest) for deep user control, but the recommended interface is the yaml config file using CLI arguments to override as necessary.</p> <ol> <li>CLI arguments - Passed to <code>load_config()</code> or command-line tools</li> <li>Environment variables - <code>BSMITH_*</code> prefix (e.g., <code>BSMITH_BUILD_DIR</code>)</li> <li>Project config file - <code>brainsmith.yaml</code> in project root</li> <li>Built-in defaults - Field defaults in <code>SystemConfig</code></li> </ol> <p>Relative Path Resolution</p> <ul> <li>CLI args: Resolve from current working directory</li> <li>YAML/env: Resolve from project root</li> </ul>"},{"location":"getting-started/#brainsmith-settings","title":"Brainsmith Settings","text":"Field Type Default Description <code>build_dir</code> <code>Path</code> <code>\"build\"</code> Build directory for compilation artifacts. Relative paths resolve to <code>project_dir</code>. <code>component_sources</code> <code>Dict[str, Path | None]</code> <code>{'project': None}</code> Filesystem-based component source paths. <code>'project'</code> defaults to <code>project_dir</code> (supports <code>kernels/</code> and <code>steps/</code> subdirectories). Core namespace (<code>'brainsmith'</code>) and entry points (<code>'finn'</code>) are loaded automatically. <code>source_priority</code> <code>List[str]</code> <code>['project', 'brainsmith', 'finn', 'custom']</code> Component source resolution priority (first match wins). Custom sources are auto-appended if not listed. <code>source_module_prefixes</code> <code>Dict[str, str]</code> <code>{'brainsmith.': 'brainsmith', 'finn.': 'finn'}</code> Module prefix \u2192 source name mapping for component classification. <code>components_strict</code> <code>bool</code> <code>True</code> Enable strict component loading (fail on errors vs. warn). Set to <code>false</code> for development. <code>cache_components</code> <code>bool</code> <code>True</code> Enable manifest caching for component discovery."},{"location":"getting-started/#external-tool-settings","title":"External Tool Settings","text":"Field Type Default Description <code>netron_port</code> <code>int</code> <code>8080</code> Port for Netron neural network visualization server. <code>xilinx_path</code> <code>Path</code> <code>\"/tools/Xilinx\"</code> Xilinx root installation path. <code>xilinx_version</code> <code>str</code> <code>\"2024.2\"</code> Xilinx tool version (e.g., <code>\"2024.2\"</code>, <code>\"2025.1\"</code>). <code>vivado_ip_cache</code> <code>Path \\| None</code> <code>{build_dir}/vivado_ip_cache</code> Vivado IP cache directory for faster builds. <code>vendor_platform_paths</code> <code>str</code> <code>\"/opt/xilinx/platforms\"</code> Colon-separated vendor platform repository paths."},{"location":"getting-started/#runtime-configuration","title":"Runtime Configuration","text":"Field Type Default Description <code>default_workers</code> <code>int</code> <code>4</code> Default number of workers for parallel operations. Exported as <code>NUM_DEFAULT_WORKERS</code>. <code>logging.level</code> <code>str</code> <code>\"normal\"</code> Console verbosity: <code>quiet</code> | <code>normal</code> | <code>verbose</code> | <code>debug</code>. <code>logging.finn_tools</code> <code>Dict[str, str]</code> <code>None</code> Per-tool log levels for FINN tools (e.g., <code>{'vivado': 'WARNING', 'hls': 'INFO'}</code>). <code>logging.suppress_patterns</code> <code>List[str]</code> <code>None</code> Regex patterns to suppress from console output (file logs unaffected). <code>logging.max_log_size_mb</code> <code>int</code> <code>0</code> Maximum log file size in MB (0 = no rotation). <code>logging.keep_backups</code> <code>int</code> <code>3</code> Number of rotated log backups to keep. <p>Additional configuration fields (FINN settings, direct Xilinx tool paths, etc.) can be set directly, are recommended to let auto-configure from the core brainsmith fields.</p>"},{"location":"getting-started/#running-design-space-exploration","title":"Running Design Space Exploration","text":""},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<p>Make sure you've completed the installation above and activated your environment:</p> <pre><code># Option 1: direnv users\ncd /path/to/brainsmith\n\n# Option 2: Manual activation\nsource .venv/bin/activate &amp;&amp; source .brainsmith/env.sh\n</code></pre>"},{"location":"getting-started/#run-your-first-dse","title":"Run Your First DSE","text":""},{"location":"getting-started/#1-navigate-to-the-bert-example","title":"1. Navigate to the BERT Example","text":"<pre><code>cd examples/bert\n</code></pre>"},{"location":"getting-started/#2-run-the-quick-test","title":"2. Run the Quick Test","text":"<pre><code>./quicktest.sh\n</code></pre> <p>This automated script will:</p> <ol> <li>Generate a quantized ONNX model - Single-layer BERT for rapid testing</li> <li>Generate a folding configuration - Minimal resource usage</li> <li>Build the dataflow accelerator - RTL generation with FINN</li> <li>Run RTL simulation - Verify correctness</li> </ol> <p>Build Time</p> <p>The quicktest takes approximately 30-60 minutes, depending on your system. The build process involves:</p> <ul> <li>ONNX model transformations</li> <li>Kernel inference and specialization</li> <li>RTL code generation</li> <li>IP packaging with Vivado</li> <li>RTL simulation</li> </ul>"},{"location":"getting-started/#3-monitor-progress","title":"3. Monitor Progress","text":"<p>The script outputs detailed progress to the console and log files. The build process transforms your model through several stages:</p> <p>Transformation Pipeline: </p><pre><code>PyTorch \u2192 ONNX \u2192 Hardware Kernels \u2192 HLS/RTL \u2192 IP Cores \u2192 Bitfile\n</code></pre><p></p> <p>Key stages you'll see:</p> <ul> <li>Model transformation: Converting ONNX operations to hardware kernels</li> <li>Design space exploration: Determining parallelization factors (PE/SIMD)</li> <li>Code generation: Generating HLS C++ and RTL (Verilog/VHDL)</li> <li>IP packaging: Creating Vivado IP cores</li> <li>Simulation: Verifying correctness with RTL simulation</li> </ul> <p>Check <code>build/quicktest/brainsmith.log</code> for detailed progress and diagnostics.</p>"},{"location":"getting-started/#explore-results","title":"Explore Results","text":"<p>Results are saved in <code>examples/bert/quicktest/</code>:</p> <pre><code>quicktest/\n\u251c\u2500\u2500 model.onnx              # Quantized ONNX model\n\u251c\u2500\u2500 final_output/           # Generated RTL and reports\n\u2502   \u251c\u2500\u2500 stitched_ip/       # Synthesizable RTL\n\u2502   \u2502   \u251c\u2500\u2500 finn_design_wrapper.v\n\u2502   \u2502   \u2514\u2500\u2500 *.v\n\u2502   \u2514\u2500\u2500 report/            # Performance estimates\n\u2502       \u2514\u2500\u2500 estimate_reports.json\n\u2514\u2500\u2500 build_dataflow.log     # Detailed build log\n</code></pre>"},{"location":"getting-started/#understanding-the-output","title":"Understanding the Output","text":""},{"location":"getting-started/#performance-report","title":"Performance Report","text":"<p>Check <code>final_output/report/estimate_reports.json</code>:</p> <pre><code>{\n  \"critical_path_cycles\": 123,\n  \"max_cycles\": 456,\n  \"estimated_throughput_fps\": 1234.5,\n  \"resources\": {\n    \"LUT\": 12345,\n    \"FF\": 23456,\n    \"BRAM_18K\": 34,\n    \"DSP48\": 56\n  }\n}\n</code></pre>"},{"location":"getting-started/#rtl-output","title":"RTL Output","text":"<p>The generated RTL is in <code>final_output/stitched_ip/</code>:</p> <ul> <li><code>finn_design_wrapper.v</code> \u2014 Top-level wrapper with AXI stream interfaces</li> <li>Individual kernel implementations (e.g., <code>MVAU_hls_*.v</code>, <code>Thresholding_rtl_*.v</code>)</li> <li>Stream infrastructure (FIFOs, width converters, etc.)</li> </ul>"},{"location":"getting-started/#customize-the-design","title":"Customize the Design","text":"<p>Now that you've run the basic example, try customizing it:</p>"},{"location":"getting-started/#adjust-target-performance","title":"Adjust Target Performance","text":"<p>Edit <code>bert_quicktest.yaml</code> to increase throughput:</p> <pre><code>finn_config:\n  target_fps: 10  # Increase from 1 to 10 FPS\n</code></pre> <p>Higher target FPS will:</p> <ul> <li>Increase parallelization factors (PE/SIMD parameters)</li> <li>Use more FPGA resources (LUTs, DSPs, BRAM)</li> <li>Reduce latency per inference</li> </ul>"},{"location":"getting-started/#create-custom-configurations","title":"Create Custom Configurations","text":"<p>Create your own blueprint that inherits from the BERT base:</p> <pre><code># my_custom_bert.yaml\nname: \"My Custom BERT\"\nextends: \"${BSMITH_DIR}/examples/blueprints/bert.yaml\"\n\nclock_ns: 4.0           # 250MHz (faster clock)\noutput: \"estimates\"     # Just get resource estimates\n\nfinn_config:\n  target_fps: 5000      # Very high throughput\n</code></pre> <p>Then run it:</p> <pre><code>python bert_demo.py -o my_output -l 2 --blueprint my_custom_bert.yaml\n</code></pre>"},{"location":"getting-started/#run-full-bert-demo","title":"Run Full BERT Demo","text":"<p>The full demo processes larger models. From <code>examples/bert/</code>:</p> <pre><code># Generate standard folding configuration\npython gen_folding_config.py --simd 16 --pe 16 -o configs/demo_folding.json\n\n# Run with 4 layers instead of 1\npython bert_demo.py -o bert_demo_output -l 4 --blueprint bert_demo.yaml\n</code></pre> <p>This creates a more realistic accelerator but takes significantly longer to build.</p>"},{"location":"getting-started/#understanding-blueprints","title":"Understanding Blueprints","text":"<p>A Blueprint is a YAML configuration that defines your hardware design space - the kernels, transformation steps, and build parameters.</p>"},{"location":"getting-started/#blueprint-inheritance","title":"Blueprint Inheritance","text":"<p>Blueprints support inheritance via the <code>extends</code> key, allowing you to build on existing configurations:</p> <pre><code># bert_quicktest.yaml - Quick test configuration\nname: \"BERT Quicktest\"\nextends: \"${BSMITH_DIR}/examples/bert/bert_demo.yaml\"\n\noutput: \"bitfile\"\n\nfinn_config:\n  target_fps: 1                     # Low FPS for quick testing\n  folding_config_file: \"configs/quicktest_folding.json\"\n  fifosim_n_inferences: 2           # Faster FIFO sizing\n</code></pre>"},{"location":"getting-started/#getting-help","title":"Getting Help","text":"<p>Build logs: Check <code>&lt;output_dir&gt;/brainsmith.log</code> for detailed error messages and transformation steps.</p> <p>Resources:</p> <ul> <li>GitHub Issues - Report bugs or search existing issues</li> <li>GitHub Discussions - Ask questions and share experiences</li> </ul>"},{"location":"api/","title":"Overview","text":""},{"location":"api/#api-reference","title":"API Reference","text":"<p>Complete API documentation for Brainsmith's public interfaces.</p>"},{"location":"api/#modules","title":"Modules","text":"<ul> <li>CLI Reference - Command-line interface for project setup and design generation</li> <li>Design Space Exploration - Explore hardware configurations</li> <li>Dataflow Modeling - Kernel operators, design spaces, and schema definitions</li> <li>Component Registry - Register and discover kernels, backends, and pipeline steps</li> <li>Settings - Configuration management</li> </ul>"},{"location":"api/cli/","title":"CLI","text":""},{"location":"api/cli/#cli-reference","title":"CLI Reference","text":"<p>Dual command-line interface: <code>brainsmith</code> for project management (setup, configuration), and <code>smith</code> for hardware design generation (DFC creation).</p>"},{"location":"api/cli/#global-options","title":"Global Options","text":"Option Type Default Description <code>-b, --build-dir</code> Path <code>build/</code> Override build directory <code>-c, --config</code> Path <code>brainsmith.yaml</code> Override configuration file <code>-l, --log-level</code> Choice <code>normal</code> Set log verbosity (<code>quiet</code>, <code>normal</code>, <code>verbose</code>, <code>debug</code>) <code>--no-progress</code> Flag - Disable progress spinners and animations <code>--version</code> Flag - Show version and exit <code>-h, --help</code> Flag - Show help message and exit"},{"location":"api/cli/#operational-commands","title":"Operational Commands","text":""},{"location":"api/cli/#dfc","title":"dfc","text":"<p>Create a dataflow core accelerator for neural network acceleration.</p> <p>Syntax:</p> <pre><code>smith MODEL BLUEPRINT [OPTIONS]\n</code></pre> <p>Arguments:</p> Argument Type Description <code>MODEL</code> Path Path to ONNX model file <code>BLUEPRINT</code> Path Path to Blueprint YAML file defining the dataflow architecture <p>Options:</p> Option Type Default Description <code>-o, --output-dir</code> Path <code>build/{timestamp}</code> Output directory for generated files <code>--start-step</code> Text - Override blueprint start_step (start execution from this step, inclusive) <code>--stop-step</code> Text - Override blueprint stop_step (stop execution at this step, inclusive) <p>Example:</p> <pre><code># Basic usage\nsmith model.onnx blueprint.yaml\n\n# Custom output directory\nsmith model.onnx blueprint.yaml --output-dir ./results\n\n# Run specific step range\nsmith model.onnx blueprint.yaml \\\n  --start-step streamline \\\n  --stop-step specialize_layers\n</code></pre> <p>See also: Blueprint Schema, Design Space Exploration</p>"},{"location":"api/cli/#administrative-commands","title":"Administrative Commands","text":""},{"location":"api/cli/#project","title":"project","text":"<p>Manage Brainsmith projects and configuration.</p> <p>Syntax:</p> <pre><code>brainsmith project &lt;SUBCOMMAND&gt; [OPTIONS]\n</code></pre> <p>Subcommands:</p> Subcommand Description <code>init</code> Initialize project with configuration and environment scripts <code>info</code> Display current project configuration <code>allow-direnv</code> Enable direnv integration for automatic environment activation"},{"location":"api/cli/#project-init","title":"project init","text":"<p>Initialize a Brainsmith project with configuration file and environment scripts.</p> <p>Syntax:</p> <pre><code>brainsmith project init [PATH] [OPTIONS]\n</code></pre> <p>Arguments:</p> Argument Type Default Description <code>PATH</code> Path <code>.</code> Directory to initialize (current directory if not specified) <p>Options:</p> Option Type Description <code>-f, --force</code> Flag Overwrite existing <code>brainsmith.yaml</code> if present <p>Behavior:</p> <ul> <li>Creates <code>brainsmith.yaml</code> configuration file</li> <li>Creates <code>.brainsmith/</code> directory for project metadata</li> <li>Generates <code>env.sh</code> activation script</li> <li>Generates <code>.envrc</code> for direnv support</li> </ul> <p>Example:</p> <pre><code># Initialize current directory\nbrainsmith project init\n\n# Create and initialize new project\nbrainsmith project init ./my-fpga-project\n\n# Overwrite existing configuration\nbrainsmith project init --force\n</code></pre>"},{"location":"api/cli/#project-info","title":"project info","text":"<p>Display current project configuration with source information.</p> <p>Syntax:</p> <pre><code>brainsmith project info [OPTIONS]\n</code></pre> <p>Options:</p> Option Type Description <code>--finn</code> Flag Include FINN-specific configuration settings <p>Output:</p> <ul> <li>Configuration metadata (project directory, environment status)</li> <li>Core paths (build directory, dependencies directory)</li> <li>Component registry settings</li> <li>Toolchain configuration</li> <li>Xilinx tools paths</li> <li>FINN configuration (with <code>--finn</code> flag)</li> </ul> <p>Example:</p> <pre><code># Show project configuration\nbrainsmith project info\n\n# Include FINN-specific settings\nbrainsmith project info --finn\n</code></pre>"},{"location":"api/cli/#project-allow-direnv","title":"project allow-direnv","text":"<p>Enable direnv integration for automatic environment activation when entering project directory.</p> <p>Syntax:</p> <pre><code>brainsmith project allow-direnv\n</code></pre> <p>Behavior:</p> <ul> <li>Verifies direnv is installed</li> <li>Generates <code>.envrc</code> file if needed</li> <li>Executes <code>direnv allow</code> to trust the configuration</li> <li>Validates shell hook is configured</li> </ul> <p>Example:</p> <pre><code>brainsmith project allow-direnv\n</code></pre>"},{"location":"api/cli/#registry","title":"registry","text":"<p>Display registered components (kernels, backends, pipeline steps) organized by source.</p> <p>Syntax:</p> <pre><code>brainsmith registry [OPTIONS]\n</code></pre> <p>Options:</p> Option Type Description <code>-v, --verbose</code> Flag Show detailed component information with full listings <code>-r, --rebuild</code> Flag Rebuild component cache and validate all entries (slower but thorough) <p>Output:</p> <ul> <li>Component sources table (source, type, path, status)</li> <li>Component summary by source (steps, kernels, backends counts)</li> <li>With <code>--verbose</code>: Detailed listings organized by component type</li> <li>With <code>--rebuild</code>: Validation results for all discovered components</li> </ul> <p>Example:</p> <pre><code># Quick listing using cached registry\nbrainsmith registry\n\n# Show detailed component information\nbrainsmith registry --verbose\n\n# Rebuild cache and validate all components\nbrainsmith registry --rebuild\n\n# Verbose output with cache rebuild\nbrainsmith registry -v -r\n</code></pre> <p>See also: Component Registry - Programmatic access to registered components</p>"},{"location":"api/cli/#setup","title":"setup","text":"<p>Install and configure dependencies for Brainsmith development and testing.</p> <p>Syntax:</p> <pre><code>brainsmith setup &lt;SUBCOMMAND&gt; [OPTIONS]\n</code></pre> <p>Subcommands:</p> Subcommand Description <code>all</code> Install all dependencies (cppsim, xsim, boards) <code>cppsim</code> Setup C++ simulation for fast functional testing <code>xsim</code> Setup Xilinx RTL simulation for cycle-accurate validation (requires Vivado) <code>boards</code> Download FPGA board definition files for deployment <code>check</code> Check installation status of all setup components"},{"location":"api/cli/#setup-all","title":"setup all","text":"<p>Install all dependencies at once.</p> <p>Syntax:</p> <pre><code>brainsmith setup all [OPTIONS]\n</code></pre> <p>Options:</p> Option Type Description <code>-f, --force</code> Flag Force reinstallation even if already installed <code>-r, --remove</code> Flag Remove all dependencies <code>-y, --yes</code> Flag Skip confirmation prompts <p>Example:</p> <pre><code># Install all dependencies\nbrainsmith setup all\n\n# Remove all dependencies\nbrainsmith setup all --remove --yes\n</code></pre>"},{"location":"api/cli/#setup-cppsim","title":"setup cppsim","text":"<p>Setup C++ simulation dependencies (cnpy for NPY file support, finn-hlslib headers).</p> <p>Syntax:</p> <pre><code>brainsmith setup cppsim [OPTIONS]\n</code></pre> <p>Options:</p> Option Type Description <code>-f, --force</code> Flag Force reinstallation <code>-r, --remove</code> Flag Remove C++ simulation dependencies <code>-y, --yes</code> Flag Skip confirmation prompts <p>Example:</p> <pre><code># Install C++ simulation dependencies\nbrainsmith setup cppsim\n\n# Force reinstallation\nbrainsmith setup cppsim --force\n\n# Remove dependencies\nbrainsmith setup cppsim --remove --yes\n</code></pre>"},{"location":"api/cli/#setup-xsim","title":"setup xsim","text":"<p>Setup Xilinx simulation by building finn-xsim with Vivado.</p> <p>Syntax:</p> <pre><code>brainsmith setup xsim [OPTIONS]\n</code></pre> <p>Options:</p> Option Type Description <code>-f, --force</code> Flag Force rebuild even if already built <code>-r, --remove</code> Flag Remove Xilinx simulation dependencies <code>-y, --yes</code> Flag Skip confirmation prompts <p>Requirements:</p> <ul> <li>Vivado must be configured in <code>brainsmith.yaml</code> or via environment variables</li> </ul> <p>Example:</p> <pre><code># Build finn-xsim\nbrainsmith setup xsim\n\n# Force rebuild\nbrainsmith setup xsim --force\n\n# Remove xsim\nbrainsmith setup xsim --remove --yes\n</code></pre>"},{"location":"api/cli/#setup-boards","title":"setup boards","text":"<p>Download FPGA board definition files from supported repositories.</p> <p>Syntax:</p> <pre><code>brainsmith setup boards [OPTIONS]\n</code></pre> <p>Options:</p> Option Type Description <code>-f, --force</code> Flag Force redownload even if already present <code>--remove</code> Flag Remove board definition files <code>-r, --repo</code> Text Specific repository to download (multiple allowed) <code>-v, --verbose</code> Flag Show detailed list of all board definitions by repository <code>-y, --yes</code> Flag Skip confirmation prompts <p>Available Repositories:</p> <ul> <li><code>xilinx</code> - Official Xilinx board files</li> <li><code>avnet</code> - Avnet board files</li> <li><code>rfsoc4x2</code> - RFSoC 4x2 board files</li> <li><code>kv260</code> - Kria KV260 board files</li> <li><code>aupzu3</code> - AU+PZU3 board files</li> <li><code>pynq-z1</code> - PYNQ-Z1 board files</li> <li><code>pynq-z2</code> - PYNQ-Z2 board files</li> </ul> <p>Example:</p> <pre><code># Download all board repositories\nbrainsmith setup boards\n\n# Download specific repositories\nbrainsmith setup boards --repo xilinx --repo avnet\n\n# Show detailed board list\nbrainsmith setup boards --verbose\n\n# Remove all board files\nbrainsmith setup boards --remove --yes\n</code></pre>"},{"location":"api/cli/#setup-check","title":"setup check","text":"<p>Check the installation status of all setup components.</p> <p>Syntax:</p> <pre><code>brainsmith setup check\n</code></pre> <p>Output:</p> <p>Displays installation status table showing:</p> <ul> <li>cnpy (C++ NPY file support)</li> <li>finn-hlslib headers</li> <li>finn-xsim</li> <li>Vivado (with version and sourcing status)</li> <li>Vitis HLS (with version and sourcing status)</li> <li>Board files (repository count)</li> </ul> <p>Example:</p> <pre><code>brainsmith setup check\n</code></pre>"},{"location":"api/cli/#see-also","title":"See Also","text":"<ul> <li>Design Space Exploration - DSE API for exploring hardware configurations</li> <li>Settings - Configuration management API</li> <li>Component Registry - Programmatic access to registered components</li> <li>Getting Started - Installation and quickstart guide</li> <li>GitHub - Issues and questions</li> </ul>"},{"location":"api/dataflow/","title":"Dataflow Modeling","text":""},{"location":"api/dataflow/#dataflow-modeling","title":"Dataflow Modeling","text":"<p>Schema-driven kernel modeling for ONNX-to-hardware transformation with efficient design space exploration.</p> <p>Two-phase construction separates expensive setup from fast configuration: Design Space is built once and defines valid parameter ranges, while Design Point is configured many times to represent specific hardware instances. This enables efficient exploration by avoiding redundant computation.</p> <p>Example:</p> <pre><code>import brainsmith.dataflow as df\nfrom brainsmith.registry import kernel\nfrom onnx import NodeProto, helper\nfrom qonnx.core.modelwrapper import ModelWrapper\n\n@kernel(description=\"Hardware LayerNorm\", author=\"Your Name\")\nclass LayerNorm(df.KernelOp):\n    \"\"\"Hardware LayerNorm kernel.\"\"\"\n\n    @classmethod\n    def build_schema(cls, node: NodeProto, model: ModelWrapper) -&gt; df.KernelSchema:\n        \"\"\"Define kernel structure.\"\"\"\n        return LAYERNORM_SCHEMA\n\n    @classmethod\n    def can_infer_from(cls, node: NodeProto, model: ModelWrapper) -&gt; bool:\n        \"\"\"Check if node can be converted to this kernel.\"\"\"\n        return node.op_type == \"FuncLayerNorm\"\n\n    @classmethod\n    def infer_from(cls, node: NodeProto, model: ModelWrapper, insert_index: int):\n        \"\"\"Transform ONNX node to hardware kernel.\"\"\"\n        hw_node = helper.make_node(\n            \"LayerNorm\",\n            inputs=list(node.input),\n            outputs=list(node.output),\n            domain=\"brainsmith.kernels\",\n        )\n        return df.TransformationResult(\n            nodes_to_insert=[hw_node],\n            nodes_to_remove=[node]\n        )\n</code></pre> <p>Example:</p> <pre><code>import brainsmith.dataflow as df\nfrom brainsmith.dataflow import FULL_DIM\n\n# Define kernel schema\nLAYERNORM_SCHEMA = df.KernelSchema(\n    name=\"LayerNorm\",\n    inputs=[\n        df.InputSchema(\n            name=\"input\",\n            block_tiling=[FULL_DIM],\n            stream_tiling=[\"SIMD\"],\n            required_layout=\"NHWC\",\n        )\n    ],\n    outputs=[\n        df.OutputSchema(\n            name=\"output\",\n            block_tiling=[FULL_DIM],\n            stream_tiling=[df.derive_dim(\"input\", df.ShapeHierarchy.STREAM, -1)],\n            required_layout=\"NHWC\",\n        )\n    ],\n    kernel_params={\n        \"epsilon\": (\"f\", True, 1e-5),\n    },\n    constraints=[\n        df.AttrCompare(\"epsilon\", \"&gt;\", 0),\n    ],\n)\n</code></pre> <p>Example:</p> <pre><code>import brainsmith.dataflow as df\n\n# Ordered parameter (list/tuple enables navigation)\ndepth_param = df.ParameterSpec(\"depth\", [128, 256, 512], default=256)\n\n# Discrete parameter (set for unordered categories)\nram_param = df.ParameterSpec(\"ram_style\", {\"distributed\", \"block\"}, default=\"distributed\")\n\n# Callable parameter (explicit type required)\ndynamic_param = df.ParameterSpec(\"depth\", lambda ctx: [128, 256], type=\"int\", default=128)\n\n# Use in kernel schema\nschema = df.KernelSchema(\n    name=\"MyKernel\",\n    inputs=[...],\n    outputs=[...],\n    dse_parameters={\n        \"ram_style\": ram_param,\n        \"depth\": depth_param,\n    }\n)\n</code></pre> <p>Example:</p> <pre><code># Get design point from kernel operator\nop._ensure_ready(model)\npoint = op.design_point\n\n# Configure using interface-based API (for stream parameters)\npoint = point.with_input_stream(0, 32)   # Set input PE=32\npoint = point.with_output_stream(0, 16)  # Set output PE=16\n\n# Configure using dimension-based API (for generic DSE)\npoint = point.with_dimension(\"ram_style\", \"distributed\")\npoint = point.with_dimension(\"depth\", 256)\n\n# Apply configuration\nop.apply_design_point(point)\n</code></pre> <p>Example:</p> <pre><code>import brainsmith.dataflow as df\nfrom onnx import helper\n\n# Create transformation result when converting ONNX to HW node\nhw_node = helper.make_node(\n    \"LayerNorm\",\n    inputs=list(node.input),\n    outputs=list(node.output),\n    domain=\"brainsmith.kernels\",\n    name=f\"LayerNorm_{node.name}\",\n)\n\nresult = df.TransformationResult(\n    nodes_to_insert=[hw_node],\n    nodes_to_remove=[node]\n)\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelOp","title":"KernelOp","text":"<pre><code>KernelOp(onnx_node, **kwargs)\n</code></pre> <p>               Bases: <code>HWCustomOp</code>, <code>ABC</code></p> <p>Kernel operator base class.</p> <p>Shapes extracted from ModelWrapper context, never stored in nodeattrs. Subclasses implement build_schema() to construct their KernelSchema.</p> Caching Strategy <ul> <li>design_space: Cached (expensive to build, invalidated on structural changes)</li> <li>design_point: Regenerated from nodeattrs (guarantees consistency)</li> </ul> <p>For execution compatibility notes, see module docstring.</p> Source code in <code>brainsmith/dataflow/kernel_op.py</code> <pre><code>def __init__(self, onnx_node, **kwargs):\n    super().__init__(onnx_node, **kwargs)\n    # Build and freeze schema from node structure\n    self.kernel_schema = self.build_schema(onnx_node, model=None)\n\n    # Design space caching for DSE performance\n    # Design points are regenerated on each access to ensure consistency with nodeattrs\n    self._design_space: \"KernelDesignSpace\" | None = (\n        None  # Cached, call invalidate() to rebuild\n    )\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelOp.design_point","title":"design_point  <code>property</code>","text":"<pre><code>design_point: KernelDesignPoint\n</code></pre> <p>Current kernel configuration as design point (regenerated from nodeattrs).</p> <p>This property regenerates on every access to ensure consistency with current nodeattrs. For better performance when accessing multiple properties, cache the design point in a local variable:</p> Example"},{"location":"api/dataflow/#brainsmith.dataflow.KernelOp.design_point--good-cache-locally-for-multiple-accesses","title":"GOOD: Cache locally for multiple accesses","text":"<p>point = self.design_point simd = point.inputs[\"input\"].stream_shape[-1] width = point.inputs[\"input\"].tensor_shape[-1] dtype = point.inputs[\"input\"].datatype</p>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelOp.design_point--avoid-multiple-accesses-trigger-multiple-rebuilds","title":"AVOID: Multiple accesses trigger multiple rebuilds","text":"<p>simd = self.design_point.inputs[\"input\"].stream_shape[-1] width = self.design_point.inputs[\"input\"].tensor_shape[-1] dtype = self.design_point.inputs[\"input\"].datatype</p>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelOp.design_space","title":"design_space  <code>property</code>","text":"<pre><code>design_space: KernelDesignSpace\n</code></pre> <p>Cached design space (call method with model_w first to initialize).</p>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelOp.apply_design_point","title":"apply_design_point","text":"<pre><code>apply_design_point(point: KernelDesignPoint) -&gt; None\n</code></pre> <p>Apply chosen design point to nodeattrs (persist to ONNX).</p> <p>Syncs design point configuration back to node attributes. The design_point property will regenerate from these nodeattrs on next access.</p> <p>Parameters:</p> Name Type Description Default <code>point</code> <code>KernelDesignPoint</code> <p>Design point to apply</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If point from different design space</p> <code>RuntimeError</code> <p>If design space not initialized</p> Example Source code in <code>brainsmith/dataflow/kernel_op.py</code> <pre><code>def apply_design_point(self, point: \"KernelDesignPoint\") -&gt; None:\n    \"\"\"Apply chosen design point to nodeattrs (persist to ONNX).\n\n    Syncs design point configuration back to node attributes.\n    The design_point property will regenerate from these nodeattrs on next access.\n\n    Args:\n        point: Design point to apply\n\n    Raises:\n        ValueError: If point from different design space\n        RuntimeError: If design space not initialized\n\n    Example:\n        &gt;&gt;&gt; # DSE exploration\n        &gt;&gt;&gt; best_point = None\n        &gt;&gt;&gt; best_cycles = float('inf')\n        &gt;&gt;&gt; for point in op.design_space.sweep_dimension(\"SIMD\"):\n        ...     if point.initiation_interval &lt; best_cycles:\n        ...         best_cycles = point.initiation_interval\n        ...         best_point = point\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Apply winner to node\n        &gt;&gt;&gt; op.apply_design_point(best_point)\n    \"\"\"\n    if self._design_space is None:\n        raise RuntimeError(\n            f\"{self.onnx_node.name}: Design space not initialized. \"\n            f\"Call a method with model_w parameter first.\"\n        )\n\n    if point.design_space is not self._design_space:\n        raise ValueError(\n            f\"{self.onnx_node.name}: DesignPoint from different DesignSpace. \"\n            f\"Cannot apply to this node.\"\n        )\n\n    # Sync config \u2192 nodeattrs (bypass set_nodeattr to avoid invalidation)\n    for dim_name, value in point.config.items():\n        super().set_nodeattr(dim_name, value)\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelOp.apply_design_point--dse-exploration","title":"DSE exploration","text":"<p>best_point = None best_cycles = float('inf') for point in op.design_space.sweep_dimension(\"SIMD\"): ...     if point.initiation_interval &lt; best_cycles: ...         best_cycles = point.initiation_interval ...         best_point = point</p>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelOp.apply_design_point--apply-winner-to-node","title":"Apply winner to node","text":"<p>op.apply_design_point(best_point)</p>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelOp.build_design_space","title":"build_design_space","text":"<pre><code>build_design_space(model_w: ModelWrapper) -&gt; None\n</code></pre> <p>FINN API compatibility: Build design space.</p> <p>This method provides compatibility with FINN's getHWCustomOp() utility, which detects KernelOp via kernel_schema attribute and calls this method.</p> <p>Parameters:</p> Name Type Description Default <code>model_w</code> <code>ModelWrapper</code> <p>ModelWrapper for graph context</p> required Source code in <code>brainsmith/dataflow/kernel_op.py</code> <pre><code>def build_design_space(self, model_w: ModelWrapper) -&gt; None:\n    \"\"\"FINN API compatibility: Build design space.\n\n    This method provides compatibility with FINN's getHWCustomOp() utility,\n    which detects KernelOp via kernel_schema attribute and calls this method.\n\n    Args:\n        model_w: ModelWrapper for graph context\n    \"\"\"\n    # Delegate to existing lazy initialization\n    self._ensure_ready(model_w)\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelOp.build_schema","title":"build_schema  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>build_schema(node: NodeProto, model: ModelWrapper | None) -&gt; KernelSchema\n</code></pre> <p>Build kernel schema from ONNX node.</p> <p>Polymorphic method that handles both static and dynamic schemas: - Static schemas: return constant, ignore parameters - Dynamic schemas: inspect node structure to build schema</p> <p>Called in two contexts: 1. During init: model=None (schema built for instance) 2. During can_infer_from(): model provided (schema built for validation)</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>NodeProto</code> <p>ONNX node (provides inputs, outputs, attributes)</p> required <code>model</code> <code>ModelWrapper | None</code> <p>Optional ModelWrapper (provides shapes, datatypes for validation context)</p> required <p>Returns:</p> Type Description <code>KernelSchema</code> <p>KernelSchema defining kernel structure</p> <p>Example (static schema):     @classmethod     def build_schema(cls, node, model):         return LAYERNORM_SCHEMA</p> <p>Example (dynamic schema):     @classmethod     def build_schema(cls, node, model):         num_inputs = len(node.input)         inputs = [InputSchema(name=f\"input{i}\", ...) for i in range(num_inputs)]         return KernelSchema(name=\"Concat\", inputs=inputs, outputs=[...])</p> Source code in <code>brainsmith/dataflow/kernel_op.py</code> <pre><code>@classmethod\n@abstractmethod\ndef build_schema(cls, node: NodeProto, model: ModelWrapper | None) -&gt; KernelSchema:\n    \"\"\"Build kernel schema from ONNX node.\n\n    Polymorphic method that handles both static and dynamic schemas:\n    - Static schemas: return constant, ignore parameters\n    - Dynamic schemas: inspect node structure to build schema\n\n    Called in two contexts:\n    1. During __init__: model=None (schema built for instance)\n    2. During can_infer_from(): model provided (schema built for validation)\n\n    Args:\n        node: ONNX node (provides inputs, outputs, attributes)\n        model: Optional ModelWrapper (provides shapes, datatypes for validation context)\n\n    Returns:\n        KernelSchema defining kernel structure\n\n    Example (static schema):\n        @classmethod\n        def build_schema(cls, node, model):\n            return LAYERNORM_SCHEMA\n\n    Example (dynamic schema):\n        @classmethod\n        def build_schema(cls, node, model):\n            num_inputs = len(node.input)\n            inputs = [InputSchema(name=f\"input{i}\", ...) for i in range(num_inputs)]\n            return KernelSchema(name=\"Concat\", inputs=inputs, outputs=[...])\n    \"\"\"\n    raise NotImplementedError(f\"{cls.__name__}.build_schema()\")\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelOp.can_infer_from","title":"can_infer_from  <code>classmethod</code>","text":"<pre><code>can_infer_from(node: NodeProto, model: ModelWrapper) -&gt; bool\n</code></pre> <p>Check if this kernel can transform the given ONNX node (default: no).</p> Source code in <code>brainsmith/dataflow/kernel_op.py</code> <pre><code>@classmethod\ndef can_infer_from(cls, node: NodeProto, model: ModelWrapper) -&gt; bool:\n    \"\"\"Check if this kernel can transform the given ONNX node (default: no).\"\"\"\n    return False\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelOp.get_input_datatype","title":"get_input_datatype","text":"<pre><code>get_input_datatype(ind=0) -&gt; DataType\n</code></pre> <p>Get input datatype.</p> Source code in <code>brainsmith/dataflow/kernel_op.py</code> <pre><code>def get_input_datatype(self, ind=0) -&gt; DataType:\n    \"\"\"Get input datatype.\"\"\"\n    return DataType[self.get_nodeattr(f\"input{ind}Datatype\")]\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelOp.get_nodeattr_types","title":"get_nodeattr_types","text":"<pre><code>get_nodeattr_types()\n</code></pre> <p>Return nodeattr registry (datatypes + user params + kernel params).</p> <p>Auto-delegates to kernel_schema.build_nodeattr_registry() which includes: - Interface datatypes (input0Datatype, output0Datatype, etc.) - Internal datatypes (accumulatorDatatype, etc.) - Template parameters (SIMD, PE, etc.) - Kernel-specific parameters (epsilon, algorithm, etc.)</p> <p>Automatically sets FIFO depth defaults based on kernel schema interface counts.</p> <p>Only override if build_schema() needs to read nodeattrs (circular dependency). In that case, define nodeattrs explicitly before calling build_schema().</p> Source code in <code>brainsmith/dataflow/kernel_op.py</code> <pre><code>def get_nodeattr_types(self):\n    \"\"\"Return nodeattr registry (datatypes + user params + kernel params).\n\n    Auto-delegates to kernel_schema.build_nodeattr_registry() which includes:\n    - Interface datatypes (input0Datatype, output0Datatype, etc.)\n    - Internal datatypes (accumulatorDatatype, etc.)\n    - Template parameters (SIMD, PE, etc.)\n    - Kernel-specific parameters (epsilon, algorithm, etc.)\n\n    Automatically sets FIFO depth defaults based on kernel schema interface counts.\n\n    Only override if build_schema() needs to read nodeattrs (circular dependency).\n    In that case, define nodeattrs explicitly before calling build_schema().\n    \"\"\"\n    base_attrs = super().get_nodeattr_types()\n\n    # Add implementation attribute for backend selection\n    # Replaces domain mutation used by FINN's SpecializeLayers\n    base_attrs.update(\n        {\n            \"implementation\": (\n                \"s\",\n                False,\n                \"\",\n                {\n                    \"\",  # Not yet specialized\n                    \"vitis_hls\",  # Vitis HLS (2020.1+)\n                    \"verilog\",  # Verilog RTL\n                    \"systemverilog\",  # SystemVerilog RTL\n                    \"static_ip\",  # Pre-generated IP core\n                },\n            ),\n        }\n    )\n\n    try:\n        base_attrs.update(self.kernel_schema.build_nodeattr_registry())\n    except RecursionError as e:\n        raise RuntimeError(\n            f\"{self.__class__.__name__}.kernel_schema property calls get_nodeattr(), \"\n            f\"creating circular dependency. You must override get_nodeattr_types() \"\n            f\"explicitly to define nodeattrs before schema construction. \"\n            f\"See KernelOp docstring for mode-dependent schema pattern.\"\n        ) from e\n\n    # Auto-configure FIFO depths based on schema interface counts\n    # This prevents IndexError in FINN's InsertFIFO transform for multi-input/output kernels\n    num_inputs = len(self.kernel_schema.inputs)\n    num_outputs = len(self.kernel_schema.outputs)\n\n    if num_inputs &gt; 0:\n        base_attrs[\"inFIFODepths\"] = (\"ints\", False, [2] * num_inputs)\n    if num_outputs &gt; 0:\n        base_attrs[\"outFIFODepths\"] = (\"ints\", False, [2] * num_outputs)\n\n    return base_attrs\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelOp.get_normal_input_shape","title":"get_normal_input_shape","text":"<pre><code>get_normal_input_shape(ind=0) -&gt; tuple[int, ...]\n</code></pre> <p>Return normal (unfolded) input shape as immutable tuple (FINN convention).</p> Source code in <code>brainsmith/dataflow/kernel_op.py</code> <pre><code>def get_normal_input_shape(self, ind=0) -&gt; tuple[int, ...]:\n    \"\"\"Return normal (unfolded) input shape as immutable tuple (FINN convention).\"\"\"\n    return self.design_point.input_list[ind].tensor_shape\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelOp.get_normal_output_shape","title":"get_normal_output_shape","text":"<pre><code>get_normal_output_shape(ind=0) -&gt; tuple[int, ...]\n</code></pre> <p>Return normal (unfolded) output shape as immutable tuple (FINN convention).</p> Source code in <code>brainsmith/dataflow/kernel_op.py</code> <pre><code>def get_normal_output_shape(self, ind=0) -&gt; tuple[int, ...]:\n    \"\"\"Return normal (unfolded) output shape as immutable tuple (FINN convention).\"\"\"\n    return self.design_point.output_list[ind].tensor_shape\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelOp.get_number_output_values","title":"get_number_output_values","text":"<pre><code>get_number_output_values()\n</code></pre> <p>Get iteration count(s) for output values.</p> <p>Matches FINN API pattern: - Single-output kernels: Returns int (iteration count) - Multi-output kernels: Returns dict mapping output names \u2192 iteration counts</p> <p>Returns:</p> Name Type Description <code>int</code> <p>For single-output kernels (e.g., MVAU, Thresholding, AddStreams)</p> <code>dict</code> <p>For multi-output kernels (e.g., DuplicateStreams, Split)</p> <p>Examples:</p> <p>Single-output: 512 Multi-output: {'out0': 512, 'out1': 512}</p> Source code in <code>brainsmith/dataflow/kernel_op.py</code> <pre><code>def get_number_output_values(self):\n    \"\"\"Get iteration count(s) for output values.\n\n    Matches FINN API pattern:\n    - Single-output kernels: Returns int (iteration count)\n    - Multi-output kernels: Returns dict mapping output names \u2192 iteration counts\n\n    Returns:\n        int: For single-output kernels (e.g., MVAU, Thresholding, AddStreams)\n        dict: For multi-output kernels (e.g., DuplicateStreams, Split)\n\n    Examples:\n        Single-output: 512\n        Multi-output: {'out0': 512, 'out1': 512}\n    \"\"\"\n    num_outputs = len(self.onnx_node.output)\n\n    if num_outputs == 1:\n        # Single-output: Return int (FINN pattern for MVAU, Thresholding, etc.)\n        folded_shape = self.get_folded_output_shape(ind=0)\n        return math.prod(folded_shape[:-1])\n    else:\n        # Multi-output: Return dict (FINN pattern for DuplicateStreams, Split)\n        out_val = {}\n        for i in range(num_outputs):\n            folded_shape = self.get_folded_output_shape(ind=i)\n            iteration_count = math.prod(folded_shape[:-1])\n            out_val[f\"out{i}\"] = iteration_count\n        return out_val\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelOp.get_output_datatype","title":"get_output_datatype","text":"<pre><code>get_output_datatype(ind=0) -&gt; DataType\n</code></pre> <p>Get output datatype.</p> Source code in <code>brainsmith/dataflow/kernel_op.py</code> <pre><code>def get_output_datatype(self, ind=0) -&gt; DataType:\n    \"\"\"Get output datatype.\"\"\"\n    return DataType[self.get_nodeattr(f\"output{ind}Datatype\")]\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelOp.get_valid_ranges","title":"get_valid_ranges","text":"<pre><code>get_valid_ranges(model_w: ModelWrapper) -&gt; dict[str, Union[OrderedParameter, frozenset]]\n</code></pre> <p>Valid parameter values for DSE (tiling + resource).</p> <p>Returns:</p> Type Description <code>dict[str, Union[OrderedParameter, frozenset]]</code> <p>Dict mapping parameter names to OrderedParameter (ordered sequences)</p> <code>dict[str, Union[OrderedParameter, frozenset]]</code> <p>or frozenset (discrete categories).</p> Source code in <code>brainsmith/dataflow/kernel_op.py</code> <pre><code>def get_valid_ranges(\n    self, model_w: ModelWrapper\n) -&gt; dict[str, Union[\"OrderedParameter\", frozenset]]:\n    \"\"\"Valid parameter values for DSE (tiling + resource).\n\n    Returns:\n        Dict mapping parameter names to OrderedParameter (ordered sequences)\n        or frozenset (discrete categories).\n    \"\"\"\n    self._ensure_ready(model_w)\n    return self.design_space.parameters\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelOp.infer_from","title":"infer_from  <code>classmethod</code>","text":"<pre><code>infer_from(node: NodeProto, model: ModelWrapper, insert_index: int) -&gt; TransformationResult\n</code></pre> <p>Transform ONNX node to hardware kernel node(s).</p> Source code in <code>brainsmith/dataflow/kernel_op.py</code> <pre><code>@classmethod\ndef infer_from(\n    cls, node: NodeProto, model: ModelWrapper, insert_index: int\n) -&gt; TransformationResult:\n    \"\"\"Transform ONNX node to hardware kernel node(s).\"\"\"\n    raise NotImplementedError(f\"{cls.__name__}.infer_from()\")\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelOp.infer_node_datatype","title":"infer_node_datatype","text":"<pre><code>infer_node_datatype(model_w)\n</code></pre> <p>Sync datatypes: model \u2192 nodeattrs (inputs), nodeattrs \u2192 model (outputs).</p> <p>Initializes design space which syncs input datatypes from model to nodeattrs. Then propagates output datatypes from nodeattrs back to model.</p> Source code in <code>brainsmith/dataflow/kernel_op.py</code> <pre><code>def infer_node_datatype(self, model_w):\n    \"\"\"Sync datatypes: model \u2192 nodeattrs (inputs), nodeattrs \u2192 model (outputs).\n\n    Initializes design space which syncs input datatypes from model to nodeattrs.\n    Then propagates output datatypes from nodeattrs back to model.\n    \"\"\"\n    # Initialize (syncs inputs: model \u2192 nodeattrs)\n    self._ensure_ready(model_w)\n\n    # Propagate output datatypes: nodeattrs \u2192 model\n    for i, out_name in enumerate(self.onnx_node.output):\n        if out_name:\n            odt = self.get_output_datatype(i)\n            model_w.set_tensor_datatype(out_name, odt)\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelOp.invalidate","title":"invalidate","text":"<pre><code>invalidate() -&gt; None\n</code></pre> <p>Invalidate cached design space after external graph changes.</p> <p>Call this after transforms that change: - Tensor shapes (padding, reshape) - Datatypes in graph metadata - Node rewiring (FIFO insertion)</p> <p>Next method call with model_w will rebuild design space automatically. Design points regenerate on every access, so no explicit invalidation needed.</p> Example Source code in <code>brainsmith/dataflow/kernel_op.py</code> <pre><code>def invalidate(self) -&gt; None:\n    \"\"\"Invalidate cached design space after external graph changes.\n\n    Call this after transforms that change:\n    - Tensor shapes (padding, reshape)\n    - Datatypes in graph metadata\n    - Node rewiring (FIFO insertion)\n\n    Next method call with model_w will rebuild design space automatically.\n    Design points regenerate on every access, so no explicit invalidation needed.\n\n    Example:\n        &gt;&gt;&gt; # After transform changes graph\n        &gt;&gt;&gt; model = ApplyPadding().apply(model)\n        &gt;&gt;&gt; for node in model.graph.node:\n        ...     op = getCustomOp(node)\n        ...     if isinstance(op, KernelOp):\n        ...         op.invalidate()\n    \"\"\"\n    self._design_space = None\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelOp.invalidate--after-transform-changes-graph","title":"After transform changes graph","text":"<p>model = ApplyPadding().apply(model) for node in model.graph.node: ...     op = getCustomOp(node) ...     if isinstance(op, KernelOp): ...         op.invalidate()</p>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelOp.make_shape_compatible_op","title":"make_shape_compatible_op","text":"<pre><code>make_shape_compatible_op(model_w)\n</code></pre> <p>Create standard ONNX op for shape inference (auto-detects pattern).</p> Source code in <code>brainsmith/dataflow/kernel_op.py</code> <pre><code>def make_shape_compatible_op(self, model_w):\n    \"\"\"Create standard ONNX op for shape inference (auto-detects pattern).\"\"\"\n    from onnx import helper\n\n    num_out = len(self.onnx_node.output)\n    num_in = len(self.onnx_node.input)\n\n    if num_in == 1 and num_out &gt; 1:\n        return helper.make_node(\n            \"Split\",\n            inputs=[self.onnx_node.input[0]],\n            outputs=list(self.onnx_node.output),\n            axis=-1,\n        )\n\n    if num_out == 1:\n        input_shapes = [tuple(model_w.get_tensor_shape(inp)) for inp in self.onnx_node.input]\n\n        if len(set(input_shapes)) == 1:\n            return super().make_const_shape_op(input_shapes[0])\n        else:\n            raise NotImplementedError(\n                f\"{self.__class__.__name__}: {num_in} inputs with different shapes \"\n                f\"{input_shapes}. Override make_shape_compatible_op().\"\n            )\n\n    raise NotImplementedError(\n        f\"{self.__class__.__name__}: {num_in} inputs \u2192 {num_out} outputs. \"\n        f\"Override make_shape_compatible_op().\"\n    )\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelOp.set_nodeattr","title":"set_nodeattr","text":"<pre><code>set_nodeattr(name: str, value: Any) -&gt; None\n</code></pre> <p>Set nodeattr and auto-invalidate design space if needed.</p> <p>Design points regenerate on each access, so no explicit invalidation needed.</p> Source code in <code>brainsmith/dataflow/kernel_op.py</code> <pre><code>def set_nodeattr(self, name: str, value: Any) -&gt; None:\n    \"\"\"Set nodeattr and auto-invalidate design space if needed.\n\n    Design points regenerate on each access, so no explicit invalidation needed.\n    \"\"\"\n    try:\n        old_value = self.get_nodeattr(name)\n    except (AttributeError, Exception):\n        old_value = None\n\n    if old_value != value:\n        super().set_nodeattr(name, value)\n\n        # Only invalidate design space for structural changes (datatypes)\n        # Dimension changes (PE, SIMD, etc.) don't require invalidation since\n        # design points regenerate from nodeattrs on each access\n        if name in self.kernel_schema.get_structural_nodeattrs():\n            self._design_space = None\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelOpError","title":"KernelOpError","text":"<pre><code>KernelOpError(node, message)\n</code></pre> <p>               Bases: <code>Exception</code></p> <p>Exception raised by kernel operators with node context.</p> <p>Attributes:</p> Name Type Description <code>node</code> <p>ONNX node that caused the error</p> <code>message</code> <p>Error message</p> Source code in <code>brainsmith/dataflow/kernel_op.py</code> <pre><code>def __init__(self, node, message):\n    self.node = node\n    super().__init__(f\"{node.name}: {message}\")\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelSchema","title":"KernelSchema  <code>dataclass</code>","text":"<pre><code>KernelSchema(name: str, inputs: list[InputSchema] = list(), outputs: list[OutputSchema] = list(), internal_datatypes: dict[str, Any] = dict(), kernel_params: dict[str, tuple] = dict(), dse_parameters: dict[str, ParameterSpec] = dict(), constraints: list[Constraint] = list(), attribute_mapping: dict[str, str] = dict())\n</code></pre> <p>Kernel specification defining structure and validation.</p> <p>Combines interface definitions, validation constraints, and design space parameters. Defines structure only - shapes come from ONNX context, execution logic lives in KernelOp.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Kernel name</p> <code>inputs</code> <code>list[InputSchema]</code> <p>Input interface schemas</p> <code>outputs</code> <code>list[OutputSchema]</code> <p>Output interface schemas</p> <code>internal_datatypes</code> <code>dict[str, Any]</code> <p>Internal datatype derivation specs (e.g., accumulator)</p> <code>kernel_params</code> <code>dict[str, tuple]</code> <p>Kernel-specific parameters (e.g., epsilon, algorithm)</p> <code>dse_parameters</code> <code>dict[str, ParameterSpec]</code> <p>Explorable resource/implementation parameters (e.g., ram_style)</p> <code>constraints</code> <code>list[Constraint]</code> <p>Validation constraints (datatype, shape, ONNX requirements)</p> <code>attribute_mapping</code> <code>dict[str, str]</code> <p>Map ONNX attributes to kernel parameters</p>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelSchema.attribute_mapping","title":"attribute_mapping  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>attribute_mapping: dict[str, str] = field(default_factory=dict)\n</code></pre> <p>Map ONNX attributes to kernel parameters.</p> <p>Example: {\"epsilon\": \"epsilon\", \"axis\": \"normalized_axis\"}</p>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelSchema.dse_parameters","title":"dse_parameters  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dse_parameters: dict[str, ParameterSpec] = field(default_factory=dict)\n</code></pre> <p>Explorable resource/implementation parameters (ram_style, res_type, etc.).</p> <p>Tiling parameters (PE, SIMD) NOT declared here - auto-extracted from stream_tiling templates with defaults computed from factoring.</p> <p>Example: {\"ram_style\": ParameterSpec(\"ram_style\", {\"distributed\", \"block\"}, \"distributed\")}</p>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelSchema.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Validate schema structure and transformation consistency.</p> Source code in <code>brainsmith/dataflow/schemas.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate schema structure and transformation consistency.\"\"\"\n    self.validate()\n\n    # Validate transformation fields\n    self._validate_transformation_fields()\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelSchema.build_nodeattr_registry","title":"build_nodeattr_registry","text":"<pre><code>build_nodeattr_registry() -&gt; dict[str, tuple]\n</code></pre> <p>Build nodeattr registry from schema definition.</p> <p>Schemas define STRUCTURE, not STORAGE. Generates persistence layer from structural schema, returning only attributes that need persistence: - Datatypes (for interfaces and internals) - Tiling parameters (SIMD, PE, etc.) - auto-extracted from stream_tiling - DSE parameters (ram_style, res_type, etc.) - from dse_parameters - Kernel-specific parameters (epsilon, algorithm, etc.) - from kernel_params</p> <p>Shapes are NEVER stored in nodeattrs. They are either: - Tensor shapes: extracted from ModelWrapper (ONNX graph) - Block/stream shapes: computed from schema templates</p> <p>Returns:</p> Name Type Description <code>dict[str, tuple]</code> <p>Dict mapping nodeattr name to (type, required, default_value)</p> <code>Format</code> <code>dict[str, tuple]</code> <p>{\"attrName\": (\"i\"|\"s\"|\"f\", True|False, default)}</p> Source code in <code>brainsmith/dataflow/schemas.py</code> <pre><code>def build_nodeattr_registry(self) -&gt; dict[str, tuple]:\n    \"\"\"Build nodeattr registry from schema definition.\n\n    Schemas define STRUCTURE, not STORAGE. Generates persistence layer\n    from structural schema, returning only attributes that need persistence:\n    - Datatypes (for interfaces and internals)\n    - Tiling parameters (SIMD, PE, etc.) - auto-extracted from stream_tiling\n    - DSE parameters (ram_style, res_type, etc.) - from dse_parameters\n    - Kernel-specific parameters (epsilon, algorithm, etc.) - from kernel_params\n\n    Shapes are NEVER stored in nodeattrs. They are either:\n    - Tensor shapes: extracted from ModelWrapper (ONNX graph)\n    - Block/stream shapes: computed from schema templates\n\n    Returns:\n        Dict mapping nodeattr name to (type, required, default_value)\n        Format: {\"attrName\": (\"i\"|\"s\"|\"f\", True|False, default)}\n    \"\"\"\n    attrs = {}\n\n    # Datatypes\n    for i in range(len(self.inputs)):\n        attrs[f\"input{i}Datatype\"] = (\"s\", False, \"\")\n\n    for i in range(len(self.outputs)):\n        attrs[f\"output{i}Datatype\"] = (\"s\", False, \"\")\n\n    for internal_name in self.internal_datatypes.keys():\n        attrs[f\"{internal_name}Datatype\"] = (\"s\", False, \"\")\n\n    # Tiling parameters (PE, SIMD, etc.) - auto-extracted\n    template_params = self._extract_template_params()\n    for param in template_params:\n        attrs[param] = (\"i\", False, 1)  # Default 1, will be computed from factoring\n\n    # DSE parameters (resource parameters)\n    for param_name, param_spec in self.dse_parameters.items():\n        attrs[param_name] = _infer_nodeattr_type(param_spec)\n\n    # Kernel-specific parameters (structural)\n    attrs.update(self.kernel_params)\n\n    return attrs\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelSchema.get_optimization_nodeattrs","title":"get_optimization_nodeattrs","text":"<pre><code>get_optimization_nodeattrs() -&gt; set\n</code></pre> <p>Get nodeattrs that affect optimization (re-explore if changed).</p> <p>Optimization nodeattrs are those whose changes only require re-exploring the design space (trying different stream shapes), not rebuilding the entire design space.</p> <p>These include: - Parallelization parameters (SIMD, PE, MW, MH, etc.): Appear in   stream_tiling templates and determine stream shapes during DSE</p> <p>Returns:</p> Type Description <code>set</code> <p>Set of optimization nodeattr names</p> Example <p>schema.get_optimization_nodeattrs()</p> Source code in <code>brainsmith/dataflow/schemas.py</code> <pre><code>def get_optimization_nodeattrs(self) -&gt; set:\n    \"\"\"Get nodeattrs that affect optimization (re-explore if changed).\n\n    Optimization nodeattrs are those whose changes only require\n    re-exploring the design space (trying different stream shapes),\n    not rebuilding the entire design space.\n\n    These include:\n    - Parallelization parameters (SIMD, PE, MW, MH, etc.): Appear in\n      stream_tiling templates and determine stream shapes during DSE\n\n    Returns:\n        Set of optimization nodeattr names\n\n    Example:\n        &gt;&gt;&gt; schema.get_optimization_nodeattrs()\n        {'SIMD', 'PE'}\n    \"\"\"\n    # Parameters in stream_tiling affect optimization\n    return self._extract_template_params()\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelSchema.get_structural_nodeattrs","title":"get_structural_nodeattrs","text":"<pre><code>get_structural_nodeattrs() -&gt; set\n</code></pre> <p>Get nodeattrs that affect design space (rebuild if changed).</p> <p>Structural nodeattrs are those whose changes require rebuilding the entire KernelDesignSpace (not just reconfiguration).</p> <p>These include: - All datatypes (input, output, internal): Affect internal datatype   derivation (e.g., accumulator width depends on input datatype) - Parameters in block_tiling (rare): Affect block shape computation</p> <p>Returns:</p> Type Description <code>set</code> <p>Set of structural nodeattr names</p> Example <p>schema.get_structural_nodeattrs()</p> Source code in <code>brainsmith/dataflow/schemas.py</code> <pre><code>def get_structural_nodeattrs(self) -&gt; set:\n    \"\"\"Get nodeattrs that affect design space (rebuild if changed).\n\n    Structural nodeattrs are those whose changes require rebuilding\n    the entire KernelDesignSpace (not just reconfiguration).\n\n    These include:\n    - All datatypes (input, output, internal): Affect internal datatype\n      derivation (e.g., accumulator width depends on input datatype)\n    - Parameters in block_tiling (rare): Affect block shape computation\n\n    Returns:\n        Set of structural nodeattr names\n\n    Example:\n        &gt;&gt;&gt; schema.get_structural_nodeattrs()\n        {'input0Datatype', 'output0Datatype', 'accumulatorDatatype'}\n    \"\"\"\n    structural = set()\n\n    # All datatypes are structural\n    for i in range(len(self.inputs)):\n        structural.add(f\"input{i}Datatype\")\n    for i in range(len(self.outputs)):\n        structural.add(f\"output{i}Datatype\")\n    for internal_name in self.internal_datatypes.keys():\n        structural.add(f\"{internal_name}Datatype\")\n\n    # Parameters in block_tiling are structural (rare)\n    for inp in self.inputs:\n        if inp.block_tiling and inp.block_tiling is not FULL_SHAPE:\n            for elem in inp.block_tiling:\n                if isinstance(elem, str):\n                    structural.add(elem)\n    for out in self.outputs:\n        if out.block_tiling and out.block_tiling is not FULL_SHAPE:\n            for elem in out.block_tiling:\n                if isinstance(elem, str):\n                    structural.add(elem)\n\n    return structural\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelSchema.validate","title":"validate","text":"<pre><code>validate() -&gt; None\n</code></pre> <p>Validate the schema structure.</p> Source code in <code>brainsmith/dataflow/schemas.py</code> <pre><code>def validate(self) -&gt; None:\n    \"\"\"Validate the schema structure.\"\"\"\n\n    # Create sets directly (no intermediate lists)\n    input_names = {inp.name for inp in self.inputs}\n    output_names = {out.name for out in self.outputs}\n\n    # Check for duplicates (comparing lengths to original counts)\n    if len(input_names) != len(self.inputs):\n        raise ValueError(f\"Duplicate input names in kernel '{self.name}'\")\n    if len(output_names) != len(self.outputs):\n        raise ValueError(f\"Duplicate output names in kernel '{self.name}'\")\n\n    # Check for conflicts between inputs and outputs\n    conflicts = input_names &amp; output_names\n    if conflicts:\n        raise ValueError(\n            f\"Interface names must be unique across inputs and outputs in kernel '{self.name}'. \"\n            f\"Duplicate names: {', '.join(sorted(conflicts))}\"\n        )\n\n    # Check internal datatypes (use set operation instead of loop)\n    all_interface_names = input_names | output_names\n    internal_conflicts = set(self.internal_datatypes) &amp; all_interface_names\n    if internal_conflicts:\n        raise ValueError(\n            f\"Internal datatypes conflict with interface names in kernel '{self.name}': \"\n            f\"{', '.join(sorted(internal_conflicts))}\"\n        )\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.InputSchema","title":"InputSchema  <code>dataclass</code>","text":"<pre><code>InputSchema(name: str, block_tiling: TilingSpec | None = None, stream_tiling: TilingSpec | None = None, datatype: Any | None = None, required_layout: str | None = None)\n</code></pre> <p>Input interface specification.</p> <p>Defines input structure (tiling) and requirements (layout, datatype).</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Interface name (e.g., \"input\", \"input0\")</p> <code>block_tiling</code> <code>TilingSpec | None</code> <p>Block tiling specification (e.g., [FULL_DIM, FULL_DIM])</p> <code>stream_tiling</code> <code>TilingSpec | None</code> <p>Stream tiling specification (e.g., [\"SIMD\"], [1, 1, 1, \"PE\"])</p> <code>datatype</code> <code>Any | None</code> <p>Datatype spec (None to use from ONNX, or DatatypeSpec union type to derive/optimize)</p> <code>required_layout</code> <code>str | None</code> <p>Expected input layout (e.g., \"NHWC\", \"NCHW\"), None if no requirement</p>"},{"location":"api/dataflow/#brainsmith.dataflow.InputSchema.tiling_attrs","title":"tiling_attrs  <code>property</code>","text":"<pre><code>tiling_attrs: list[str]\n</code></pre> <p>Extract unique template parameter names from tiling specs.</p>"},{"location":"api/dataflow/#brainsmith.dataflow.InputSchema.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Validate interface requirements.</p> Source code in <code>brainsmith/dataflow/schemas.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate interface requirements.\"\"\"\n    if self.required_layout and self.required_layout not in {\"NCHW\", \"NHWC\"}:\n        raise ValueError(\n            f\"Invalid required_layout '{self.required_layout}' for input '{self.name}'. \"\n            f\"Must be 'NCHW' or 'NHWC'.\"\n        )\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.OutputSchema","title":"OutputSchema  <code>dataclass</code>","text":"<pre><code>OutputSchema(name: str, block_tiling: TilingSpec | None = None, stream_tiling: TilingSpec | None = None, datatype: Any | None = None, required_layout: str | None = None, preserves_input_layout: bool = True)\n</code></pre> <p>Output interface specification.</p> <p>Defines output structure (tiling), datatype derivation, and layout requirements.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Interface name (e.g., \"output\", \"output0\")</p> <code>block_tiling</code> <code>TilingSpec | None</code> <p>Block tiling specification</p> <code>stream_tiling</code> <code>TilingSpec | None</code> <p>Stream tiling specification</p> <code>datatype</code> <code>Any | None</code> <p>Datatype spec (None to use from ONNX, or DatatypeSpec union type to derive)</p> <code>required_layout</code> <code>str | None</code> <p>Expected output layout (e.g., \"NHWC\"), None if no requirement</p> <code>preserves_input_layout</code> <code>bool</code> <p>Whether output preserves first input's layout (default True)</p>"},{"location":"api/dataflow/#brainsmith.dataflow.OutputSchema.tiling_attrs","title":"tiling_attrs  <code>property</code>","text":"<pre><code>tiling_attrs: list[str]\n</code></pre> <p>Extract unique template parameter names from tiling specs.</p>"},{"location":"api/dataflow/#brainsmith.dataflow.OutputSchema.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Validate interface requirements.</p> Source code in <code>brainsmith/dataflow/schemas.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate interface requirements.\"\"\"\n    if self.required_layout and self.required_layout not in {\"NCHW\", \"NHWC\"}:\n        raise ValueError(\n            f\"Invalid required_layout '{self.required_layout}' for output '{self.name}'. \"\n            f\"Must be 'NCHW' or 'NHWC'.\"\n        )\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.ParameterSpec","title":"ParameterSpec  <code>dataclass</code>","text":"<pre><code>ParameterSpec(name: str, values: set[int | str] | Callable[[BuildContext], set[int | str]], type: Literal['int', 'string'] | None = None, default: int | str | None = None)\n</code></pre> <p>Explorable parameter in design space.</p> <p>Represents resource allocation or implementation choices that can be explored during DSE (ram_style, res_type, mem_mode, etc.).</p> <p>Does NOT include tiling dimensions (PE, SIMD) - those are auto-extracted from stream_tiling templates with valid values computed from factoring.</p> <p>Container Type Convention (Ordered vs Discrete):</p> <p>The container type determines how the dimension is treated during DSE:</p> <ul> <li> <p>list/tuple \u2192 OrderedParameter (ordered sequences with navigation)</p> <ul> <li>Supports min/max access, step_up/step_down, percentage-based indexing</li> <li>Values are sorted automatically</li> <li>Examples: depth=[128, 256, 512], num_layers=[1, 2, 4, 8]</li> </ul> </li> <li> <p>set/frozenset \u2192 Discrete (unordered categories)</p> <ul> <li>Membership testing only, no navigation</li> <li>Order doesn't matter</li> <li>Examples: ram_style={\"distributed\", \"block\"}, res_type={\"lut\", \"dsp\"}</li> </ul> </li> </ul> <p>Type Declaration (Hybrid Approach):</p> <ul> <li>Literal values: Type inferred from first value (optional to specify)</li> <li>Callable values: Type MUST be explicitly specified</li> </ul> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Dimension name (e.g., \"ram_style\", \"depth\")</p> <code>values</code> <code>set[int | str] | Callable[[BuildContext], set[int | str]]</code> <p>Valid values for this dimension - list/tuple: Ordered sequence (enables navigation methods) - set/frozenset: Discrete categories (membership only) - Callable: Computed from BuildContext (for context-dependent values)</p> <code>type</code> <code>Literal['int', 'string'] | None</code> <p>Value type (\"int\" or \"string\") - Required for callable values - Optional for literal values (inferred from first value) - Validated against values if both provided</p> <code>default</code> <code>int | str | None</code> <p>Default value (None = auto-select: min for ordered, first for discrete)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Ordered parameter - type inferred\n&gt;&gt;&gt; ParameterSpec(\"depth\", [128, 256, 512, 1024], default=256)\n</code></pre> <pre><code>&gt;&gt;&gt; # Discrete parameter - type inferred\n&gt;&gt;&gt; ParameterSpec(\"ram_style\", {\"distributed\", \"block\"}, default=\"distributed\")\n</code></pre> <pre><code>&gt;&gt;&gt; # Callable parameter - type required\n&gt;&gt;&gt; ParameterSpec(\"depth\", lambda ctx: compute_depths(ctx), type=\"int\", default=256)\n</code></pre> <pre><code>&gt;&gt;&gt; # Explicit type for documentation (optional)\n&gt;&gt;&gt; ParameterSpec(\"mode\", {\"fast\", \"accurate\"}, type=\"string\")\n</code></pre> Validation <ul> <li>Callable values without type \u2192 ValueError</li> <li>Type mismatch with literal values \u2192 ValueError</li> <li>Invalid type (not \"int\" or \"string\") \u2192 ValueError</li> </ul> Note <p>Tiling dimensions (PE, SIMD) are ALWAYS ordered (auto-wrapped in OrderedParameter) since they're computed as divisors (naturally ordered sequences).</p>"},{"location":"api/dataflow/#brainsmith.dataflow.ParameterSpec.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Validate type specification against values.</p> Source code in <code>brainsmith/dataflow/schemas.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate type specification against values.\"\"\"\n    # Callable values MUST specify type\n    if callable(self.values):\n        if self.type is None:\n            raise ValueError(\n                f\"ParameterSpec '{self.name}': Callable values require explicit type declaration. \"\n                f\"Specify type='int' or type='string'.\"\n            )\n        if self.type not in (\"int\", \"string\"):\n            raise ValueError(\n                f\"ParameterSpec '{self.name}': Invalid type '{self.type}'. \"\n                f\"Must be 'int' or 'string'.\"\n            )\n        return  # Cannot validate callable values without context\n\n    # Validate type matches literal values (if type specified)\n    if self.type is not None:\n        if self.type not in (\"int\", \"string\"):\n            raise ValueError(\n                f\"ParameterSpec '{self.name}': Invalid type '{self.type}'. \"\n                f\"Must be 'int' or 'string'.\"\n            )\n\n        first_val = next(iter(self.values))\n        expected_type = int if self.type == \"int\" else str\n\n        if not isinstance(first_val, expected_type):\n            actual_type = type(first_val).__name__\n            raise ValueError(\n                f\"ParameterSpec '{self.name}': Type mismatch. \"\n                f\"Declared type='{self.type}' but values contain {actual_type}. \"\n                f\"First value: {first_val!r}\"\n            )\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelDesignSpace","title":"KernelDesignSpace  <code>dataclass</code>","text":"<pre><code>KernelDesignSpace(name: str, inputs: dict[str, InterfaceDesignSpace], outputs: dict[str, InterfaceDesignSpace], internal_datatypes: dict[str, BaseDataType], optimization_constraints: list[Constraint], parameters: dict[str, Union[OrderedParameter, frozenset]])\n</code></pre> <p>Kernel design space built once, configured many times.</p> <p>Built by DesignSpaceBuilder from ONNX context, acts as factory for KernelDesignPoint via configure(). Contains structure constant during DSE plus valid ranges for all explorable dimensions.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Kernel name</p> <code>inputs</code> <code>dict[str, InterfaceDesignSpace]</code> <p>Input interface design spaces (by name)</p> <code>outputs</code> <code>dict[str, InterfaceDesignSpace]</code> <p>Output interface design spaces (by name)</p> <code>internal_datatypes</code> <code>dict[str, BaseDataType]</code> <p>Internal datatypes (e.g., accumulator)</p> <code>optimization_constraints</code> <code>list[Constraint]</code> <p>Parametric constraints validated at configure()</p> <code>parameters</code> <code>dict[str, Union[OrderedParameter, frozenset]]</code> <p>Explorable parameters - OrderedParameter (with navigation) or        frozenset (discrete categories like ram_style)</p>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelDesignSpace.input_list","title":"input_list  <code>property</code>","text":"<pre><code>input_list: list[InterfaceDesignSpace]\n</code></pre> <p>Inputs in declaration order (for ONNX positional mapping).</p> <p>Returns inputs as list preserving dict insertion order (Python 3.7+). Useful when mapping to ONNX node.input[i] positions.</p>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelDesignSpace.output_list","title":"output_list  <code>property</code>","text":"<pre><code>output_list: list[InterfaceDesignSpace]\n</code></pre> <p>Outputs in declaration order (for ONNX positional mapping).</p> <p>Returns outputs as list preserving dict insertion order (Python 3.7+). Useful when mapping to ONNX node.output[i] positions.</p>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelDesignSpace.configure","title":"configure","text":"<pre><code>configure(config: dict[str, int | str]) -&gt; KernelDesignPoint\n</code></pre> <p>Instantiate kernel at specified point in design space.</p> <p>Creates a KernelDesignPoint with resolved stream shapes and validates all parametric constraints.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict[str, int | str]</code> <p>Dimension values (tiling + resource) specifying the instance point</p> required <p>Returns:</p> Type Description <code>KernelDesignPoint</code> <p>KernelDesignPoint with fully resolved configuration</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If config invalid or missing dimensions</p> <code>ValidationError</code> <p>If parametric constraints fail</p> Source code in <code>brainsmith/dataflow/dse_models.py</code> <pre><code>def configure(self, config: dict[str, int | str]) -&gt; \"KernelDesignPoint\":\n    \"\"\"Instantiate kernel at specified point in design space.\n\n    Creates a KernelDesignPoint with resolved stream shapes and validates\n    all parametric constraints.\n\n    Args:\n        config: Dimension values (tiling + resource) specifying the instance point\n\n    Returns:\n        KernelDesignPoint with fully resolved configuration\n\n    Raises:\n        ValueError: If config invalid or missing dimensions\n        ValidationError: If parametric constraints fail\n    \"\"\"\n    self._validate_params(config)\n\n    interface_lookup = {}\n    inputs = self._instantiate_interfaces(self.inputs, config, interface_lookup)\n    outputs = self._instantiate_interfaces(self.outputs, config, interface_lookup)\n\n    instance = KernelDesignPoint(\n        design_space=self,\n        inputs=inputs,\n        outputs=outputs,\n        config=config,\n    )\n\n    self._validate_instance(instance, config)\n    return instance\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelDesignSpace.get_ordered_parameter","title":"get_ordered_parameter","text":"<pre><code>get_ordered_parameter(name: str) -&gt; OrderedParameter\n</code></pre> <p>Get ordered parameter by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Parameter name</p> required <p>Returns:</p> Type Description <code>OrderedParameter</code> <p>OrderedParameter instance</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If parameter not found</p> <code>TypeError</code> <p>If parameter is discrete (not ordered)</p> Source code in <code>brainsmith/dataflow/dse_models.py</code> <pre><code>def get_ordered_parameter(self, name: str) -&gt; \"OrderedParameter\":\n    \"\"\"Get ordered parameter by name.\n\n    Args:\n        name: Parameter name\n\n    Returns:\n        OrderedParameter instance\n\n    Raises:\n        KeyError: If parameter not found\n        TypeError: If parameter is discrete (not ordered)\n    \"\"\"\n    param = self.parameters[name]\n    if not isinstance(param, OrderedParameter):\n        raise TypeError(\n            f\"Parameter '{name}' is discrete (frozenset), not ordered. \"\n            f\"Use get_parameter() for type-agnostic access.\"\n        )\n    return param\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelDesignSpace.get_parameter","title":"get_parameter","text":"<pre><code>get_parameter(name: str) -&gt; Union[OrderedParameter, frozenset]\n</code></pre> <p>Get parameter by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Parameter name</p> required <p>Returns:</p> Type Description <code>Union[OrderedParameter, frozenset]</code> <p>OrderedParameter for ordered parameters, frozenset for discrete</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If parameter not found</p> Source code in <code>brainsmith/dataflow/dse_models.py</code> <pre><code>def get_parameter(self, name: str) -&gt; Union[\"OrderedParameter\", frozenset]:\n    \"\"\"Get parameter by name.\n\n    Args:\n        name: Parameter name\n\n    Returns:\n        OrderedParameter for ordered parameters, frozenset for discrete\n\n    Raises:\n        KeyError: If parameter not found\n    \"\"\"\n    return self.parameters[name]\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelDesignSpace.is_discrete_parameter","title":"is_discrete_parameter","text":"<pre><code>is_discrete_parameter(name: str) -&gt; bool\n</code></pre> <p>Check if parameter is discrete.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Parameter name</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if parameter is discrete (frozenset), False if ordered</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If parameter not found</p> Source code in <code>brainsmith/dataflow/dse_models.py</code> <pre><code>def is_discrete_parameter(self, name: str) -&gt; bool:\n    \"\"\"Check if parameter is discrete.\n\n    Args:\n        name: Parameter name\n\n    Returns:\n        True if parameter is discrete (frozenset), False if ordered\n\n    Raises:\n        KeyError: If parameter not found\n    \"\"\"\n    return isinstance(self.parameters[name], frozenset)\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelDesignSpace.is_ordered_parameter","title":"is_ordered_parameter","text":"<pre><code>is_ordered_parameter(name: str) -&gt; bool\n</code></pre> <p>Check if parameter is ordered.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Parameter name</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if parameter is OrderedParameter, False if discrete (frozenset)</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If parameter not found</p> Source code in <code>brainsmith/dataflow/dse_models.py</code> <pre><code>def is_ordered_parameter(self, name: str) -&gt; bool:\n    \"\"\"Check if parameter is ordered.\n\n    Args:\n        name: Parameter name\n\n    Returns:\n        True if parameter is OrderedParameter, False if discrete (frozenset)\n\n    Raises:\n        KeyError: If parameter not found\n    \"\"\"\n    return isinstance(self.parameters[name], OrderedParameter)\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.InterfaceDesignSpace","title":"InterfaceDesignSpace  <code>dataclass</code>","text":"<pre><code>InterfaceDesignSpace(name: str, tensor_shape: Shape, block_shape: Shape, stream_tiling: TilingSpec, datatype: BaseDataType, is_weight: bool = False, tensor_name: str | None = None, parallelism_dimension: OrderedParameter | None = None, parallelism_param: str | None = None)\n</code></pre> <p>Interface design space built once, configured many times.</p> <p>Defines interface structure constant during DSE. Stream tiling preserved as template for resolution with specific parallelization parameters.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Interface name</p> <code>tensor_shape</code> <code>Shape</code> <p>Full tensor dimensions</p> <code>block_shape</code> <code>Shape</code> <p>Block dimensions (per-operation tile size)</p> <code>stream_tiling</code> <code>TilingSpec</code> <p>Stream tiling template (e.g., [\"SIMD\"] or [1, 1, 1, \"PE\"])</p> <code>datatype</code> <code>BaseDataType</code> <p>Interface datatype</p> <code>is_weight</code> <code>bool</code> <p>Whether this is a weight tensor (constant)</p> <code>tensor_name</code> <code>str | None</code> <p>ONNX tensor name for initializer lookups</p> <code>parallelism_dimension</code> <code>OrderedParameter | None</code> <p>OrderedParameter for stream parameter (None if no parallelism)</p> <code>parallelism_param</code> <code>str | None</code> <p>Parameter name for stream dimension (e.g., \"SIMD\", \"PE\")</p>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelDesignPoint","title":"KernelDesignPoint  <code>dataclass</code>","text":"<pre><code>KernelDesignPoint(design_space: KernelDesignSpace, inputs: dict[str, InterfaceDesignPoint], outputs: dict[str, InterfaceDesignPoint], config: dict[str, int | str])\n</code></pre> <p>Immutable kernel instance at specific design point.</p> <p>Created by KernelDesignSpace.configure() with specific dimension values. Flyweight pattern minimizes memory - references parent design space, stores only configuration-specific data.</p> <p>Navigation methods return new instances - the design point itself is immutable. Use with_dimension(), with_step_up(), sweep_dimension() to explore the space.</p> <p>Attributes:</p> Name Type Description <code>design_space</code> <code>KernelDesignSpace</code> <p>Parent KernelDesignSpace</p> <code>inputs</code> <code>dict[str, InterfaceDesignPoint]</code> <p>Configured input interfaces (by name)</p> <code>outputs</code> <code>dict[str, InterfaceDesignPoint]</code> <p>Configured output interfaces (by name)</p> <code>config</code> <code>dict[str, int | str]</code> <p>Dimension values defining this point (e.g., {\"SIMD\": 16, \"PE\": 4})</p>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelDesignPoint.initiation_interval","title":"initiation_interval  <code>property</code>","text":"<pre><code>initiation_interval: int\n</code></pre> <p>Kernel initiation interval in cycles.</p>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelDesignPoint.input_list","title":"input_list  <code>property</code>","text":"<pre><code>input_list: list[InterfaceDesignPoint]\n</code></pre> <p>Inputs in declaration order (for ONNX positional mapping).</p>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelDesignPoint.max_block_folding_factor","title":"max_block_folding_factor  <code>property</code>","text":"<pre><code>max_block_folding_factor: int\n</code></pre> <p>Maximum block folding factor across all inputs.</p>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelDesignPoint.max_tensor_folding_factor","title":"max_tensor_folding_factor  <code>property</code>","text":"<pre><code>max_tensor_folding_factor: int\n</code></pre> <p>Maximum tensor folding factor across all inputs.</p>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelDesignPoint.output_list","title":"output_list  <code>property</code>","text":"<pre><code>output_list: list[InterfaceDesignPoint]\n</code></pre> <p>Outputs in declaration order (for ONNX positional mapping).</p>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelDesignPoint.total_output_values","title":"total_output_values  <code>property</code>","text":"<pre><code>total_output_values: int\n</code></pre> <p>Total output values across all outputs.</p>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelDesignPoint.get_input_stream_dimension","title":"get_input_stream_dimension","text":"<pre><code>get_input_stream_dimension(index: int) -&gt; Optional[OrderedParameter]\n</code></pre> <p>Get parallelism dimension for input interface.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Input interface index (0-based)</p> required <p>Returns:</p> Type Description <code>Optional[OrderedParameter]</code> <p>OrderedParameter or None if no parallelism</p> <p>Raises:</p> Type Description <code>IndexError</code> <p>If index out of range</p> Source code in <code>brainsmith/dataflow/dse_models.py</code> <pre><code>def get_input_stream_dimension(self, index: int) -&gt; Optional[\"OrderedParameter\"]:\n    \"\"\"Get parallelism dimension for input interface.\n\n    Args:\n        index: Input interface index (0-based)\n\n    Returns:\n        OrderedParameter or None if no parallelism\n\n    Raises:\n        IndexError: If index out of range\n    \"\"\"\n    if index &lt; 0 or index &gt;= len(self.input_list):\n        raise IndexError(f\"Input index {index} out of range [0, {len(self.input_list)})\")\n\n    return self.input_list[index].design_space.parallelism_dimension\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelDesignPoint.get_input_stream_param","title":"get_input_stream_param","text":"<pre><code>get_input_stream_param(index: int) -&gt; str | None\n</code></pre> <p>Get parallelism parameter name for input interface.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Input interface index (0-based)</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>Parameter name (e.g., \"SIMD\", \"PE\") or None if no parallelism</p> <p>Raises:</p> Type Description <code>IndexError</code> <p>If index out of range</p> Source code in <code>brainsmith/dataflow/dse_models.py</code> <pre><code>def get_input_stream_param(self, index: int) -&gt; str | None:\n    \"\"\"Get parallelism parameter name for input interface.\n\n    Args:\n        index: Input interface index (0-based)\n\n    Returns:\n        Parameter name (e.g., \"SIMD\", \"PE\") or None if no parallelism\n\n    Raises:\n        IndexError: If index out of range\n    \"\"\"\n    if index &lt; 0 or index &gt;= len(self.input_list):\n        raise IndexError(f\"Input index {index} out of range [0, {len(self.input_list)})\")\n\n    return self.input_list[index].design_space.parallelism_param\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelDesignPoint.get_input_stream_value","title":"get_input_stream_value","text":"<pre><code>get_input_stream_value(index: int) -&gt; int | None\n</code></pre> <p>Get current parallelism value for input interface.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Input interface index (0-based)</p> required <p>Returns:</p> Type Description <code>int | None</code> <p>Current parallelism value or None if no parallelism</p> <p>Raises:</p> Type Description <code>IndexError</code> <p>If index out of range</p> Source code in <code>brainsmith/dataflow/dse_models.py</code> <pre><code>def get_input_stream_value(self, index: int) -&gt; int | None:\n    \"\"\"Get current parallelism value for input interface.\n\n    Args:\n        index: Input interface index (0-based)\n\n    Returns:\n        Current parallelism value or None if no parallelism\n\n    Raises:\n        IndexError: If index out of range\n    \"\"\"\n    param = self.get_input_stream_param(index)\n    return self.config.get(param) if param else None\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelDesignPoint.get_output_stream_dimension","title":"get_output_stream_dimension","text":"<pre><code>get_output_stream_dimension(index: int) -&gt; Optional[OrderedParameter]\n</code></pre> <p>Get parallelism dimension for output interface.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Output interface index (0-based)</p> required <p>Returns:</p> Type Description <code>Optional[OrderedParameter]</code> <p>OrderedParameter or None if no parallelism</p> <p>Raises:</p> Type Description <code>IndexError</code> <p>If index out of range</p> Source code in <code>brainsmith/dataflow/dse_models.py</code> <pre><code>def get_output_stream_dimension(self, index: int) -&gt; Optional[\"OrderedParameter\"]:\n    \"\"\"Get parallelism dimension for output interface.\n\n    Args:\n        index: Output interface index (0-based)\n\n    Returns:\n        OrderedParameter or None if no parallelism\n\n    Raises:\n        IndexError: If index out of range\n    \"\"\"\n    if index &lt; 0 or index &gt;= len(self.output_list):\n        raise IndexError(f\"Output index {index} out of range [0, {len(self.output_list)})\")\n\n    return self.output_list[index].design_space.parallelism_dimension\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelDesignPoint.get_output_stream_param","title":"get_output_stream_param","text":"<pre><code>get_output_stream_param(index: int) -&gt; str | None\n</code></pre> <p>Get parallelism parameter name for output interface.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Output interface index (0-based)</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>Parameter name or None if no parallelism</p> <p>Raises:</p> Type Description <code>IndexError</code> <p>If index out of range</p> Source code in <code>brainsmith/dataflow/dse_models.py</code> <pre><code>def get_output_stream_param(self, index: int) -&gt; str | None:\n    \"\"\"Get parallelism parameter name for output interface.\n\n    Args:\n        index: Output interface index (0-based)\n\n    Returns:\n        Parameter name or None if no parallelism\n\n    Raises:\n        IndexError: If index out of range\n    \"\"\"\n    if index &lt; 0 or index &gt;= len(self.output_list):\n        raise IndexError(f\"Output index {index} out of range [0, {len(self.output_list)})\")\n\n    return self.output_list[index].design_space.parallelism_param\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelDesignPoint.get_output_stream_value","title":"get_output_stream_value","text":"<pre><code>get_output_stream_value(index: int) -&gt; int | None\n</code></pre> <p>Get current parallelism value for output interface.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Output interface index (0-based)</p> required <p>Returns:</p> Type Description <code>int | None</code> <p>Current parallelism value or None if no parallelism</p> <p>Raises:</p> Type Description <code>IndexError</code> <p>If index out of range</p> Source code in <code>brainsmith/dataflow/dse_models.py</code> <pre><code>def get_output_stream_value(self, index: int) -&gt; int | None:\n    \"\"\"Get current parallelism value for output interface.\n\n    Args:\n        index: Output interface index (0-based)\n\n    Returns:\n        Current parallelism value or None if no parallelism\n\n    Raises:\n        IndexError: If index out of range\n    \"\"\"\n    param = self.get_output_stream_param(index)\n    return self.config.get(param) if param else None\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelDesignPoint.output_stream_shape","title":"output_stream_shape","text":"<pre><code>output_stream_shape(output_idx: int = 0) -&gt; Shape\n</code></pre> <p>Stream shape for output.</p> <p>Returns the output's stream_shape attribute (resolved during configure).</p> Source code in <code>brainsmith/dataflow/dse_models.py</code> <pre><code>def output_stream_shape(self, output_idx: int = 0) -&gt; Shape:\n    \"\"\"Stream shape for output.\n\n    Returns the output's stream_shape attribute (resolved during configure).\n    \"\"\"\n    return self.output_list[output_idx].stream_shape\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelDesignPoint.output_stream_width_bits","title":"output_stream_width_bits","text":"<pre><code>output_stream_width_bits(output_idx: int = 0) -&gt; int\n</code></pre> <p>Stream width in bits for output.</p> <p>Returns the actual stream width based on the output's stream_shape.</p> Source code in <code>brainsmith/dataflow/dse_models.py</code> <pre><code>def output_stream_width_bits(self, output_idx: int = 0) -&gt; int:\n    \"\"\"Stream width in bits for output.\n\n    Returns the actual stream width based on the output's stream_shape.\n    \"\"\"\n    output = self.output_list[output_idx]\n    return output.stream_width_bits\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelDesignPoint.sweep_dimension","title":"sweep_dimension","text":"<pre><code>sweep_dimension(name: str, start: int | str | None = None, stop: int | str | None = None) -&gt; Iterator[KernelDesignPoint]\n</code></pre> <p>Sweep through all valid values for a dimension.</p> <p>For ordered dimensions, iterates in order from start to stop. For discrete dimensions, iterates in sorted order (ignores start/stop).</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Dimension to sweep</p> required <code>start</code> <code>int | str | None</code> <p>Start value (None = use min/first), ordered dims only</p> <code>None</code> <code>stop</code> <code>int | str | None</code> <p>Stop value (None = use max/last), ordered dims only</p> <code>None</code> <p>Yields:</p> Type Description <code>KernelDesignPoint</code> <p>KernelDesignPoint for each value in range</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If dimension not found</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Full sweep (ordered)\n&gt;&gt;&gt; for point in base.sweep_dimension(\"PE\"):\n...     evaluate(point)\n</code></pre> <pre><code>&gt;&gt;&gt; # Partial sweep (ordered)\n&gt;&gt;&gt; for point in base.sweep_dimension(\"SIMD\", start=8, stop=64):\n...     evaluate(point)\n</code></pre> <pre><code>&gt;&gt;&gt; # Discrete sweep (ignores start/stop)\n&gt;&gt;&gt; for point in base.sweep_dimension(\"ram_style\"):\n...     evaluate(point)\n</code></pre> Source code in <code>brainsmith/dataflow/dse_models.py</code> <pre><code>def sweep_dimension(\n    self, name: str, start: int | str | None = None, stop: int | str | None = None\n) -&gt; Iterator[\"KernelDesignPoint\"]:\n    \"\"\"Sweep through all valid values for a dimension.\n\n    For ordered dimensions, iterates in order from start to stop.\n    For discrete dimensions, iterates in sorted order (ignores start/stop).\n\n    Args:\n        name: Dimension to sweep\n        start: Start value (None = use min/first), ordered dims only\n        stop: Stop value (None = use max/last), ordered dims only\n\n    Yields:\n        KernelDesignPoint for each value in range\n\n    Raises:\n        KeyError: If dimension not found\n\n    Examples:\n        &gt;&gt;&gt; # Full sweep (ordered)\n        &gt;&gt;&gt; for point in base.sweep_dimension(\"PE\"):\n        ...     evaluate(point)\n\n        &gt;&gt;&gt; # Partial sweep (ordered)\n        &gt;&gt;&gt; for point in base.sweep_dimension(\"SIMD\", start=8, stop=64):\n        ...     evaluate(point)\n\n        &gt;&gt;&gt; # Discrete sweep (ignores start/stop)\n        &gt;&gt;&gt; for point in base.sweep_dimension(\"ram_style\"):\n        ...     evaluate(point)\n    \"\"\"\n    dim = self.design_space.get_parameter(name)\n\n    if isinstance(dim, OrderedParameter):\n        # Ordered: sweep in order from start to stop\n        start_idx = 0 if start is None else dim.index_of(start)\n        stop_idx = len(dim) - 1 if stop is None else dim.index_of(stop)\n\n        for idx in range(start_idx, stop_idx + 1):\n            value = dim.at_index(idx)\n            yield self.with_dimension(name, value)\n    else:  # frozenset\n        # Discrete: iterate in sorted order\n        for value in sorted(dim):\n            yield self.with_dimension(name, value)\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelDesignPoint.sweep_percentage","title":"sweep_percentage","text":"<pre><code>sweep_percentage(name: str, percentages: list[float], rounding: Literal['natural', 'down', 'up'] = 'natural') -&gt; Iterator[KernelDesignPoint]\n</code></pre> <p>Sweep through ordered dimension at specified percentage points.</p> <p>Only valid for ordered dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Ordered dimension to sweep</p> required <code>percentages</code> <code>list[float]</code> <p>List of percentage points (0.0-1.0)</p> required <code>rounding</code> <code>Literal['natural', 'down', 'up']</code> <p>Rounding mode for fractional indices</p> <code>'natural'</code> <p>Yields:</p> Type Description <code>KernelDesignPoint</code> <p>KernelDesignPoint for each percentage</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If dimension not found</p> <code>TypeError</code> <p>If dimension is discrete (not ordered)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Quartile sweep\n&gt;&gt;&gt; for point in base.sweep_percentage(\"PE\", [0.0, 0.25, 0.5, 0.75, 1.0]):\n...     evaluate(point)\n</code></pre> <pre><code>&gt;&gt;&gt; # Decile sweep\n&gt;&gt;&gt; deciles = [i/10 for i in range(11)]\n&gt;&gt;&gt; for point in base.sweep_percentage(\"SIMD\", deciles):\n...     evaluate(point)\n</code></pre> Source code in <code>brainsmith/dataflow/dse_models.py</code> <pre><code>def sweep_percentage(\n    self,\n    name: str,\n    percentages: list[float],\n    rounding: Literal[\"natural\", \"down\", \"up\"] = \"natural\",\n) -&gt; Iterator[\"KernelDesignPoint\"]:\n    \"\"\"Sweep through ordered dimension at specified percentage points.\n\n    Only valid for ordered dimensions.\n\n    Args:\n        name: Ordered dimension to sweep\n        percentages: List of percentage points (0.0-1.0)\n        rounding: Rounding mode for fractional indices\n\n    Yields:\n        KernelDesignPoint for each percentage\n\n    Raises:\n        KeyError: If dimension not found\n        TypeError: If dimension is discrete (not ordered)\n\n    Examples:\n        &gt;&gt;&gt; # Quartile sweep\n        &gt;&gt;&gt; for point in base.sweep_percentage(\"PE\", [0.0, 0.25, 0.5, 0.75, 1.0]):\n        ...     evaluate(point)\n\n        &gt;&gt;&gt; # Decile sweep\n        &gt;&gt;&gt; deciles = [i/10 for i in range(11)]\n        &gt;&gt;&gt; for point in base.sweep_percentage(\"SIMD\", deciles):\n        ...     evaluate(point)\n    \"\"\"\n    for pct in percentages:\n        yield self.with_percentage(name, pct, rounding)\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelDesignPoint.with_dimension","title":"with_dimension","text":"<pre><code>with_dimension(name: str, value: int | str) -&gt; KernelDesignPoint\n</code></pre> <p>Create new design point with specified dimension value.</p> <p>Works for both ordered and discrete dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Dimension name</p> required <code>value</code> <code>int | str</code> <p>New value for dimension</p> required <p>Returns:</p> Type Description <code>KernelDesignPoint</code> <p>New KernelDesignPoint with updated dimension</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If dimension not found</p> <code>ValueError</code> <p>If value not valid for dimension</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; point = design_space.configure({\"SIMD\": 4, \"PE\": 1})\n&gt;&gt;&gt; point2 = point.with_dimension(\"SIMD\", 8)\n&gt;&gt;&gt; point2.config[\"SIMD\"]\n8\n</code></pre> Source code in <code>brainsmith/dataflow/dse_models.py</code> <pre><code>def with_dimension(self, name: str, value: int | str) -&gt; \"KernelDesignPoint\":\n    \"\"\"Create new design point with specified dimension value.\n\n    Works for both ordered and discrete dimensions.\n\n    Args:\n        name: Dimension name\n        value: New value for dimension\n\n    Returns:\n        New KernelDesignPoint with updated dimension\n\n    Raises:\n        KeyError: If dimension not found\n        ValueError: If value not valid for dimension\n\n    Examples:\n        &gt;&gt;&gt; point = design_space.configure({\"SIMD\": 4, \"PE\": 1})\n        &gt;&gt;&gt; point2 = point.with_dimension(\"SIMD\", 8)\n        &gt;&gt;&gt; point2.config[\"SIMD\"]\n        8\n    \"\"\"\n    new_config = {**self.config, name: value}\n    return self.design_space.configure(new_config)\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelDesignPoint.with_input_stream","title":"with_input_stream","text":"<pre><code>with_input_stream(index: int, value: int) -&gt; KernelDesignPoint\n</code></pre> <p>Set input interface stream parallelism by index.</p> <p>Convenience method for interface-agnostic parallelism navigation. Automatically resolves the parallelism parameter name from the interface.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Input interface index (0-based)</p> required <code>value</code> <code>int</code> <p>Parallelism value</p> required <p>Returns:</p> Type Description <code>KernelDesignPoint</code> <p>New KernelDesignPoint with updated parallelism</p> <p>Raises:</p> Type Description <code>IndexError</code> <p>If index out of range</p> <code>ValueError</code> <p>If interface has no parallelism parameter or value invalid</p> Example Source code in <code>brainsmith/dataflow/dse_models.py</code> <pre><code>def with_input_stream(self, index: int, value: int) -&gt; \"KernelDesignPoint\":\n    \"\"\"Set input interface stream parallelism by index.\n\n    Convenience method for interface-agnostic parallelism navigation.\n    Automatically resolves the parallelism parameter name from the interface.\n\n    Args:\n        index: Input interface index (0-based)\n        value: Parallelism value\n\n    Returns:\n        New KernelDesignPoint with updated parallelism\n\n    Raises:\n        IndexError: If index out of range\n        ValueError: If interface has no parallelism parameter or value invalid\n\n    Example:\n        &gt;&gt;&gt; # Set first input to PE=16\n        &gt;&gt;&gt; point2 = point.with_input_stream(0, 16)\n    \"\"\"\n    return self._with_stream_helper(self.input_list, \"Input\", index, value)\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelDesignPoint.with_input_stream--set-first-input-to-pe16","title":"Set first input to PE=16","text":"<p>point2 = point.with_input_stream(0, 16)</p>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelDesignPoint.with_input_stream_percentage","title":"with_input_stream_percentage","text":"<pre><code>with_input_stream_percentage(index: int, percentage: float, rounding: Literal['natural', 'down', 'up'] = 'natural') -&gt; KernelDesignPoint\n</code></pre> <p>Set input stream parallelism to percentage of range.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Input interface index (0-based)</p> required <code>percentage</code> <code>float</code> <p>Value from 0.0 to 1.0 (0.0=min, 1.0=max)</p> required <code>rounding</code> <code>Literal['natural', 'down', 'up']</code> <p>How to round fractional indices</p> <code>'natural'</code> <p>Returns:</p> Type Description <code>KernelDesignPoint</code> <p>New KernelDesignPoint with parallelism at percentage</p> <p>Raises:</p> Type Description <code>IndexError</code> <p>If index out of range</p> <code>ValueError</code> <p>If interface has no parallelism parameter or percentage invalid</p> Source code in <code>brainsmith/dataflow/dse_models.py</code> <pre><code>def with_input_stream_percentage(\n    self, index: int, percentage: float, rounding: Literal[\"natural\", \"down\", \"up\"] = \"natural\"\n) -&gt; \"KernelDesignPoint\":\n    \"\"\"Set input stream parallelism to percentage of range.\n\n    Args:\n        index: Input interface index (0-based)\n        percentage: Value from 0.0 to 1.0 (0.0=min, 1.0=max)\n        rounding: How to round fractional indices\n\n    Returns:\n        New KernelDesignPoint with parallelism at percentage\n\n    Raises:\n        IndexError: If index out of range\n        ValueError: If interface has no parallelism parameter or percentage invalid\n    \"\"\"\n    return self._with_stream_percentage_helper(\n        self.input_list, \"Input\", index, percentage, rounding\n    )\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelDesignPoint.with_max","title":"with_max","text":"<pre><code>with_max(name: str) -&gt; KernelDesignPoint\n</code></pre> <p>Create new design point with ordered dimension at maximum.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Dimension name (must be ordered)</p> required <p>Returns:</p> Type Description <code>KernelDesignPoint</code> <p>New KernelDesignPoint with dimension at maximum</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If dimension not found</p> <code>TypeError</code> <p>If dimension is discrete (not ordered)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; point = design_space.configure({\"SIMD\": 8, \"PE\": 4})\n&gt;&gt;&gt; point2 = point.with_max(\"SIMD\")\n&gt;&gt;&gt; point2.config[\"SIMD\"]\n64\n</code></pre> Source code in <code>brainsmith/dataflow/dse_models.py</code> <pre><code>def with_max(self, name: str) -&gt; \"KernelDesignPoint\":\n    \"\"\"Create new design point with ordered dimension at maximum.\n\n    Args:\n        name: Dimension name (must be ordered)\n\n    Returns:\n        New KernelDesignPoint with dimension at maximum\n\n    Raises:\n        KeyError: If dimension not found\n        TypeError: If dimension is discrete (not ordered)\n\n    Examples:\n        &gt;&gt;&gt; point = design_space.configure({\"SIMD\": 8, \"PE\": 4})\n        &gt;&gt;&gt; point2 = point.with_max(\"SIMD\")\n        &gt;&gt;&gt; point2.config[\"SIMD\"]\n        64\n    \"\"\"\n    max_val = self.design_space.get_ordered_parameter(name).max()\n    return self.with_dimension(name, max_val)\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelDesignPoint.with_min","title":"with_min","text":"<pre><code>with_min(name: str) -&gt; KernelDesignPoint\n</code></pre> <p>Create new design point with ordered dimension at minimum.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Dimension name (must be ordered)</p> required <p>Returns:</p> Type Description <code>KernelDesignPoint</code> <p>New KernelDesignPoint with dimension at minimum</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If dimension not found</p> <code>TypeError</code> <p>If dimension is discrete (not ordered)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; point = design_space.configure({\"SIMD\": 8, \"PE\": 4})\n&gt;&gt;&gt; point2 = point.with_min(\"SIMD\")\n&gt;&gt;&gt; point2.config[\"SIMD\"]\n1\n</code></pre> Source code in <code>brainsmith/dataflow/dse_models.py</code> <pre><code>def with_min(self, name: str) -&gt; \"KernelDesignPoint\":\n    \"\"\"Create new design point with ordered dimension at minimum.\n\n    Args:\n        name: Dimension name (must be ordered)\n\n    Returns:\n        New KernelDesignPoint with dimension at minimum\n\n    Raises:\n        KeyError: If dimension not found\n        TypeError: If dimension is discrete (not ordered)\n\n    Examples:\n        &gt;&gt;&gt; point = design_space.configure({\"SIMD\": 8, \"PE\": 4})\n        &gt;&gt;&gt; point2 = point.with_min(\"SIMD\")\n        &gt;&gt;&gt; point2.config[\"SIMD\"]\n        1\n    \"\"\"\n    min_val = self.design_space.get_ordered_parameter(name).min()\n    return self.with_dimension(name, min_val)\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelDesignPoint.with_output_stream","title":"with_output_stream","text":"<pre><code>with_output_stream(index: int, value: int) -&gt; KernelDesignPoint\n</code></pre> <p>Set output interface stream parallelism by index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Output interface index (0-based)</p> required <code>value</code> <code>int</code> <p>Parallelism value</p> required <p>Returns:</p> Type Description <code>KernelDesignPoint</code> <p>New KernelDesignPoint with updated parallelism</p> <p>Raises:</p> Type Description <code>IndexError</code> <p>If index out of range</p> <code>ValueError</code> <p>If interface has no parallelism parameter or value invalid</p> Source code in <code>brainsmith/dataflow/dse_models.py</code> <pre><code>def with_output_stream(self, index: int, value: int) -&gt; \"KernelDesignPoint\":\n    \"\"\"Set output interface stream parallelism by index.\n\n    Args:\n        index: Output interface index (0-based)\n        value: Parallelism value\n\n    Returns:\n        New KernelDesignPoint with updated parallelism\n\n    Raises:\n        IndexError: If index out of range\n        ValueError: If interface has no parallelism parameter or value invalid\n    \"\"\"\n    return self._with_stream_helper(self.output_list, \"Output\", index, value)\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelDesignPoint.with_output_stream_percentage","title":"with_output_stream_percentage","text":"<pre><code>with_output_stream_percentage(index: int, percentage: float, rounding: Literal['natural', 'down', 'up'] = 'natural') -&gt; KernelDesignPoint\n</code></pre> <p>Set output stream parallelism to percentage of range.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Output interface index (0-based)</p> required <code>percentage</code> <code>float</code> <p>Value from 0.0 to 1.0 (0.0=min, 1.0=max)</p> required <code>rounding</code> <code>Literal['natural', 'down', 'up']</code> <p>How to round fractional indices</p> <code>'natural'</code> <p>Returns:</p> Type Description <code>KernelDesignPoint</code> <p>New KernelDesignPoint with parallelism at percentage</p> <p>Raises:</p> Type Description <code>IndexError</code> <p>If index out of range</p> <code>ValueError</code> <p>If interface has no parallelism parameter or percentage invalid</p> Source code in <code>brainsmith/dataflow/dse_models.py</code> <pre><code>def with_output_stream_percentage(\n    self, index: int, percentage: float, rounding: Literal[\"natural\", \"down\", \"up\"] = \"natural\"\n) -&gt; \"KernelDesignPoint\":\n    \"\"\"Set output stream parallelism to percentage of range.\n\n    Args:\n        index: Output interface index (0-based)\n        percentage: Value from 0.0 to 1.0 (0.0=min, 1.0=max)\n        rounding: How to round fractional indices\n\n    Returns:\n        New KernelDesignPoint with parallelism at percentage\n\n    Raises:\n        IndexError: If index out of range\n        ValueError: If interface has no parallelism parameter or percentage invalid\n    \"\"\"\n    return self._with_stream_percentage_helper(\n        self.output_list, \"Output\", index, percentage, rounding\n    )\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelDesignPoint.with_percentage","title":"with_percentage","text":"<pre><code>with_percentage(name: str, percentage: float, rounding: str = 'natural') -&gt; KernelDesignPoint\n</code></pre> <p>Create new design point with ordered dimension at percentage.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Dimension name (must be ordered)</p> required <code>percentage</code> <code>float</code> <p>Position in range [0.0, 1.0]</p> required <code>rounding</code> <code>str</code> <p>'natural', 'down', or 'up'</p> <code>'natural'</code> <p>Returns:</p> Type Description <code>KernelDesignPoint</code> <p>New KernelDesignPoint with dimension at percentage</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If dimension not found</p> <code>TypeError</code> <p>If dimension is discrete (not ordered)</p> <code>ValueError</code> <p>If percentage out of range</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; point = design_space.configure({\"SIMD\": 4, \"PE\": 1})\n&gt;&gt;&gt; point2 = point.with_percentage(\"SIMD\", 0.5)\n&gt;&gt;&gt; point2.config[\"SIMD\"]\n8\n</code></pre> Source code in <code>brainsmith/dataflow/dse_models.py</code> <pre><code>def with_percentage(\n    self, name: str, percentage: float, rounding: str = \"natural\"\n) -&gt; \"KernelDesignPoint\":\n    \"\"\"Create new design point with ordered dimension at percentage.\n\n    Args:\n        name: Dimension name (must be ordered)\n        percentage: Position in range [0.0, 1.0]\n        rounding: 'natural', 'down', or 'up'\n\n    Returns:\n        New KernelDesignPoint with dimension at percentage\n\n    Raises:\n        KeyError: If dimension not found\n        TypeError: If dimension is discrete (not ordered)\n        ValueError: If percentage out of range\n\n    Examples:\n        &gt;&gt;&gt; point = design_space.configure({\"SIMD\": 4, \"PE\": 1})\n        &gt;&gt;&gt; point2 = point.with_percentage(\"SIMD\", 0.5)\n        &gt;&gt;&gt; point2.config[\"SIMD\"]\n        8\n    \"\"\"\n    value = self.design_space.get_ordered_parameter(name).at_percentage(percentage, rounding)\n    return self.with_dimension(name, value)\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelDesignPoint.with_step_down","title":"with_step_down","text":"<pre><code>with_step_down(name: str, n: int = 1) -&gt; KernelDesignPoint\n</code></pre> <p>Create new design point with ordered dimension stepped down.</p> <p>Clamps at minimum if n steps would go below bounds.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Dimension name (must be ordered)</p> required <code>n</code> <code>int</code> <p>Number of steps to move down (default 1)</p> <code>1</code> <p>Returns:</p> Type Description <code>KernelDesignPoint</code> <p>New KernelDesignPoint with dimension stepped down</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If dimension not found</p> <code>TypeError</code> <p>If dimension is discrete (not ordered)</p> <code>ValueError</code> <p>If current value not in dimension or n &lt; 0</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; point = design_space.configure({\"SIMD\": 16, \"PE\": 4})\n&gt;&gt;&gt; point2 = point.with_step_down(\"SIMD\", 1)\n&gt;&gt;&gt; point2.config[\"SIMD\"]\n8\n</code></pre> Source code in <code>brainsmith/dataflow/dse_models.py</code> <pre><code>def with_step_down(self, name: str, n: int = 1) -&gt; \"KernelDesignPoint\":\n    \"\"\"Create new design point with ordered dimension stepped down.\n\n    Clamps at minimum if n steps would go below bounds.\n\n    Args:\n        name: Dimension name (must be ordered)\n        n: Number of steps to move down (default 1)\n\n    Returns:\n        New KernelDesignPoint with dimension stepped down\n\n    Raises:\n        KeyError: If dimension not found\n        TypeError: If dimension is discrete (not ordered)\n        ValueError: If current value not in dimension or n &lt; 0\n\n    Examples:\n        &gt;&gt;&gt; point = design_space.configure({\"SIMD\": 16, \"PE\": 4})\n        &gt;&gt;&gt; point2 = point.with_step_down(\"SIMD\", 1)\n        &gt;&gt;&gt; point2.config[\"SIMD\"]\n        8\n    \"\"\"\n    dim = self.design_space.get_ordered_parameter(name)\n    current = self.config[name]\n    new_val = dim.step_down(current, n)\n    return self.with_dimension(name, new_val)\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.KernelDesignPoint.with_step_up","title":"with_step_up","text":"<pre><code>with_step_up(name: str, n: int = 1) -&gt; KernelDesignPoint\n</code></pre> <p>Create new design point with ordered dimension stepped up.</p> <p>Clamps at maximum if n steps would exceed bounds.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Dimension name (must be ordered)</p> required <code>n</code> <code>int</code> <p>Number of steps to move up (default 1)</p> <code>1</code> <p>Returns:</p> Type Description <code>KernelDesignPoint</code> <p>New KernelDesignPoint with dimension stepped up</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If dimension not found</p> <code>TypeError</code> <p>If dimension is discrete (not ordered)</p> <code>ValueError</code> <p>If current value not in dimension or n &lt; 0</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; point = design_space.configure({\"SIMD\": 4, \"PE\": 1})\n&gt;&gt;&gt; point2 = point.with_step_up(\"SIMD\", 2)\n&gt;&gt;&gt; point2.config[\"SIMD\"]\n16\n</code></pre> Source code in <code>brainsmith/dataflow/dse_models.py</code> <pre><code>def with_step_up(self, name: str, n: int = 1) -&gt; \"KernelDesignPoint\":\n    \"\"\"Create new design point with ordered dimension stepped up.\n\n    Clamps at maximum if n steps would exceed bounds.\n\n    Args:\n        name: Dimension name (must be ordered)\n        n: Number of steps to move up (default 1)\n\n    Returns:\n        New KernelDesignPoint with dimension stepped up\n\n    Raises:\n        KeyError: If dimension not found\n        TypeError: If dimension is discrete (not ordered)\n        ValueError: If current value not in dimension or n &lt; 0\n\n    Examples:\n        &gt;&gt;&gt; point = design_space.configure({\"SIMD\": 4, \"PE\": 1})\n        &gt;&gt;&gt; point2 = point.with_step_up(\"SIMD\", 2)\n        &gt;&gt;&gt; point2.config[\"SIMD\"]\n        16\n    \"\"\"\n    dim = self.design_space.get_ordered_parameter(name)\n    current = self.config[name]\n    new_val = dim.step_up(current, n)\n    return self.with_dimension(name, new_val)\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.InterfaceDesignPoint","title":"InterfaceDesignPoint  <code>dataclass</code>","text":"<pre><code>InterfaceDesignPoint(design_space: InterfaceDesignSpace, stream_shape: Shape)\n</code></pre> <p>Interface instance with resolved parallelization.</p> <p>Flyweight pattern: references parent design space, stores only configuration- specific stream_shape. Delegates tensor_shape, block_shape, and datatype to design space for minimal memory overhead.</p> <p>Attributes:</p> Name Type Description <code>design_space</code> <code>InterfaceDesignSpace</code> <p>Parent InterfaceDesignSpace</p> <code>stream_shape</code> <code>Shape</code> <p>Resolved stream dimensions for this configuration</p>"},{"location":"api/dataflow/#brainsmith.dataflow.InterfaceDesignPoint.block_folding_factor","title":"block_folding_factor  <code>property</code>","text":"<pre><code>block_folding_factor: int\n</code></pre> <p>Cycles to stream one block.</p> <p>Product of stream_cycles_shape. Uses ceiling division: a block of size 32 with stream width 10 requires ceil(32/10) = 4 cycles (3 full + 1 partial).</p>"},{"location":"api/dataflow/#brainsmith.dataflow.InterfaceDesignPoint.stream_cycles_shape","title":"stream_cycles_shape  <code>property</code>","text":"<pre><code>stream_cycles_shape: Shape\n</code></pre> <p>Per-dimension cycles needed to stream one block.</p> <p>Returns shape where each dimension is ceil(block_dim / stream_dim). Describes temporal execution: how we stream each tile.</p> block_shape=(32, 16), stream_shape=(8, 4) <p>\u2192 stream_cycles_shape=(4, 4)  # 4x4 cycles per block</p>"},{"location":"api/dataflow/#brainsmith.dataflow.InterfaceDesignPoint.stream_width_bits","title":"stream_width_bits  <code>property</code>","text":"<pre><code>stream_width_bits: int\n</code></pre> <p>Stream width in bits.</p>"},{"location":"api/dataflow/#brainsmith.dataflow.InterfaceDesignPoint.streaming_bandwidth","title":"streaming_bandwidth  <code>property</code>","text":"<pre><code>streaming_bandwidth: int\n</code></pre> <p>Elements streamed per cycle.</p>"},{"location":"api/dataflow/#brainsmith.dataflow.InterfaceDesignPoint.tensor_blocks_shape","title":"tensor_blocks_shape  <code>property</code>","text":"<pre><code>tensor_blocks_shape: Shape\n</code></pre> <p>Per-dimension blocks needed to tile tensor.</p> <p>Returns shape where each dimension is ceil(tensor_dim / block_dim). Describes spatial decomposition: how we tile the problem.</p> tensor_shape=(100, 64), block_shape=(32, 16) <p>\u2192 tensor_blocks_shape=(4, 4)  # 4x4 grid of blocks</p>"},{"location":"api/dataflow/#brainsmith.dataflow.InterfaceDesignPoint.tensor_folding_factor","title":"tensor_folding_factor  <code>property</code>","text":"<pre><code>tensor_folding_factor: int\n</code></pre> <p>Number of blocks needed to cover full tensor.</p> <p>Product of tensor_blocks_shape. Uses ceiling division: a tensor of size 100 with block size 32 requires ceil(100/32) = 4 blocks (3 full + 1 partial).</p>"},{"location":"api/dataflow/#brainsmith.dataflow.InterfaceDesignPoint.get_shape","title":"get_shape","text":"<pre><code>get_shape(hierarchy: ShapeHierarchy) -&gt; Shape\n</code></pre> <p>Get shape at specified hierarchy level.</p> <p>Parameters:</p> Name Type Description Default <code>hierarchy</code> <code>ShapeHierarchy</code> <p>Which level of the shape hierarchy to retrieve</p> required <p>Returns:</p> Type Description <code>Shape</code> <p>Shape at the specified level</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If hierarchy is invalid</p> Source code in <code>brainsmith/dataflow/dse_models.py</code> <pre><code>def get_shape(self, hierarchy: ShapeHierarchy) -&gt; Shape:\n    \"\"\"Get shape at specified hierarchy level.\n\n    Args:\n        hierarchy: Which level of the shape hierarchy to retrieve\n\n    Returns:\n        Shape at the specified level\n\n    Raises:\n        ValueError: If hierarchy is invalid\n    \"\"\"\n    if hierarchy == ShapeHierarchy.STREAM:\n        return self.stream_shape\n    elif hierarchy == ShapeHierarchy.BLOCK:\n        return self.design_space.block_shape\n    elif hierarchy == ShapeHierarchy.TENSOR:\n        return self.design_space.tensor_shape\n    else:\n        raise ValueError(f\"Invalid hierarchy: {hierarchy}\")\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.OrderedParameter","title":"OrderedParameter  <code>dataclass</code>","text":"<pre><code>OrderedParameter(name: str, values: tuple[int, ...], default: int | None = None)\n</code></pre> <p>Ordered parameter for DSE navigation.</p> <p>Stores discrete values in sorted order, enabling navigation operations like stepping, percentage-based indexing, and min/max access.</p> <p>Used for parallelization parameters (PE, SIMD, MW, MH) and other explorable parameters with natural ordering (depth, num_layers, etc.).</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Parameter name (e.g., \"SIMD\", \"PE\", \"depth\")</p> <code>values</code> <code>tuple[int, ...]</code> <p>Sorted tuple of valid values</p> <code>default</code> <code>int | None</code> <p>Default value (None = minimum)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; simd = OrderedParameter(\"SIMD\", (1, 2, 4, 8, 16, 32, 64))\n&gt;&gt;&gt; simd.min()\n1\n&gt;&gt;&gt; simd.at_percentage(0.5)\n8\n&gt;&gt;&gt; simd.step_up(8, n=2)\n32\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.OrderedParameter.__contains__","title":"__contains__","text":"<pre><code>__contains__(value: int) -&gt; bool\n</code></pre> <p>Check if value in parameter (for 'value in param' syntax).</p> Source code in <code>brainsmith/dataflow/ordered_parameter.py</code> <pre><code>def __contains__(self, value: int) -&gt; bool:\n    \"\"\"Check if value in parameter (for 'value in param' syntax).\"\"\"\n    return value in self.values\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.OrderedParameter.__iter__","title":"__iter__","text":"<pre><code>__iter__() -&gt; Iterator[int]\n</code></pre> <p>Iterate over values in order.</p> Source code in <code>brainsmith/dataflow/ordered_parameter.py</code> <pre><code>def __iter__(self) -&gt; Iterator[int]:\n    \"\"\"Iterate over values in order.\"\"\"\n    return iter(self.values)\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.OrderedParameter.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Number of valid values.</p> Source code in <code>brainsmith/dataflow/ordered_parameter.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Number of valid values.\"\"\"\n    return len(self.values)\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.OrderedParameter.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Validate invariants: sorted, unique, non-empty.</p> Source code in <code>brainsmith/dataflow/ordered_parameter.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate invariants: sorted, unique, non-empty.\"\"\"\n    if not self.values:\n        raise ValueError(f\"OrderedParameter '{self.name}' has empty values\")\n\n    # Ensure tuple (not list)\n    if not isinstance(self.values, tuple):\n        object.__setattr__(self, \"values\", tuple(self.values))\n\n    # Validate sorted\n    if self.values != tuple(sorted(self.values)):\n        raise ValueError(\n            f\"OrderedParameter '{self.name}' values must be sorted ascending. \"\n            f\"Got: {self.values}\"\n        )\n\n    # Validate unique\n    if len(self.values) != len(set(self.values)):\n        # Find duplicates efficiently (O(n) instead of O(n\u00b2))\n        seen = set()\n        duplicates = {v for v in self.values if v in seen or seen.add(v)}\n        raise ValueError(f\"OrderedParameter '{self.name}' has duplicate values: {duplicates}\")\n\n    # Validate default (if specified)\n    if self.default is not None and self.default not in self.values:\n        raise ValueError(\n            f\"Default value {self.default} not in parameter '{self.name}'. \"\n            f\"Valid values: {self.values}\"\n        )\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.OrderedParameter.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> <p>String representation.</p> Source code in <code>brainsmith/dataflow/ordered_parameter.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"String representation.\"\"\"\n    if len(self.values) &lt;= 6:\n        vals = str(self.values)\n    else:\n        vals = f\"({self.values[0]}, {self.values[1]}, ..., {self.values[-1]})\"\n\n    default_str = f\", default={self.default}\" if self.default else \"\"\n    return f\"OrderedParameter('{self.name}', {vals}{default_str})\"\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.OrderedParameter.at_index","title":"at_index","text":"<pre><code>at_index(idx: int) -&gt; int\n</code></pre> <p>Get value at index (supports negative indexing).</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index position (0-based, supports negative like Python lists)</p> required <p>Returns:</p> Type Description <code>int</code> <p>Value at index</p> <p>Raises:</p> Type Description <code>IndexError</code> <p>If index out of range</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; param = OrderedParameter(\"PE\", (1, 2, 4, 8, 16))\n&gt;&gt;&gt; param.at_index(0)\n1\n&gt;&gt;&gt; param.at_index(-1)\n16\n&gt;&gt;&gt; param.at_index(2)\n4\n</code></pre> Source code in <code>brainsmith/dataflow/ordered_parameter.py</code> <pre><code>def at_index(self, idx: int) -&gt; int:\n    \"\"\"Get value at index (supports negative indexing).\n\n    Args:\n        idx: Index position (0-based, supports negative like Python lists)\n\n    Returns:\n        Value at index\n\n    Raises:\n        IndexError: If index out of range\n\n    Examples:\n        &gt;&gt;&gt; param = OrderedParameter(\"PE\", (1, 2, 4, 8, 16))\n        &gt;&gt;&gt; param.at_index(0)\n        1\n        &gt;&gt;&gt; param.at_index(-1)\n        16\n        &gt;&gt;&gt; param.at_index(2)\n        4\n    \"\"\"\n    if not -len(self.values) &lt;= idx &lt; len(self.values):\n        raise IndexError(\n            f\"Index {idx} out of range for parameter '{self.name}' \"\n            f\"(length {len(self.values)})\"\n        )\n    return self.values[idx]\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.OrderedParameter.at_percentage","title":"at_percentage","text":"<pre><code>at_percentage(percentage: float, rounding: Literal['natural', 'down', 'up'] = 'natural') -&gt; int\n</code></pre> <p>Get value at percentage position in ordered sequence (0.0-1.0).</p> <p>Maps percentage to continuous index space, then rounds to discrete index. Useful for sweeping through parameter at regular intervals regardless of actual vector length.</p> <p>Parameters:</p> Name Type Description Default <code>percentage</code> <code>float</code> <p>Position in range [0.0, 1.0] - 0.0 \u2192 first value (min) - 1.0 \u2192 last value (max) - 0.5 \u2192 middle value</p> required <code>rounding</code> <code>Literal['natural', 'down', 'up']</code> <p>How to round fractional indices - 'natural': Round to nearest (default, balanced) - 'down': Floor (conservative, prefer smaller values) - 'up': Ceiling (aggressive, prefer larger values)</p> <code>'natural'</code> <p>Returns:</p> Type Description <code>int</code> <p>Value at percentage position</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If percentage not in [0.0, 1.0] or invalid rounding mode</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; param = OrderedParameter(\"PE\", (1, 2, 4, 8, 16))  # 5 values\n&gt;&gt;&gt; param.at_percentage(0.0)\n1\n&gt;&gt;&gt; param.at_percentage(1.0)\n16\n&gt;&gt;&gt; param.at_percentage(0.5, rounding='natural')\n4  # Middle value (index 2 of 0-4)\n&gt;&gt;&gt; param.at_percentage(0.75, rounding='down')\n8  # 0.75 * 4 = 3.0 \u2192 floor(3.0) = 3 \u2192 values[3] = 8\n</code></pre> <pre><code>&gt;&gt;&gt; # With 4 values, percentages map cleanly to indices\n&gt;&gt;&gt; param4 = OrderedParameter(\"X\", (10, 20, 30, 40))\n&gt;&gt;&gt; param4.at_percentage(0.0)\n10  # 0.0 * 3 = 0\n&gt;&gt;&gt; param4.at_percentage(0.333, rounding='natural')\n20  # 0.333 * 3 \u2248 1.0 \u2192 round(1.0) = 1\n&gt;&gt;&gt; param4.at_percentage(1.0)\n40  # 1.0 * 3 = 3\n</code></pre> Source code in <code>brainsmith/dataflow/ordered_parameter.py</code> <pre><code>def at_percentage(\n    self, percentage: float, rounding: Literal[\"natural\", \"down\", \"up\"] = \"natural\"\n) -&gt; int:\n    \"\"\"Get value at percentage position in ordered sequence (0.0-1.0).\n\n    Maps percentage to continuous index space, then rounds to discrete index.\n    Useful for sweeping through parameter at regular intervals regardless\n    of actual vector length.\n\n    Args:\n        percentage: Position in range [0.0, 1.0]\n            - 0.0 \u2192 first value (min)\n            - 1.0 \u2192 last value (max)\n            - 0.5 \u2192 middle value\n        rounding: How to round fractional indices\n            - 'natural': Round to nearest (default, balanced)\n            - 'down': Floor (conservative, prefer smaller values)\n            - 'up': Ceiling (aggressive, prefer larger values)\n\n    Returns:\n        Value at percentage position\n\n    Raises:\n        ValueError: If percentage not in [0.0, 1.0] or invalid rounding mode\n\n    Examples:\n        &gt;&gt;&gt; param = OrderedParameter(\"PE\", (1, 2, 4, 8, 16))  # 5 values\n        &gt;&gt;&gt; param.at_percentage(0.0)\n        1\n        &gt;&gt;&gt; param.at_percentage(1.0)\n        16\n        &gt;&gt;&gt; param.at_percentage(0.5, rounding='natural')\n        4  # Middle value (index 2 of 0-4)\n        &gt;&gt;&gt; param.at_percentage(0.75, rounding='down')\n        8  # 0.75 * 4 = 3.0 \u2192 floor(3.0) = 3 \u2192 values[3] = 8\n\n        &gt;&gt;&gt; # With 4 values, percentages map cleanly to indices\n        &gt;&gt;&gt; param4 = OrderedParameter(\"X\", (10, 20, 30, 40))\n        &gt;&gt;&gt; param4.at_percentage(0.0)\n        10  # 0.0 * 3 = 0\n        &gt;&gt;&gt; param4.at_percentage(0.333, rounding='natural')\n        20  # 0.333 * 3 \u2248 1.0 \u2192 round(1.0) = 1\n        &gt;&gt;&gt; param4.at_percentage(1.0)\n        40  # 1.0 * 3 = 3\n    \"\"\"\n    if not 0.0 &lt;= percentage &lt;= 1.0:\n        raise ValueError(f\"Percentage must be in [0.0, 1.0], got {percentage}\")\n\n    # Map percentage to continuous index space\n    max_idx = len(self.values) - 1\n    float_idx = percentage * max_idx\n\n    # Apply rounding strategy\n    if rounding == \"down\":\n        idx = int(math.floor(float_idx))\n    elif rounding == \"up\":\n        idx = int(math.ceil(float_idx))\n    elif rounding == \"natural\":\n        idx = round(float_idx)\n    else:\n        raise ValueError(\n            f\"Invalid rounding mode '{rounding}'. \" f\"Must be 'natural', 'down', or 'up'.\"\n        )\n\n    # Clamp to valid range (defensive, should be unnecessary)\n    idx = max(0, min(idx, max_idx))\n\n    return self.values[idx]\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.OrderedParameter.get_default","title":"get_default","text":"<pre><code>get_default() -&gt; int\n</code></pre> <p>Get default value (explicit or minimum).</p> Source code in <code>brainsmith/dataflow/ordered_parameter.py</code> <pre><code>def get_default(self) -&gt; int:\n    \"\"\"Get default value (explicit or minimum).\"\"\"\n    return self.default if self.default is not None else self.values[0]\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.OrderedParameter.index_of","title":"index_of","text":"<pre><code>index_of(value: int) -&gt; int\n</code></pre> <p>Get index of value in ordered sequence.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>int</code> <p>Value to find</p> required <p>Returns:</p> Type Description <code>int</code> <p>Zero-based index of value</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If value not in parameter</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; param = OrderedParameter(\"SIMD\", (1, 2, 4, 8, 16))\n&gt;&gt;&gt; param.index_of(4)\n2\n&gt;&gt;&gt; param.index_of(16)\n4\n</code></pre> Source code in <code>brainsmith/dataflow/ordered_parameter.py</code> <pre><code>def index_of(self, value: int) -&gt; int:\n    \"\"\"Get index of value in ordered sequence.\n\n    Args:\n        value: Value to find\n\n    Returns:\n        Zero-based index of value\n\n    Raises:\n        ValueError: If value not in parameter\n\n    Examples:\n        &gt;&gt;&gt; param = OrderedParameter(\"SIMD\", (1, 2, 4, 8, 16))\n        &gt;&gt;&gt; param.index_of(4)\n        2\n        &gt;&gt;&gt; param.index_of(16)\n        4\n    \"\"\"\n    try:\n        return self.values.index(value)\n    except ValueError:\n        raise ValueError(\n            f\"Value {value} not in parameter '{self.name}'. \"\n            f\"Valid range: [{self.min()}, {self.max()}], \"\n            f\"values: {self.values}\"\n        )\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.OrderedParameter.max","title":"max","text":"<pre><code>max() -&gt; int\n</code></pre> <p>Get maximum value (last in ordered sequence).</p> Source code in <code>brainsmith/dataflow/ordered_parameter.py</code> <pre><code>def max(self) -&gt; int:\n    \"\"\"Get maximum value (last in ordered sequence).\"\"\"\n    return self.values[-1]\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.OrderedParameter.min","title":"min","text":"<pre><code>min() -&gt; int\n</code></pre> <p>Get minimum value (first in ordered sequence).</p> Source code in <code>brainsmith/dataflow/ordered_parameter.py</code> <pre><code>def min(self) -&gt; int:\n    \"\"\"Get minimum value (first in ordered sequence).\"\"\"\n    return self.values[0]\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.OrderedParameter.step_down","title":"step_down","text":"<pre><code>step_down(current: int, n: int = 1) -&gt; int\n</code></pre> <p>Step down n positions from current value.</p> <p>Clamps at minimum if n steps would go below bounds.</p> <p>Parameters:</p> Name Type Description Default <code>current</code> <code>int</code> <p>Current value (must be in parameter)</p> required <code>n</code> <code>int</code> <p>Number of steps to move down (positive integer)</p> <code>1</code> <p>Returns:</p> Type Description <code>int</code> <p>New value n steps down (clamped at min)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If current value not in parameter or n &lt; 0</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; param = OrderedParameter(\"SIMD\", (1, 2, 4, 8, 16, 32, 64))\n&gt;&gt;&gt; param.step_down(16, 1)\n8\n&gt;&gt;&gt; param.step_down(16, 2)\n4\n&gt;&gt;&gt; param.step_down(4, 10)\n1  # Clamped at min\n</code></pre> Source code in <code>brainsmith/dataflow/ordered_parameter.py</code> <pre><code>def step_down(self, current: int, n: int = 1) -&gt; int:\n    \"\"\"Step down n positions from current value.\n\n    Clamps at minimum if n steps would go below bounds.\n\n    Args:\n        current: Current value (must be in parameter)\n        n: Number of steps to move down (positive integer)\n\n    Returns:\n        New value n steps down (clamped at min)\n\n    Raises:\n        ValueError: If current value not in parameter or n &lt; 0\n\n    Examples:\n        &gt;&gt;&gt; param = OrderedParameter(\"SIMD\", (1, 2, 4, 8, 16, 32, 64))\n        &gt;&gt;&gt; param.step_down(16, 1)\n        8\n        &gt;&gt;&gt; param.step_down(16, 2)\n        4\n        &gt;&gt;&gt; param.step_down(4, 10)\n        1  # Clamped at min\n    \"\"\"\n    if n &lt; 0:\n        raise ValueError(f\"step_down requires n &gt;= 0, got {n}\")\n\n    idx = self.index_of(current)\n    new_idx = max(idx - n, 0)\n    return self.values[new_idx]\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.OrderedParameter.step_up","title":"step_up","text":"<pre><code>step_up(current: int, n: int = 1) -&gt; int\n</code></pre> <p>Step up n positions from current value.</p> <p>Clamps at maximum if n steps would exceed bounds.</p> <p>Parameters:</p> Name Type Description Default <code>current</code> <code>int</code> <p>Current value (must be in parameter)</p> required <code>n</code> <code>int</code> <p>Number of steps to move up (positive integer)</p> <code>1</code> <p>Returns:</p> Type Description <code>int</code> <p>New value n steps up (clamped at max)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If current value not in parameter or n &lt; 0</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; param = OrderedParameter(\"PE\", (1, 2, 4, 8, 16, 32, 64))\n&gt;&gt;&gt; param.step_up(4, 1)\n8\n&gt;&gt;&gt; param.step_up(4, 2)\n16\n&gt;&gt;&gt; param.step_up(32, 10)\n64  # Clamped at max\n</code></pre> Source code in <code>brainsmith/dataflow/ordered_parameter.py</code> <pre><code>def step_up(self, current: int, n: int = 1) -&gt; int:\n    \"\"\"Step up n positions from current value.\n\n    Clamps at maximum if n steps would exceed bounds.\n\n    Args:\n        current: Current value (must be in parameter)\n        n: Number of steps to move up (positive integer)\n\n    Returns:\n        New value n steps up (clamped at max)\n\n    Raises:\n        ValueError: If current value not in parameter or n &lt; 0\n\n    Examples:\n        &gt;&gt;&gt; param = OrderedParameter(\"PE\", (1, 2, 4, 8, 16, 32, 64))\n        &gt;&gt;&gt; param.step_up(4, 1)\n        8\n        &gt;&gt;&gt; param.step_up(4, 2)\n        16\n        &gt;&gt;&gt; param.step_up(32, 10)\n        64  # Clamped at max\n    \"\"\"\n    if n &lt; 0:\n        raise ValueError(f\"step_up requires n &gt;= 0, got {n}\")\n\n    idx = self.index_of(current)\n    new_idx = min(idx + n, len(self.values) - 1)\n    return self.values[new_idx]\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.OrderedParameter.validate","title":"validate","text":"<pre><code>validate(value: int) -&gt; bool\n</code></pre> <p>Check if value is valid for this parameter.</p> Source code in <code>brainsmith/dataflow/ordered_parameter.py</code> <pre><code>def validate(self, value: int) -&gt; bool:\n    \"\"\"Check if value is valid for this parameter.\"\"\"\n    return value in self.values\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.DesignSpaceBuilder","title":"DesignSpaceBuilder","text":"<p>Builds kernel design space from schema and ONNX context.</p> <p>Two-phase construction: 1. build() creates KernelDesignSpace once (tensor/block shapes, datatypes, valid ranges) 2. design_space.configure() creates KernelDesignPoint many times (stream shapes for specific params)</p> Example <p>builder = DesignSpaceBuilder() context = BuildContext( ...     schema=kernel_schema, ...     model_w=model_wrapper, ...     node_inputs=list(node.input), ...     node_outputs=list(node.output), ...     param_getter=self.get_nodeattr, ...     param_setter=self.set_nodeattr, ...     node_name=node.name ... ) design_space = builder.build(context) point = design_space.configure({\"SIMD\": 64, \"PE\": 1})</p>"},{"location":"api/dataflow/#brainsmith.dataflow.DesignSpaceBuilder.build","title":"build","text":"<pre><code>build(ctx: BuildContext) -&gt; KernelDesignSpace\n</code></pre> <p>Build kernel design space from ONNX context.</p> <p>Resolves all properties constant across parallelization configs: - Tensor shapes (from ONNX graph) - Block shapes (from block_tiling templates) - Datatypes (from ONNX graph + union type derivation) - Internal datatypes (from union type derivation) - Structural constraints (validated once) - Valid parallelization parameter ranges (divisor sets)</p> <p>Stream shapes are left as templates for later resolution via configure().</p> <p>Parameters:</p> Name Type Description Default <code>ctx</code> <code>BuildContext</code> <p>Build context with ONNX node and ModelWrapper</p> required <p>Returns:</p> Type Description <code>KernelDesignSpace</code> <p>KernelDesignSpace ready for configuration exploration</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If structural constraints fail</p> Source code in <code>brainsmith/dataflow/builder.py</code> <pre><code>def build(self, ctx: BuildContext) -&gt; KernelDesignSpace:\n    \"\"\"Build kernel design space from ONNX context.\n\n    Resolves all properties constant across parallelization configs:\n    - Tensor shapes (from ONNX graph)\n    - Block shapes (from block_tiling templates)\n    - Datatypes (from ONNX graph + union type derivation)\n    - Internal datatypes (from union type derivation)\n    - Structural constraints (validated once)\n    - Valid parallelization parameter ranges (divisor sets)\n\n    Stream shapes are left as templates for later resolution via configure().\n\n    Args:\n        ctx: Build context with ONNX node and ModelWrapper\n\n    Returns:\n        KernelDesignSpace ready for configuration exploration\n\n    Raises:\n        ValueError: If structural constraints fail\n    \"\"\"\n    from .dse_models import KernelDesignSpace\n    from .validation import DesignSpaceValidationContext\n\n    self._ctx = ctx\n    self._interfaces: dict[str, Any] = {}\n\n    logger.debug(f\"Building KernelDesignSpace for {ctx.node_name}\")\n\n    # Build input interfaces from ONNX graph\n    inputs: dict[str, InterfaceDesignSpace] = {}\n\n    for i, inp_name in enumerate(ctx.node_inputs):\n        if not inp_name:\n            continue\n\n        if i &gt;= len(ctx.schema.inputs):\n            logger.warning(\n                f\"Node has input {i} but schema only defines {len(ctx.schema.inputs)} inputs\"\n            )\n            continue\n\n        schema = ctx.schema.inputs[i]\n\n        try:\n            interface = self._build_interface(\n                direction=\"input\", index=i, tensor_name=inp_name, schema=schema\n            )\n            inputs[schema.name] = interface\n            self._interfaces[schema.name] = interface\n        except ValueError as e:\n            raise ValueError(f\"Failed to build input '{schema.name}': {e}\") from e\n\n    # Derive internal datatypes from inputs and parameters\n    internal_datatypes = {}\n\n    if ctx.schema.internal_datatypes:\n        for internal_name, datatype_spec in ctx.schema.internal_datatypes.items():\n            try:\n                # Use unified DatatypeSpec resolver (supports union types)\n                datatype = self._resolve_datatype_spec(\n                    spec=datatype_spec,\n                    tensor_name=None,  # Internals have no ONNX tensor\n                    fallback_datatype=None,  # Internal datatypes must be explicit\n                )\n                ctx.param_setter(f\"{internal_name}Datatype\", datatype.name)\n\n                # Store datatype directly (no shapes for internal datatypes)\n                self._interfaces[internal_name] = datatype\n                internal_datatypes[internal_name] = datatype\n\n                logger.debug(f\"  Internal '{internal_name}': dtype={datatype.name}\")\n            except ValueError as e:\n                raise ValueError(\n                    f\"Failed to resolve internal datatype '{internal_name}': {e}\"\n                ) from e\n\n    # Build output interfaces (may derive datatypes from inputs)\n    outputs: dict[str, InterfaceDesignSpace] = {}\n\n    for i, out_name in enumerate(ctx.node_outputs):\n        if i &gt;= len(ctx.schema.outputs):\n            logger.warning(\n                f\"Node has output {i} but schema only defines {len(ctx.schema.outputs)} outputs\"\n            )\n            continue\n\n        schema = ctx.schema.outputs[i]\n\n        try:\n            interface = self._build_interface(\n                direction=\"output\", index=i, tensor_name=out_name, schema=schema\n            )\n            outputs[schema.name] = interface\n            self._interfaces[schema.name] = interface\n        except ValueError as e:\n            raise ValueError(f\"Failed to build output '{schema.name}': {e}\") from e\n\n    # Separate constraints by evaluation phase (structural vs optimization)\n    structural_constraints = [\n        c for c in ctx.schema.constraints if c.evaluation_phase == \"structural\"\n    ]\n    optimization_constraints = [\n        c for c in ctx.schema.constraints if c.evaluation_phase != \"structural\"\n    ]\n\n    logger.debug(\n        f\"  Split {len(ctx.schema.constraints)} constraints: \"\n        f\"{len(structural_constraints)} structural, {len(optimization_constraints)} optimization\"\n    )\n\n    # Validate structural constraints against design space\n    if structural_constraints:\n        validation_ctx = DesignSpaceValidationContext(\n            inputs=inputs,\n            outputs=outputs,\n            internal_datatypes=internal_datatypes,\n            param_getter=ctx.param_getter,\n        )\n\n        failed = [\n            f\"{c.describe()}: {e}\"\n            for c in structural_constraints\n            if (e := c.check(validation_ctx))\n        ]\n        if failed:\n            raise ValueError(f\"{ctx.node_name} validation failed:\\n\" + \"\\n\".join(failed))\n\n        logger.debug(f\"  All {len(structural_constraints)} structural constraints passed\")\n\n    # Compute valid dimension values (tiling from divisors + DSE from schema)\n    all_dimensions = self._compute_dimension_ranges(inputs, outputs, ctx.schema)\n\n    # Link parallelism metadata to interfaces (shared dimension references)\n    # Must happen AFTER dimension computation so dimensions dict is available\n    inputs = self._link_parallelism_metadata(inputs, all_dimensions)\n    outputs = self._link_parallelism_metadata(outputs, all_dimensions)\n\n    # Assemble immutable design space model\n    # Note: structural_constraints validated above but not stored (never re-validated)\n    design_space = KernelDesignSpace(\n        name=ctx.schema.name,\n        inputs=inputs,\n        outputs=outputs,\n        internal_datatypes=internal_datatypes,\n        optimization_constraints=optimization_constraints,\n        parameters=all_dimensions,\n    )\n\n    logger.debug(f\"KernelDesignSpace built successfully for {ctx.node_name}\")\n    return design_space\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.BuildContext","title":"BuildContext  <code>dataclass</code>","text":"<pre><code>BuildContext(schema: KernelSchema, model_w: ModelWrapper, node_inputs: list[str], node_outputs: list[str], param_getter: Callable[[str], Any], param_setter: Callable[[str, Any], None], node_name: str = '&lt;unknown&gt;')\n</code></pre> <p>Build context for kernel design space construction.</p> <p>Encapsulates all data needed to build a KernelDesignSpace from a schema.</p> <p>Attributes:</p> Name Type Description <code>schema</code> <code>KernelSchema</code> <p>KernelSchema defining structure</p> <code>model_w</code> <code>ModelWrapper</code> <p>ModelWrapper for ONNX graph access</p> <code>node_inputs</code> <code>list[str]</code> <p>ONNX node input tensor names</p> <code>node_outputs</code> <code>list[str]</code> <p>ONNX node output tensor names</p> <code>param_getter</code> <code>Callable[[str], Any]</code> <p>Function to retrieve nodeattr values</p> <code>param_setter</code> <code>Callable[[str, Any], None]</code> <p>Function to store nodeattr values</p> <code>node_name</code> <code>str</code> <p>Node name for error messages</p>"},{"location":"api/dataflow/#brainsmith.dataflow.Constraint","title":"Constraint","text":"<p>               Bases: <code>Protocol</code></p> <p>Validation rule for kernel constraints.</p> <p>Pure predicate that validates kernel properties during construction. Uses duck typing to work with any validation context providing required methods.</p> <p>Required methods: - check(ctx) \u2192 Optional[str] - describe() \u2192 str</p>"},{"location":"api/dataflow/#brainsmith.dataflow.Constraint.evaluation_phase","title":"evaluation_phase  <code>property</code>","text":"<pre><code>evaluation_phase: str\n</code></pre> <p>When to evaluate this constraint during kernel construction.</p> <p>Returns:</p> Type Description <code>str</code> <p>'structural' - Evaluated once during design space construction (Phase 1)           Constraints that determine backend compatibility           (tensor shapes, block shapes, datatypes, etc.)</p> <code>str</code> <p>'optimization' - Evaluated per-configuration during configure() (Phase 2)             Constraints that bound optimization space             (stream shapes, parallelization parameters, etc.)</p> <p>Default implementation uses heuristic: - Constraints with hierarchy == STREAM are optimization constraints - All other constraints are structural</p> <p>Subclasses can override this property for explicit classification.</p> <p>Examples:</p> <p>DatatypeInteger: 'structural' (no hierarchy, datatype determines compatibility) ShapesEqual(hierarchy=TENSOR): 'structural' (tensor shape determines compatibility) ShapesEqual(hierarchy=BLOCK): 'structural' (block shape determines compatibility) ShapesEqual(hierarchy=STREAM): 'optimization' (stream shape bounds optimization) DimensionDivisible(hierarchy=STREAM): 'optimization' (stream dim bounds optimization)</p>"},{"location":"api/dataflow/#brainsmith.dataflow.Constraint.check","title":"check","text":"<pre><code>check(ctx) -&gt; str | None\n</code></pre> <p>Check constraint in given context.</p> <p>Parameters:</p> Name Type Description Default <code>ctx</code> <p>Validation context (DesignSpaceValidationContext or ConfigurationValidationContext)</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>None if satisfied, error message string if violated</p> Source code in <code>brainsmith/dataflow/constraints.py</code> <pre><code>def check(self, ctx) -&gt; str | None:\n    \"\"\"Check constraint in given context.\n\n    Args:\n        ctx: Validation context (DesignSpaceValidationContext or ConfigurationValidationContext)\n\n    Returns:\n        None if satisfied, error message string if violated\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.Constraint.describe","title":"describe","text":"<pre><code>describe() -&gt; str\n</code></pre> <p>Human-readable description of constraint.</p> Source code in <code>brainsmith/dataflow/constraints.py</code> <pre><code>def describe(self) -&gt; str:\n    \"\"\"Human-readable description of constraint.\"\"\"\n    ...\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.ValidationError","title":"ValidationError","text":"<pre><code>ValidationError(message: str, location: str = '', suggestions: list = None)\n</code></pre> <p>               Bases: <code>ValueError</code></p> <p>Validation error with context and suggestions.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Error message</p> required <code>location</code> <code>str</code> <p>Optional context (e.g., \"input.stream[1]\")</p> <code>''</code> <code>suggestions</code> <code>list</code> <p>Optional list of suggestions</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; raise ValidationError(\"Invalid parameter\")\n&gt;&gt;&gt; raise ValidationError(\"PE must divide 768\", location=\"output.stream[1]\")\n</code></pre> Source code in <code>brainsmith/dataflow/validation.py</code> <pre><code>def __init__(self, message: str, location: str = \"\", suggestions: list = None):\n    self.message = message\n    self.location = location\n    self.suggestions = suggestions or []\n    super().__init__(self._format_message())\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.ValidationError.__str__","title":"__str__","text":"<pre><code>__str__() -&gt; str\n</code></pre> <p>Return formatted message (delegates to parent Exception).</p> Source code in <code>brainsmith/dataflow/validation.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return formatted message (delegates to parent Exception).\"\"\"\n    return super().__str__()\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.DesignSpaceValidationContext","title":"DesignSpaceValidationContext  <code>dataclass</code>","text":"<pre><code>DesignSpaceValidationContext(inputs: dict[str, Any], outputs: dict[str, Any], internal_datatypes: dict[str, DataType], param_getter: Callable[[str], Any] | None = None)\n</code></pre> <p>Validation context for structural constraints during design space build.</p> <p>Used during KernelDesignSpace construction to validate tensor shapes, block shapes, and datatypes. Stream shapes not available until configure().</p> <p>Attributes:</p> Name Type Description <code>inputs</code> <code>dict[str, Any]</code> <p>Input interfaces (InterfaceDesignSpace)</p> <code>outputs</code> <code>dict[str, Any]</code> <p>Output interfaces (InterfaceDesignSpace)</p> <code>internal_datatypes</code> <code>dict[str, DataType]</code> <p>Internal datatypes</p> <code>param_getter</code> <code>Callable[[str], Any] | None</code> <p>Optional nodeattr getter</p> Example <p>ctx = DesignSpaceValidationContext(     inputs=interfaces_input,     outputs=interfaces_output,     internal_datatypes=internal_datatypes,     param_getter=get_nodeattr ) for constraint in structural_constraints:     if error := constraint.check(ctx):         raise ValueError(error)</p>"},{"location":"api/dataflow/#brainsmith.dataflow.DesignSpaceValidationContext.get_datatype","title":"get_datatype","text":"<pre><code>get_datatype(name: str) -&gt; DataType\n</code></pre> <p>Get datatype from interface or internal datatypes.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Interface or internal datatype name</p> required <p>Returns:</p> Type Description <code>DataType</code> <p>DataType</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If interface/datatype not found</p> Source code in <code>brainsmith/dataflow/validation.py</code> <pre><code>def get_datatype(self, name: str) -&gt; DataType:\n    \"\"\"Get datatype from interface or internal datatypes.\n\n    Args:\n        name: Interface or internal datatype name\n\n    Returns:\n        DataType\n\n    Raises:\n        KeyError: If interface/datatype not found\n    \"\"\"\n    # Check internal datatypes first\n    if name in self.internal_datatypes:\n        return self.internal_datatypes[name]\n\n    # Check interfaces\n    return self._find_interface(name).datatype\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.DesignSpaceValidationContext.get_param","title":"get_param","text":"<pre><code>get_param(name: str) -&gt; Any\n</code></pre> <p>Get kernel parameter value.</p> <p>Note: Primarily for rare block_tiling parameters. Most parameters are stream_tiling and only available during configure().</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Parameter name</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Parameter value</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If no param_getter provided</p> <code>KeyError</code> <p>If parameter not found</p> Source code in <code>brainsmith/dataflow/validation.py</code> <pre><code>def get_param(self, name: str) -&gt; Any:\n    \"\"\"Get kernel parameter value.\n\n    Note: Primarily for rare block_tiling parameters. Most parameters\n    are stream_tiling and only available during configure().\n\n    Args:\n        name: Parameter name\n\n    Returns:\n        Parameter value\n\n    Raises:\n        RuntimeError: If no param_getter provided\n        KeyError: If parameter not found\n    \"\"\"\n    if self.param_getter is None:\n        raise RuntimeError(f\"No param_getter available. Cannot get parameter '{name}'.\")\n\n    try:\n        return self.param_getter(name)\n    except (AttributeError, KeyError) as e:\n        raise KeyError(f\"Parameter '{name}' not found in nodeattrs\") from e\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.DesignSpaceValidationContext.get_shape","title":"get_shape","text":"<pre><code>get_shape(name: str, hierarchy: ShapeHierarchy = ShapeHierarchy.TENSOR) -&gt; tuple[int, ...]\n</code></pre> <p>Get shape at hierarchy level.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Interface name</p> required <code>hierarchy</code> <code>ShapeHierarchy</code> <p>Which level of hierarchy (TENSOR or BLOCK only)</p> <code>TENSOR</code> <p>Returns:</p> Type Description <code>tuple[int, ...]</code> <p>Shape tuple</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If interface not found</p> <code>RuntimeError</code> <p>If STREAM hierarchy requested (not available in design space)</p> Source code in <code>brainsmith/dataflow/validation.py</code> <pre><code>def get_shape(\n    self, name: str, hierarchy: ShapeHierarchy = ShapeHierarchy.TENSOR\n) -&gt; tuple[int, ...]:\n    \"\"\"Get shape at hierarchy level.\n\n    Args:\n        name: Interface name\n        hierarchy: Which level of hierarchy (TENSOR or BLOCK only)\n\n    Returns:\n        Shape tuple\n\n    Raises:\n        KeyError: If interface not found\n        RuntimeError: If STREAM hierarchy requested (not available in design space)\n    \"\"\"\n    if hierarchy == ShapeHierarchy.STREAM:\n        raise RuntimeError(\n            \"Stream shapes not available in design space validation context. \"\n            \"Stream-level constraints are optimization constraints and should be \"\n            \"validated during configure(), not during build().\"\n        )\n\n    interface = self._find_interface(name)\n\n    if hierarchy == ShapeHierarchy.BLOCK:\n        return interface.block_shape\n    elif hierarchy == ShapeHierarchy.TENSOR:\n        return interface.tensor_shape\n    else:\n        raise ValueError(f\"Unknown hierarchy: {hierarchy}\")\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.DesignSpaceValidationContext.is_dynamic","title":"is_dynamic","text":"<pre><code>is_dynamic(name: str) -&gt; bool\n</code></pre> <p>Check if interface is dynamic (no initializer).</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Interface name</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if dynamic (activations), False if static (weights)</p> Source code in <code>brainsmith/dataflow/validation.py</code> <pre><code>def is_dynamic(self, name: str) -&gt; bool:\n    \"\"\"Check if interface is dynamic (no initializer).\n\n    Args:\n        name: Interface name\n\n    Returns:\n        True if dynamic (activations), False if static (weights)\n    \"\"\"\n    if name in self.inputs:\n        return not self.inputs[name].is_weight\n\n    if name in self.outputs:\n        return True  # Outputs always dynamic\n\n    # Not found\n    raise KeyError(f\"Interface '{name}' not found\")\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.ConfigurationValidationContext","title":"ConfigurationValidationContext  <code>dataclass</code>","text":"<pre><code>ConfigurationValidationContext(configured_model: Any, params: dict[str, int])\n</code></pre> <p>Validation context for optimization constraints during configure().</p> <p>Used during KernelDesignSpace.configure() to validate constraints on stream shapes and parallelization parameters.</p> <p>Attributes:</p> Name Type Description <code>configured_model</code> <code>Any</code> <p>KernelDesignPoint with configured interfaces</p> <code>params</code> <code>dict[str, int]</code> <p>Parallelization parameters</p> Example <p>ctx = ConfigurationValidationContext(     configured_model=instance,     params=params ) for constraint in parametric_constraints:     if error := constraint.check(ctx):         raise ValueError(error)</p>"},{"location":"api/dataflow/#brainsmith.dataflow.ConfigurationValidationContext.get_datatype","title":"get_datatype","text":"<pre><code>get_datatype(name: str) -&gt; DataType\n</code></pre> <p>Get datatype from interface or internal datatypes.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Interface or internal datatype name</p> required <p>Returns:</p> Type Description <code>DataType</code> <p>DataType</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If interface/datatype not found</p> Source code in <code>brainsmith/dataflow/validation.py</code> <pre><code>def get_datatype(self, name: str) -&gt; DataType:\n    \"\"\"Get datatype from interface or internal datatypes.\n\n    Args:\n        name: Interface or internal datatype name\n\n    Returns:\n        DataType\n\n    Raises:\n        KeyError: If interface/datatype not found\n    \"\"\"\n    # Check internal datatypes\n    if name in self.configured_model.internal_datatypes:\n        return self.configured_model.internal_datatypes[name]\n\n    # Check interfaces\n    return self._find_interface(name).datatype\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.ConfigurationValidationContext.get_param","title":"get_param","text":"<pre><code>get_param(name: str) -&gt; Any\n</code></pre> <p>Get kernel parameter value.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Parameter name (e.g., \"SIMD\", \"PE\", \"epsilon\")</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Parameter value</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If parameter not found</p> Source code in <code>brainsmith/dataflow/validation.py</code> <pre><code>def get_param(self, name: str) -&gt; Any:\n    \"\"\"Get kernel parameter value.\n\n    Args:\n        name: Parameter name (e.g., \"SIMD\", \"PE\", \"epsilon\")\n\n    Returns:\n        Parameter value\n\n    Raises:\n        KeyError: If parameter not found\n    \"\"\"\n    if name not in self.params:\n        raise KeyError(\n            f\"Parameter '{name}' not found in configuration. \"\n            f\"Available: {list(self.params.keys())}\"\n        )\n    return self.params[name]\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.ConfigurationValidationContext.get_shape","title":"get_shape","text":"<pre><code>get_shape(name: str, hierarchy: ShapeHierarchy = ShapeHierarchy.TENSOR) -&gt; tuple[int, ...]\n</code></pre> <p>Get shape at hierarchy level.</p> <p>All hierarchies available (TENSOR, BLOCK, STREAM).</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Interface name</p> required <code>hierarchy</code> <code>ShapeHierarchy</code> <p>Which level of hierarchy</p> <code>TENSOR</code> <p>Returns:</p> Type Description <code>tuple[int, ...]</code> <p>Shape tuple</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If interface not found</p> Source code in <code>brainsmith/dataflow/validation.py</code> <pre><code>def get_shape(\n    self, name: str, hierarchy: ShapeHierarchy = ShapeHierarchy.TENSOR\n) -&gt; tuple[int, ...]:\n    \"\"\"Get shape at hierarchy level.\n\n    All hierarchies available (TENSOR, BLOCK, STREAM).\n\n    Args:\n        name: Interface name\n        hierarchy: Which level of hierarchy\n\n    Returns:\n        Shape tuple\n\n    Raises:\n        KeyError: If interface not found\n    \"\"\"\n    interface = self._find_interface(name)\n    return interface.get_shape(hierarchy)\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.ConfigurationValidationContext.is_dynamic","title":"is_dynamic","text":"<pre><code>is_dynamic(name: str) -&gt; bool\n</code></pre> <p>Check if interface is dynamic (no initializer).</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Interface name</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if dynamic (activations), False if static (weights)</p> Source code in <code>brainsmith/dataflow/validation.py</code> <pre><code>def is_dynamic(self, name: str) -&gt; bool:\n    \"\"\"Check if interface is dynamic (no initializer).\n\n    Args:\n        name: Interface name\n\n    Returns:\n        True if dynamic (activations), False if static (weights)\n    \"\"\"\n    if name in self.configured_model.inputs:\n        return not self.configured_model.inputs[name].is_weight\n\n    if name in self.configured_model.outputs:\n        return True  # Outputs always dynamic\n\n    # Not found\n    raise KeyError(f\"Interface '{name}' not found\")\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.TransformationResult","title":"TransformationResult  <code>dataclass</code>","text":"<pre><code>TransformationResult(nodes_to_insert: list[NodeProto], nodes_to_remove: list[NodeProto], metadata: dict[str, Any] = dict())\n</code></pre> <p>Result of ONNX node to hardware kernel transformation.</p> <p>Attributes:</p> Name Type Description <code>nodes_to_insert</code> <code>list[NodeProto]</code> <p>HW nodes to insert into graph</p> <code>nodes_to_remove</code> <code>list[NodeProto]</code> <p>ONNX nodes to remove from graph</p> <code>metadata</code> <code>dict[str, Any]</code> <p>Optional transformation metadata</p>"},{"location":"api/dataflow/#brainsmith.dataflow.Shape","title":"Shape  <code>module-attribute</code>","text":"<pre><code>Shape = tuple[int, ...]\n</code></pre> <p>Immutable tensor shape (e.g., (1, 784))</p>"},{"location":"api/dataflow/#brainsmith.dataflow.ShapeHierarchy","title":"ShapeHierarchy","text":"<p>               Bases: <code>Enum</code></p> <p>Shape hierarchy level for constraints and relationships.</p> <p>Attributes:</p> Name Type Description <code>STREAM</code> <p>Stream shape (parallelism, elements per cycle)</p> <code>BLOCK</code> <p>Block shape (tiling dimensions)</p> <code>TENSOR</code> <p>Tensor shape (full logical dimensions)</p>"},{"location":"api/dataflow/#brainsmith.dataflow.FULL_DIM","title":"FULL_DIM  <code>module-attribute</code>","text":"<pre><code>FULL_DIM = _FullDimType()\n</code></pre>"},{"location":"api/dataflow/#brainsmith.dataflow.FULL_SHAPE","title":"FULL_SHAPE  <code>module-attribute</code>","text":"<pre><code>FULL_SHAPE = _FullShapeType()\n</code></pre>"},{"location":"api/dataflow/#see-also","title":"See Also","text":"<ul> <li>Component Registry - Register custom kernels, backends, and steps</li> <li>Getting Started - Installation and quickstart</li> <li>GitHub - Issues and questions</li> </ul>"},{"location":"api/dse/","title":"Design Space Exploration","text":""},{"location":"api/dse/#design-space-exploration","title":"Design Space Exploration","text":"<p>Evaluate multiple hardware configurations to find optimal designs.</p> <p>Brainsmith uses segment-based DSE to efficiently explore large design spaces by reusing computation across similar configurations.</p> <p>Example:</p> <pre><code>from brainsmith import explore_design_space\nfrom brainsmith.dse import SegmentStatus\n\n# Run complete DSE pipeline\nresults = explore_design_space(\n    model_path=\"model.onnx\",\n    blueprint_path=\"blueprint.yaml\",\n    output_dir=\"./build\"\n)\n\n# Analyze results\nstats = results.compute_stats()\nprint(f\"Successful: {stats['successful']}/{stats['total']}\")\nprint(f\"Total time: {results.total_time:.2f}s\")\n\n# Access successful outputs\nfor seg_id, result in results.segment_results.items():\n    if result.status == SegmentStatus.COMPLETED:\n        print(f\"Output: {result.output_model}\")\n</code></pre> <p>Example:</p> <pre><code>from brainsmith.dse import parse_blueprint\n\ndesign_space, config = parse_blueprint(\n    blueprint_path=\"blueprint.yaml\",\n    model_path=\"model.onnx\"\n)\n\nprint(f\"Steps: {len(design_space.steps)}\")\nprint(f\"Kernels: {design_space.kernel_backends}\")\n</code></pre> <p>Example:</p> <pre><code>from brainsmith.dse import parse_blueprint, build_tree\n\ndesign_space, config = parse_blueprint(\n    blueprint_path=\"blueprint.yaml\",\n    model_path=\"model.onnx\"\n)\n\ntree = build_tree(design_space, config)\n\n# Inspect before execution\nstats = tree.get_statistics()\nprint(f\"Paths: {stats['total_paths']:,}\")\nprint(f\"Segments: {stats['total_segments']:,}\")\n</code></pre> <p>Example:</p> <pre><code>from brainsmith.dse import build_tree, execute_tree\n\ntree = build_tree(design_space, config)\n\nresult = execute_tree(\n    tree=tree,\n    model_path=\"model.onnx\",\n    config=config,\n    output_dir=\"./build\"\n)\n</code></pre> <p>Example:</p> <pre><code>results = explore_design_space(\n    model_path=\"model.onnx\",\n    blueprint_path=\"blueprint.yaml\",\n    output_dir=\"./build\"\n)\n\n# Compute statistics\nstats = results.compute_stats()\nprint(stats['successful'], stats['failed'], stats['total'])\n# Also available: stats['cached'], stats['skipped']\n\n# Access individual segments\nfor seg_id, seg_result in results.segment_results.items():\n    print(f\"{seg_id}: {seg_result.status}\")\n</code></pre>"},{"location":"api/dse/#brainsmith.dse.explore_design_space","title":"explore_design_space","text":"<pre><code>explore_design_space(model_path: str, blueprint_path: str, output_dir: str | None = None, start_step_override: str | None = None, stop_step_override: str | None = None, verbosity: str = 'normal') -&gt; TreeExecutionResult\n</code></pre> <p>Explore design space and synthesize FPGA accelerator from neural network model.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to ONNX model file</p> required <code>blueprint_path</code> <code>str</code> <p>Path to Blueprint YAML file</p> required <code>output_dir</code> <code>str | None</code> <p>Output directory (defaults to $BSMITH_BUILD_DIR/dfc_YYYYMMDD_HHMMSS)</p> <code>None</code> <code>start_step_override</code> <code>str | None</code> <p>Override blueprint start_step</p> <code>None</code> <code>stop_step_override</code> <code>str | None</code> <p>Override blueprint stop_step</p> <code>None</code> <code>verbosity</code> <code>str</code> <p>Logging verbosity (quiet | normal | verbose | debug)</p> <code>'normal'</code> <p>Returns:</p> Type Description <code>TreeExecutionResult</code> <p>TreeExecutionResult containing build artifacts and statistics</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If model or blueprint file doesn't exist</p> <code>ValueError</code> <p>If blueprint is invalid or tree exceeds size limits</p> <code>ExecutionError</code> <p>If no successful builds were produced</p> Source code in <code>brainsmith/dse/api.py</code> <pre><code>def explore_design_space(\n    model_path: str,\n    blueprint_path: str,\n    output_dir: str | None = None,\n    start_step_override: str | None = None,\n    stop_step_override: str | None = None,\n    verbosity: str = \"normal\",\n) -&gt; TreeExecutionResult:\n    \"\"\"\n    Explore design space and synthesize FPGA accelerator from neural network model.\n\n    Args:\n        model_path: Path to ONNX model file\n        blueprint_path: Path to Blueprint YAML file\n        output_dir: Output directory (defaults to $BSMITH_BUILD_DIR/dfc_YYYYMMDD_HHMMSS)\n        start_step_override: Override blueprint start_step\n        stop_step_override: Override blueprint stop_step\n        verbosity: Logging verbosity (quiet | normal | verbose | debug)\n\n    Returns:\n        TreeExecutionResult containing build artifacts and statistics\n\n    Raises:\n        FileNotFoundError: If model or blueprint file doesn't exist\n        ValueError: If blueprint is invalid or tree exceeds size limits\n        ExecutionError: If no successful builds were produced\n    \"\"\"\n    # Verify files exist\n    model_path_obj = Path(model_path)\n    if not model_path_obj.exists():\n        raise FileNotFoundError(f\"Model file not found: {model_path_obj}\")\n    blueprint_path_obj = Path(blueprint_path)\n    if not blueprint_path_obj.exists():\n        raise FileNotFoundError(f\"Blueprint file not found: {blueprint_path_obj}\")\n\n    # Load config early to ensure BSMITH_* vars are exported for YAML expansion\n    from brainsmith.settings import get_config\n\n    get_config()  # Exports BSMITH_DIR and other env vars\n\n    # Determine output directory\n    if not output_dir:\n        build_dir = get_config().build_dir\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        output_dir = str(build_dir / f\"dfc_{timestamp}\")\n\n    output_path = Path(output_dir)\n    output_path.mkdir(parents=True, exist_ok=True)\n\n    # Configure logging with output directory for file logging\n    from brainsmith._internal.logging import setup_logging\n\n    config = get_config()\n    setup_logging(level=verbosity, output_dir=output_path, config=config.logging)\n\n    logger.info(\"Exploring design space for dataflow accelerator:\")\n    logger.info(f\"    Model: {model_path}\")\n    logger.info(f\"    Blueprint: {blueprint_path}\")\n    logger.info(f\"    Output: {output_dir}\")\n\n    # Parse blueprint (with optional step slicing)\n    design_space, blueprint_config = parse_blueprint(\n        blueprint_path,\n        str(Path(model_path).absolute()),\n        start_step=start_step_override,\n        stop_step=stop_step_override,\n    )\n\n    # Build DSE tree\n    tree_builder = DSETreeBuilder()\n    tree = tree_builder.build_tree(design_space, blueprint_config)\n\n    logger.debug(\n        f\"Design space: {len(design_space.steps)} steps, \"\n        f\"{len(design_space.kernel_backends)} kernels\"\n    )\n\n    # Log tree statistics\n    stats = tree.get_statistics()\n    logger.debug(\"DSE tree:\")\n    logger.debug(f\"  - Total paths: {stats['total_paths']:,}\")\n    logger.debug(f\"  - Total segments: {stats['total_segments']:,}\")\n    logger.debug(f\"  - Segment efficiency: {stats['segment_efficiency']}%\")\n\n    # Create runner and execute\n    finn_adapter = FINNAdapter()\n    runner = SegmentRunner(finn_adapter, tree.root.finn_config)\n    results = runner.run_tree(\n        tree=tree, initial_model=Path(model_path), output_dir=Path(output_dir)\n    )\n\n    # Validate results\n    results.validate_success(Path(output_dir))\n\n    # Log success\n    result_stats = results.compute_stats()\n    logger.info(\"Design space exploration completed successfully\")\n    logger.info(f\"  Successful builds: {result_stats['successful']}/{result_stats['total']}\")\n    logger.info(f\"  Total time: {results.total_time:.2f}s\")\n    logger.info(f\"  Output directory: {output_dir}\")\n\n    return TreeExecutionResult(\n        segment_results=results.segment_results,\n        total_time=results.total_time,\n        design_space=design_space,\n        dse_tree=tree,\n    )\n</code></pre>"},{"location":"api/dse/#brainsmith.dse.parse_blueprint","title":"parse_blueprint","text":"<pre><code>parse_blueprint(blueprint_path: str, model_path: str, start_step: str | None = None, stop_step: str | None = None) -&gt; tuple[GlobalDesignSpace, DSEConfig]\n</code></pre> <p>Parse blueprint YAML to design space and configuration.</p> <p>Supports blueprint inheritance via the 'extends' field.</p> <p>Parameters:</p> Name Type Description Default <code>blueprint_path</code> <code>str</code> <p>Path to blueprint YAML file</p> required <code>model_path</code> <code>str</code> <p>Path to model file</p> required <code>start_step</code> <code>str | None</code> <p>Optional start step override</p> <code>None</code> <code>stop_step</code> <code>str | None</code> <p>Optional stop step override</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[GlobalDesignSpace, DSEConfig]</code> <p>Tuple of (GlobalDesignSpace, DSEConfig)</p> Source code in <code>brainsmith/dse/_parser/__init__.py</code> <pre><code>def parse_blueprint(\n    blueprint_path: str,\n    model_path: str,\n    start_step: str | None = None,\n    stop_step: str | None = None,\n) -&gt; tuple[GlobalDesignSpace, DSEConfig]:\n    \"\"\"Parse blueprint YAML to design space and configuration.\n\n    Supports blueprint inheritance via the 'extends' field.\n\n    Args:\n        blueprint_path: Path to blueprint YAML file\n        model_path: Path to model file\n        start_step: Optional start step override\n        stop_step: Optional stop step override\n\n    Returns:\n        Tuple of (GlobalDesignSpace, DSEConfig)\n    \"\"\"\n    # Load blueprint data and check for inheritance\n    raw_data, merged_data, parent_path = load_blueprint_with_inheritance(blueprint_path)\n\n    parent_steps = None\n\n    # If this blueprint extends another, first parse the parent\n    if parent_path:\n        # Recursively parse parent to get its fully resolved steps\n        parent_design_space, _ = parse_blueprint(parent_path, model_path)\n        parent_steps = parent_design_space.steps\n\n    # Extract config from merged data\n    blueprint_config = extract_config(merged_data)\n\n    # Override start_step/stop_step from parameters if provided\n    if start_step is not None:\n        blueprint_config.start_step = start_step\n    if stop_step is not None:\n        blueprint_config.stop_step = stop_step\n\n    # Log step range if specified\n    if blueprint_config.start_step or blueprint_config.stop_step:\n        logger.info(\n            f\"Step range: start={blueprint_config.start_step or 'beginning'}, \"\n            f\"stop={blueprint_config.stop_step or 'end'}\"\n        )\n\n    # Parse steps from THIS blueprint only (not inherited steps)\n    # Use raw_data to get only the steps defined in this file\n    steps_data = raw_data.get(\"design_space\", {}).get(\"steps\", [])\n    steps = parse_steps(steps_data, parent_steps=parent_steps)\n\n    # Parse kernels (use merged data to inherit kernels)\n    kernel_backends = parse_kernels(merged_data.get(\"design_space\", {}).get(\"kernels\", []))\n\n    # Get max_combinations from environment or use default\n    max_combinations = int(os.environ.get(\"BRAINSMITH_MAX_COMBINATIONS\", \"100000\"))\n\n    design_space = GlobalDesignSpace(\n        model_path=model_path,\n        steps=steps,\n        kernel_backends=kernel_backends,\n        max_combinations=max_combinations,\n    )\n    return design_space, blueprint_config\n</code></pre>"},{"location":"api/dse/#brainsmith.dse.build_tree","title":"build_tree","text":"<pre><code>build_tree(design_space: GlobalDesignSpace, config: DSEConfig) -&gt; DSETree\n</code></pre> <p>Build execution tree from design space.</p> <p>Separates tree construction from execution for inspection and validation.</p> <p>Parameters:</p> Name Type Description Default <code>design_space</code> <code>GlobalDesignSpace</code> <p>GlobalDesignSpace with steps and kernel backends</p> required <code>config</code> <code>DSEConfig</code> <p>DSEConfig with FINN configuration</p> required <p>Returns:</p> Type Description <code>DSETree</code> <p>DSETree ready for execution</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If tree exceeds max_combinations limit</p> Source code in <code>brainsmith/dse/api.py</code> <pre><code>def build_tree(design_space: GlobalDesignSpace, config: DSEConfig) -&gt; DSETree:\n    \"\"\"Build execution tree from design space.\n\n    Separates tree construction from execution for inspection and validation.\n\n    Args:\n        design_space: GlobalDesignSpace with steps and kernel backends\n        config: DSEConfig with FINN configuration\n\n    Returns:\n        DSETree ready for execution\n\n    Raises:\n        ValueError: If tree exceeds max_combinations limit\n    \"\"\"\n    builder = DSETreeBuilder()\n    return builder.build_tree(design_space, config)\n</code></pre>"},{"location":"api/dse/#brainsmith.dse.execute_tree","title":"execute_tree","text":"<pre><code>execute_tree(tree: DSETree, model_path: str, config: DSEConfig, output_dir: str, runner: SegmentRunner | None = None) -&gt; TreeExecutionResult\n</code></pre> <p>Execute a pre-built DSE tree.</p> <p>Separates execution from tree construction for custom execution strategies.</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>DSETree</code> <p>Pre-built DSETree to execute</p> required <code>model_path</code> <code>str</code> <p>Path to initial ONNX model</p> required <code>config</code> <code>DSEConfig</code> <p>DSEConfig (used for validation)</p> required <code>output_dir</code> <code>str</code> <p>Base output directory</p> required <code>runner</code> <code>SegmentRunner | None</code> <p>Custom segment runner (uses default if None)</p> <code>None</code> <p>Returns:</p> Type Description <code>TreeExecutionResult</code> <p>TreeExecutionResult with segment results</p> <p>Raises:</p> Type Description <code>ExecutionError</code> <p>If no successful builds were produced</p> Source code in <code>brainsmith/dse/api.py</code> <pre><code>def execute_tree(\n    tree: DSETree,\n    model_path: str,\n    config: DSEConfig,\n    output_dir: str,\n    runner: SegmentRunner | None = None,\n) -&gt; TreeExecutionResult:\n    \"\"\"Execute a pre-built DSE tree.\n\n    Separates execution from tree construction for custom execution strategies.\n\n    Args:\n        tree: Pre-built DSETree to execute\n        model_path: Path to initial ONNX model\n        config: DSEConfig (used for validation)\n        output_dir: Base output directory\n        runner: Custom segment runner (uses default if None)\n\n    Returns:\n        TreeExecutionResult with segment results\n\n    Raises:\n        ExecutionError: If no successful builds were produced\n    \"\"\"\n    output_path = Path(output_dir)\n    output_path.mkdir(parents=True, exist_ok=True)\n\n    # Create default runner if not provided\n    if runner is None:\n        finn_adapter = FINNAdapter()\n        runner = SegmentRunner(finn_adapter, tree.root.finn_config)\n\n    # Execute tree\n    results = runner.run_tree(tree=tree, initial_model=Path(model_path), output_dir=output_path)\n\n    # Validate results\n    results.validate_success(output_path)\n\n    # Return result with all fields\n    return TreeExecutionResult(\n        segment_results=results.segment_results,\n        total_time=results.total_time,\n        design_space=None,  # Not available in this path\n        dse_tree=tree,\n    )\n</code></pre>"},{"location":"api/dse/#brainsmith.dse.SegmentRunner","title":"SegmentRunner","text":"<pre><code>SegmentRunner(finn_adapter: FINNAdapter, base_config: dict[str, Any])\n</code></pre> <p>Runs DSE segments using FINN.</p> <p>Handles both tree traversal and individual segment execution using FINNAdapter for all FINN interactions.</p> <p>Initialize runner.</p> <p>Parameters:</p> Name Type Description Default <code>finn_adapter</code> <code>FINNAdapter</code> <p>Adapter for FINN-specific operations</p> required <code>base_config</code> <code>dict[str, Any]</code> <p>FINN configuration from blueprint</p> required Source code in <code>brainsmith/dse/runner.py</code> <pre><code>def __init__(self, finn_adapter: FINNAdapter, base_config: dict[str, Any]) -&gt; None:\n    \"\"\"Initialize runner.\n\n    Args:\n        finn_adapter: Adapter for FINN-specific operations\n        base_config: FINN configuration from blueprint\n    \"\"\"\n    self.finn_adapter = finn_adapter\n    self.base_config = base_config\n\n    # Extract settings from FINN config\n    self.fail_fast = False  # TODO: Add more robust tree exit options\n    output_products = base_config.get(\"output_products\", [\"estimates\"])\n    output_product = output_products[0] if output_products else \"estimates\"\n    self.output_type = OutputType.from_finn_product(output_product)\n</code></pre>"},{"location":"api/dse/#brainsmith.dse.SegmentRunner.run_segment","title":"run_segment","text":"<pre><code>run_segment(segment: DSESegment, input_model: Path, base_output_dir: Path) -&gt; SegmentResult\n</code></pre> <p>Run a single DSE segment.</p> <p>Parameters:</p> Name Type Description Default <code>segment</code> <code>DSESegment</code> <p>Segment to execute</p> required <code>input_model</code> <code>Path</code> <p>Input ONNX model path</p> required <code>base_output_dir</code> <code>Path</code> <p>Base output directory</p> required <p>Returns:</p> Type Description <code>SegmentResult</code> <p>SegmentResult with execution details</p> Source code in <code>brainsmith/dse/runner.py</code> <pre><code>def run_segment(\n    self, segment: DSESegment, input_model: Path, base_output_dir: Path\n) -&gt; SegmentResult:\n    \"\"\"Run a single DSE segment.\n\n    Args:\n        segment: Segment to execute\n        input_model: Input ONNX model path\n        base_output_dir: Base output directory\n\n    Returns:\n        SegmentResult with execution details\n    \"\"\"\n    segment_dir = base_output_dir / segment.segment_id\n    output_model = segment_dir / \"output.onnx\"\n\n    # Check cache validity\n    if output_model.exists():\n        try:\n            onnx.load(str(output_model))\n            # Valid cache - return immediately\n            logger.debug(f\"Cache hit: {segment.segment_id}\")\n            return SegmentResult(\n                segment_id=segment.segment_id,\n                status=SegmentStatus.COMPLETED,\n                output_model=output_model,\n                output_dir=segment_dir,\n                cached=True,\n            )\n        except (OnnxValidationError, OnnxInferenceError) as e:\n            # Invalid ONNX model - rebuild\n            logger.warning(f\"Invalid cache for {segment.segment_id}, rebuilding: {e}\")\n            output_model.unlink()\n        except OSError as e:\n            # File corruption (rare but possible)\n            logger.warning(f\"Corrupted cache for {segment.segment_id}, rebuilding: {e}\")\n            output_model.unlink()\n\n    # Cache miss or invalid - execute build\n    logger.debug(f\"Building segment: {segment.segment_id}\")\n\n    # Create FINN config\n    finn_config = self._make_finn_config(segment, segment_dir)\n\n    # Prepare directory and model\n    segment_dir.mkdir(parents=True, exist_ok=True)\n    segment_input = segment_dir / \"input.onnx\"\n    self.finn_adapter.prepare_model(input_model, segment_input)\n\n    # Execute build\n    start_time = time.time()\n\n    try:\n        # Use adapter for clean FINN interaction\n        final_model = self.finn_adapter.build(segment_input, finn_config, segment_dir)\n\n        if final_model:\n            # Copy to expected location\n            self.finn_adapter.prepare_model(final_model, output_model)\n            logger.debug(\n                f\"Completed segment: {segment.segment_id} ({time.time() - start_time:.1f}s)\"\n            )\n            return SegmentResult(\n                segment_id=segment.segment_id,\n                status=SegmentStatus.COMPLETED,\n                output_model=output_model,\n                output_dir=segment_dir,\n                execution_time=time.time() - start_time,\n            )\n        else:\n            raise RuntimeError(\"Build succeeded but no output model generated\")\n\n    except Exception as e:\n        contextualized_error = self._add_segment_context(segment.segment_id, e)\n        logger.error(f\"Segment failed: {segment.segment_id}: {contextualized_error}\")\n        if not isinstance(e, ExecutionError):\n            logger.exception(\"Unexpected error details:\")\n        raise contextualized_error\n</code></pre>"},{"location":"api/dse/#brainsmith.dse.SegmentRunner.run_tree","title":"run_tree","text":"<pre><code>run_tree(tree: DSETree, initial_model: Path, output_dir: Path) -&gt; TreeExecutionResult\n</code></pre> <p>Run all segments in the DSE tree.</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>DSETree</code> <p>DSE tree to execute</p> required <code>initial_model</code> <code>Path</code> <p>Path to initial ONNX model</p> required <code>output_dir</code> <code>Path</code> <p>Base output directory</p> required <p>Returns:</p> Type Description <code>TreeExecutionResult</code> <p>TreeExecutionResult with all segment results</p> Source code in <code>brainsmith/dse/runner.py</code> <pre><code>def run_tree(self, tree: DSETree, initial_model: Path, output_dir: Path) -&gt; TreeExecutionResult:\n    \"\"\"Run all segments in the DSE tree.\n\n    Args:\n        tree: DSE tree to execute\n        initial_model: Path to initial ONNX model\n        output_dir: Base output directory\n\n    Returns:\n        TreeExecutionResult with all segment results\n    \"\"\"\n    output_dir = Path(output_dir)\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    logger.debug(f\"Executing tree with fail_fast={self.fail_fast}\")\n    logger.debug(f\"Output directory: {output_dir}\")\n\n    results = {}\n    skipped = set()\n    start_time = time.time()\n\n    # Use a stack for cleaner iteration\n    stack = [(tree.root, initial_model, 0)]\n\n    while stack:\n        segment, input_model, depth = stack.pop()\n        indent = \"  \" * depth\n\n        # Skip if parent failed\n        if segment.segment_id in skipped:\n            logger.warning(f\"{indent}Skipped: {segment.segment_id}\")\n            results[segment.segment_id] = SegmentResult(\n                segment_id=segment.segment_id,\n                status=SegmentStatus.SKIPPED,\n                error=\"Parent segment failed\",\n            )\n            continue\n\n        # Execute segment\n        logger.debug(f\"{indent}Executing: {segment.segment_id}\")\n\n        # Skip empty segments (e.g., root with immediate branches)\n        if not segment.steps:\n            logger.debug(f\"{indent}  (empty segment, passing through)\")\n            # Create a pass-through result\n            results[segment.segment_id] = SegmentResult(\n                segment_id=segment.segment_id,\n                status=SegmentStatus.COMPLETED,\n                output_model=input_model,  # Pass input as output\n                output_dir=output_dir / segment.segment_id,\n                execution_time=0,\n            )\n            # Add children to stack\n            for child in reversed(list(segment.children.values())):\n                stack.append((child, input_model, depth + 1))\n            continue\n\n        try:\n            result = self.run_segment(segment, input_model, output_dir)\n            results[segment.segment_id] = result\n        except KeyboardInterrupt:\n            logger.warning(\"Build cancelled by user\")\n            raise\n        except Exception as e:\n            wrapped_error = self._add_segment_context(segment.segment_id, e)\n            logger.error(f\"Segment failed: {segment.segment_id}: {wrapped_error}\")\n            if not isinstance(e, ExecutionError):\n                logger.exception(\"Unexpected error details:\")\n\n            if self.fail_fast:\n                raise wrapped_error\n\n            # Create failure result\n            results[segment.segment_id] = SegmentResult(\n                segment_id=segment.segment_id,\n                status=SegmentStatus.FAILED,\n                error=str(wrapped_error),\n                execution_time=0,\n            )\n\n            # Mark descendants for skipping\n            self._mark_descendants_skipped(segment, skipped)\n            for child in reversed(list(segment.children.values())):\n                stack.append((child, None, depth + 1))\n            continue\n\n        # Share artifacts at branch points\n        if segment.is_branch_point:\n            _share_artifacts_at_branch(result, list(segment.children.values()), output_dir)\n\n        # Add children to stack (reversed for correct order)\n        for child in reversed(list(segment.children.values())):\n            stack.append((child, result.output_model, depth + 1))\n\n    # Create result and print summary\n    total_time = time.time() - start_time\n    result = TreeExecutionResult(results, total_time)\n    self._print_summary(result)\n\n    return result\n</code></pre>"},{"location":"api/dse/#brainsmith.dse.DSEConfig","title":"DSEConfig  <code>dataclass</code>","text":"<pre><code>DSEConfig(clock_ns: float, output: OutputType = OutputType.ESTIMATES, board: str | None = None, start_step: str | None = None, stop_step: str | None = None, finn_overrides: dict[str, Any] = dict())\n</code></pre> <p>Configuration for Design Space Exploration.</p> <p>Attributes:</p> Name Type Description <code>clock_ns</code> <code>float</code> <p>Target clock period in nanoseconds</p> <code>output</code> <code>OutputType</code> <p>Output type (estimates, rtl, or bitfile)</p> <code>board</code> <code>str | None</code> <p>Target board (required for rtl/bitfile)</p> <code>start_step</code> <code>str | None</code> <p>Optional pipeline starting step</p> <code>stop_step</code> <code>str | None</code> <p>Optional pipeline ending step</p> <code>finn_overrides</code> <code>dict[str, Any]</code> <p>Direct FINN configuration overrides</p>"},{"location":"api/dse/#brainsmith.dse.DSEConfig.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> <p>Validate configuration invariants.</p> Source code in <code>brainsmith/dse/config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Validate configuration invariants.\"\"\"\n    # Validate clock_ns is positive\n    if self.clock_ns &lt;= 0:\n        raise ValueError(f\"clock_ns must be positive, got {self.clock_ns}\")\n\n    # Validate output type dependencies\n    if self.output != OutputType.ESTIMATES and not self.board:\n        raise ValueError(f\"{self.output.value} requires board specification\")\n</code></pre>"},{"location":"api/dse/#brainsmith.dse.GlobalDesignSpace","title":"GlobalDesignSpace  <code>dataclass</code>","text":"<pre><code>GlobalDesignSpace(model_path: str, steps: list[str | list[str | None]], kernel_backends: list[tuple[str, list[type]]], max_combinations: int = 100000)\n</code></pre> <p>Design space ready for DSE tree construction.</p> <p>Attributes:</p> Name Type Description <code>model_path</code> <code>str</code> <p>Path to ONNX model file</p> <code>steps</code> <code>list[str | list[str | None]]</code> <p>Pipeline steps with optional variations (list = branch point)</p> <code>kernel_backends</code> <code>list[tuple[str, list[type]]]</code> <p>Kernel names mapped to backend classes</p> <code>max_combinations</code> <code>int</code> <p>Maximum allowed design space size (default: 100,000)</p> <p>Validates size on initialization and provides kernel summary formatting.</p>"},{"location":"api/dse/#brainsmith.dse.GlobalDesignSpace.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Validate design space after initialization.</p> Source code in <code>brainsmith/dse/design_space.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate design space after initialization.\"\"\"\n    self._validate_size()\n</code></pre>"},{"location":"api/dse/#brainsmith.dse.GlobalDesignSpace.__str__","title":"__str__","text":"<pre><code>__str__() -&gt; str\n</code></pre> <p>Human-readable representation.</p> Source code in <code>brainsmith/dse/design_space.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Human-readable representation.\"\"\"\n    return (\n        f\"GlobalDesignSpace(\\n\"\n        f\"  model: {self.model_path}\\n\"\n        f\"  steps: {len(self.steps)}\\n\"\n        f\"  kernels: {len(self.kernel_backends)}\\n\"\n        f\")\"\n    )\n</code></pre>"},{"location":"api/dse/#brainsmith.dse.GlobalDesignSpace.get_kernel_summary","title":"get_kernel_summary","text":"<pre><code>get_kernel_summary() -&gt; str\n</code></pre> <p>Get human-readable summary of kernels and backends.</p> Source code in <code>brainsmith/dse/design_space.py</code> <pre><code>def get_kernel_summary(self) -&gt; str:\n    \"\"\"Get human-readable summary of kernels and backends.\"\"\"\n    lines = []\n    for kernel_name, backend_classes in self.kernel_backends:\n        backend_names = [cls.__name__ for cls in backend_classes]\n        lines.append(f\"  {kernel_name}: {', '.join(backend_names)}\")\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"api/dse/#brainsmith.dse.TreeExecutionResult","title":"TreeExecutionResult  <code>dataclass</code>","text":"<pre><code>TreeExecutionResult(segment_results: dict[str, SegmentResult], total_time: float, design_space: GlobalDesignSpace | None = None, dse_tree: DSETree | None = None)\n</code></pre> <p>Results from design space exploration execution.</p> <p>Attributes:</p> Name Type Description <code>segment_results</code> <code>dict[str, SegmentResult]</code> <p>Execution results for each DSE segment</p> <code>total_time</code> <code>float</code> <p>Total execution time in seconds</p> <code>design_space</code> <code>GlobalDesignSpace | None</code> <p>Original design space (if available)</p> <code>dse_tree</code> <code>DSETree | None</code> <p>Execution tree structure (if available)</p>"},{"location":"api/dse/#brainsmith.dse.TreeExecutionResult.compute_stats","title":"compute_stats","text":"<pre><code>compute_stats() -&gt; dict[str, int]\n</code></pre> <p>Compute execution statistics.</p> <p>Returns:</p> Type Description <code>dict[str, int]</code> <p>Dict with counts: total, successful, failed, cached, skipped</p> Source code in <code>brainsmith/dse/types.py</code> <pre><code>def compute_stats(self) -&gt; dict[str, int]:\n    \"\"\"Compute execution statistics.\n\n    Returns:\n        Dict with counts: total, successful, failed, cached, skipped\n    \"\"\"\n    total = successful = failed = cached = skipped = 0\n\n    for r in self.segment_results.values():\n        total += 1\n        if r.status == SegmentStatus.COMPLETED:\n            if r.cached:\n                cached += 1\n            else:\n                successful += 1\n        elif r.status == SegmentStatus.FAILED:\n            failed += 1\n        elif r.status == SegmentStatus.SKIPPED:\n            skipped += 1\n\n    return {\n        \"total\": total,\n        \"successful\": successful,\n        \"failed\": failed,\n        \"cached\": cached,\n        \"skipped\": skipped,\n    }\n</code></pre>"},{"location":"api/dse/#brainsmith.dse.TreeExecutionResult.validate_success","title":"validate_success","text":"<pre><code>validate_success(output_dir: Path) -&gt; None\n</code></pre> <p>Validate that results contain at least one successful build.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>Path</code> <p>Output directory for error messages</p> required <p>Raises:</p> Type Description <code>ExecutionError</code> <p>If no valid builds exist</p> Source code in <code>brainsmith/dse/types.py</code> <pre><code>def validate_success(self, output_dir: Path) -&gt; None:\n    \"\"\"Validate that results contain at least one successful build.\n\n    Args:\n        output_dir: Output directory for error messages\n\n    Raises:\n        ExecutionError: If no valid builds exist\n    \"\"\"\n    stats = self.compute_stats()\n    valid_builds = stats[\"successful\"] + stats[\"cached\"]\n\n    if valid_builds == 0:\n        raise ExecutionError(\n            f\"DSE failed: No successful builds\\n\"\n            f\"  Failed: {stats['failed']}\\n\"\n            f\"  Skipped: {stats['skipped']}\\n\"\n            f\"  Check segment logs in: {output_dir}/*/\\n\"\n            f\"  Run with --log-level debug for detailed output\"\n        )\n\n    if stats[\"successful\"] == 0 and stats[\"cached\"] &gt; 0:\n        logger.warning(\n            f\"All builds used cached results ({stats['cached']} cached). \"\n            f\"No new builds were executed.\"\n        )\n</code></pre>"},{"location":"api/dse/#brainsmith.dse.SegmentResult","title":"SegmentResult  <code>dataclass</code>","text":"<pre><code>SegmentResult(segment_id: str, status: SegmentStatus, output_model: Path | None = None, output_dir: Path | None = None, error: str | None = None, execution_time: float = 0, cached: bool = False)\n</code></pre> <p>Result from executing a single DSE segment.</p> <p>Attributes:</p> Name Type Description <code>segment_id</code> <code>str</code> <p>Unique identifier for the segment</p> <code>status</code> <code>SegmentStatus</code> <p>Execution status (completed, failed, skipped, etc.)</p> <code>output_model</code> <code>Path | None</code> <p>Path to output ONNX model (if successful)</p> <code>output_dir</code> <code>Path | None</code> <p>Directory containing build artifacts</p> <code>error</code> <code>str | None</code> <p>Error message (if failed)</p> <code>execution_time</code> <code>float</code> <p>Execution time in seconds</p> <code>cached</code> <code>bool</code> <p>Whether result was retrieved from cache</p>"},{"location":"api/dse/#brainsmith.dse.SegmentStatus","title":"SegmentStatus","text":"<p>               Bases: <code>Enum</code></p> <p>Execution status for DSE segments.</p> <p>Attributes:</p> Name Type Description <code>PENDING</code> <p>Segment not yet executed</p> <code>RUNNING</code> <p>Segment currently executing</p> <code>COMPLETED</code> <p>Segment executed successfully</p> <code>FAILED</code> <p>Segment execution failed</p> <code>SKIPPED</code> <p>Segment skipped due to parent failure</p>"},{"location":"api/dse/#brainsmith.dse.OutputType","title":"OutputType","text":"<p>               Bases: <code>Enum</code></p> <p>Build output types for DSE execution.</p> <p>Attributes:</p> Name Type Description <code>ESTIMATES</code> <p>Performance estimates only (fastest)</p> <code>RTL</code> <p>RTL simulation and IP generation</p> <code>BITFILE</code> <p>Full bitstream generation (slowest)</p>"},{"location":"api/dse/#brainsmith.dse.OutputType.from_finn_product","title":"from_finn_product  <code>classmethod</code>","text":"<pre><code>from_finn_product(product: str) -&gt; OutputType\n</code></pre> <p>Get OutputType from FINN product string.</p> <p>Parameters:</p> Name Type Description Default <code>product</code> <code>str</code> <p>FINN product name (e.g., 'estimates', 'bitfile', 'rtl_sim')</p> required <p>Returns:</p> Type Description <code>OutputType</code> <p>Matching OutputType</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If product is unknown</p> Example <p>OutputType.from_finn_product('bitfile')  OutputType.from_finn_product('rtl_sim') </p> Source code in <code>brainsmith/dse/types.py</code> <pre><code>@classmethod\ndef from_finn_product(cls, product: str) -&gt; OutputType:\n    \"\"\"Get OutputType from FINN product string.\n\n    Args:\n        product: FINN product name (e.g., 'estimates', 'bitfile', 'rtl_sim')\n\n    Returns:\n        Matching OutputType\n\n    Raises:\n        ValueError: If product is unknown\n\n    Example:\n        &gt;&gt;&gt; OutputType.from_finn_product('bitfile')\n        &lt;OutputType.BITFILE: 'bitfile'&gt;\n        &gt;&gt;&gt; OutputType.from_finn_product('rtl_sim')\n        &lt;OutputType.RTL: 'rtl'&gt;\n    \"\"\"\n    for output_type in cls:\n        if product in output_type.to_finn_products():\n            return output_type\n\n    # Product not found - create helpful error message\n    all_products = [p for ot in cls for p in ot.to_finn_products()]\n    raise ValueError(\n        f\"Unknown FINN product '{product}'. \" f\"Valid products: {', '.join(all_products)}\"\n    )\n</code></pre>"},{"location":"api/dse/#brainsmith.dse.OutputType.to_finn_outputs","title":"to_finn_outputs","text":"<pre><code>to_finn_outputs() -&gt; list[str]\n</code></pre> <p>Convert to FINN generate_outputs configuration.</p> Source code in <code>brainsmith/dse/types.py</code> <pre><code>def to_finn_outputs(self) -&gt; list[str]:\n    \"\"\"Convert to FINN generate_outputs configuration.\"\"\"\n    return {\n        OutputType.ESTIMATES: [\"estimate_reports\"],\n        OutputType.RTL: [\"estimate_reports\", \"rtlsim_performance\", \"stitched_ip\"],\n        OutputType.BITFILE: [\n            \"estimate_reports\",\n            \"rtlsim_performance\",\n            \"stitched_ip\",\n            \"bitfile\",\n            \"deployment_package\",\n        ],\n    }[self]\n</code></pre>"},{"location":"api/dse/#brainsmith.dse.OutputType.to_finn_products","title":"to_finn_products","text":"<pre><code>to_finn_products() -&gt; list[str]\n</code></pre> <p>Convert to FINN output_products configuration.</p> Source code in <code>brainsmith/dse/types.py</code> <pre><code>def to_finn_products(self) -&gt; list[str]:\n    \"\"\"Convert to FINN output_products configuration.\"\"\"\n    return {\n        OutputType.ESTIMATES: [\"estimates\"],\n        OutputType.RTL: [\"rtl_sim\", \"ip_gen\"],\n        OutputType.BITFILE: [\"bitfile\"],\n    }[self]\n</code></pre>"},{"location":"api/dse/#brainsmith.dse.ExecutionError","title":"ExecutionError","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised during DSE execution failures.</p>"},{"location":"api/dse/#brainsmith.dse.DSETree","title":"DSETree","text":"<pre><code>DSETree(root: DSESegment)\n</code></pre> <p>Design space exploration tree structure.</p> <p>Represents the complete exploration space as a tree of segments, where each segment contains steps to execute between branch points.</p> <p>Attributes:</p> Name Type Description <code>root</code> <p>Root segment of the tree</p> Source code in <code>brainsmith/dse/tree.py</code> <pre><code>def __init__(self, root: DSESegment):\n    self.root = root\n</code></pre>"},{"location":"api/dse/#brainsmith.dse.DSETree.format_tree","title":"format_tree","text":"<pre><code>format_tree() -&gt; str\n</code></pre> <p>Format tree as a string representation.</p> <p>Returns:</p> Type Description <code>str</code> <p>Multi-line string with ASCII tree visualization</p> Example <p>tree = build_tree(design_space, config) print(tree.format_tree()) \u2514\u2500\u2500 transform_step_1 (3 steps)     \u251c\u2500\u2500 kernel_backend_A (2 steps)     \u2514\u2500\u2500 kernel_backend_B (2 steps)</p> Source code in <code>brainsmith/dse/tree.py</code> <pre><code>def format_tree(self) -&gt; str:\n    \"\"\"Format tree as a string representation.\n\n    Returns:\n        Multi-line string with ASCII tree visualization\n\n    Example:\n        &gt;&gt;&gt; tree = build_tree(design_space, config)\n        &gt;&gt;&gt; print(tree.format_tree())\n        \u2514\u2500\u2500 transform_step_1 (3 steps)\n            \u251c\u2500\u2500 kernel_backend_A (2 steps)\n            \u2514\u2500\u2500 kernel_backend_B (2 steps)\n    \"\"\"\n    lines = []\n    self._format_node(self.root, \"\", True, lines)\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"api/dse/#brainsmith.dse.DSETree.get_all_segments","title":"get_all_segments","text":"<pre><code>get_all_segments() -&gt; list[DSESegment]\n</code></pre> <p>Get all segments in the tree.</p> Source code in <code>brainsmith/dse/tree.py</code> <pre><code>def get_all_segments(self) -&gt; list[DSESegment]:\n    \"\"\"Get all segments in the tree.\"\"\"\n    all_segments = []\n\n    def collect_segments(node: DSESegment):\n        all_segments.append(node)\n        for child in node.children.values():\n            collect_segments(child)\n\n    collect_segments(self.root)\n    return all_segments\n</code></pre>"},{"location":"api/dse/#brainsmith.dse.DSETree.get_execution_order","title":"get_execution_order","text":"<pre><code>get_execution_order() -&gt; list[DSESegment]\n</code></pre> <p>Get breadth-first execution order for the tree.</p> <p>Skips root if it has no steps (purely structural).</p> Source code in <code>brainsmith/dse/tree.py</code> <pre><code>def get_execution_order(self) -&gt; list[DSESegment]:\n    \"\"\"Get breadth-first execution order for the tree.\n\n    Skips root if it has no steps (purely structural).\n    \"\"\"\n    # Start from root's children if root is structural-only\n    if not self.root.steps and self.root.children:\n        queue = deque(self.root.children.values())\n    else:\n        queue = deque([self.root])\n\n    order = []\n\n    while queue:\n        node = queue.popleft()  # O(1)\n        order.append(node)\n        queue.extend(node.children.values())\n\n    return order\n</code></pre>"},{"location":"api/dse/#brainsmith.dse.DSETree.get_statistics","title":"get_statistics","text":"<pre><code>get_statistics() -&gt; dict[str, Any]\n</code></pre> <p>Get statistics about the DSE tree.</p> Source code in <code>brainsmith/dse/tree.py</code> <pre><code>def get_statistics(self) -&gt; dict[str, Any]:\n    \"\"\"Get statistics about the DSE tree.\"\"\"\n    stats = {\"nodes\": 0, \"leaves\": 0, \"max_depth\": 0, \"total_steps\": 0, \"leaf_steps\": []}\n\n    def traverse(node: DSESegment, depth: int = 0):\n        stats[\"nodes\"] += 1\n        stats[\"total_steps\"] += len(node.steps)\n        stats[\"max_depth\"] = max(stats[\"max_depth\"], depth)\n\n        if not node.children:\n            # Leaf node\n            stats[\"leaves\"] += 1\n            stats[\"leaf_steps\"].append(len(node.get_all_steps()))\n        else:\n            for child in node.children.values():\n                traverse(child, depth + 1)\n\n    traverse(self.root)\n\n    # Calculate efficiency\n    steps_without_segments = sum(stats[\"leaf_steps\"])\n    segment_efficiency = (\n        1 - stats[\"total_steps\"] / steps_without_segments if steps_without_segments else 0\n    )\n\n    return {\n        \"total_paths\": stats[\"leaves\"],\n        \"total_segments\": stats[\"nodes\"],\n        \"max_depth\": stats[\"max_depth\"],\n        \"total_steps\": stats[\"total_steps\"],\n        \"steps_without_segments\": steps_without_segments,\n        \"segment_efficiency\": round(segment_efficiency * 100, 1),\n        \"avg_steps_per_segment\": (\n            round(stats[\"total_steps\"] / stats[\"nodes\"], 1) if stats[\"nodes\"] &gt; 0 else 0\n        ),\n    }\n</code></pre>"},{"location":"api/dse/#brainsmith.dse.DSESegment","title":"DSESegment  <code>dataclass</code>","text":"<pre><code>DSESegment(steps: list[dict[str, Any]], branch_choice: str | None = None, parent: DSESegment | None = None, children: dict[str, DSESegment] = dict(), status: SegmentStatus = SegmentStatus.PENDING, output_dir: Path | None = None, error: str | None = None, execution_time: float | None = None, finn_config: dict[str, Any] = dict())\n</code></pre> <p>A segment in the design space exploration tree.</p> <p>Each segment is executed as a single FINN build, containing all steps from the last branch point (or root) to the next branch point (or leaf).</p>"},{"location":"api/dse/#brainsmith.dse.DSESegment.segment_id","title":"segment_id  <code>cached</code> <code>property</code>","text":"<pre><code>segment_id: str\n</code></pre> <p>Deterministic ID from branch path (cached for O(1) access).</p>"},{"location":"api/dse/#brainsmith.dse.DSESegment.add_child","title":"add_child","text":"<pre><code>add_child(branch_id: str, steps: list[dict[str, Any]]) -&gt; DSESegment\n</code></pre> <p>Create a child segment for a branch.</p> Source code in <code>brainsmith/dse/segment.py</code> <pre><code>def add_child(self, branch_id: str, steps: list[dict[str, Any]]) -&gt; DSESegment:\n    \"\"\"Create a child segment for a branch.\"\"\"\n    child = DSESegment(\n        steps=steps, branch_choice=branch_id, parent=self, finn_config=self.finn_config.copy()\n    )\n    self.children[branch_id] = child\n    return child\n</code></pre>"},{"location":"api/dse/#brainsmith.dse.DSESegment.count_descendants","title":"count_descendants","text":"<pre><code>count_descendants() -&gt; int\n</code></pre> <p>Count total number of descendant nodes.</p> Source code in <code>brainsmith/dse/segment.py</code> <pre><code>def count_descendants(self) -&gt; int:\n    \"\"\"Count total number of descendant nodes.\"\"\"\n    count = len(self.children)\n    for child in self.children.values():\n        count += child.count_descendants()\n    return count\n</code></pre>"},{"location":"api/dse/#brainsmith.dse.DSESegment.get_all_steps","title":"get_all_steps","text":"<pre><code>get_all_steps() -&gt; list[dict[str, Any]]\n</code></pre> <p>Get all steps from root to end of this segment.</p> Source code in <code>brainsmith/dse/segment.py</code> <pre><code>def get_all_steps(self) -&gt; list[dict[str, Any]]:\n    \"\"\"Get all steps from root to end of this segment.\"\"\"\n    steps = []\n    for segment in self.get_path():\n        steps.extend(segment.steps)\n    return steps\n</code></pre>"},{"location":"api/dse/#brainsmith.dse.DSESegment.get_path","title":"get_path","text":"<pre><code>get_path() -&gt; list[DSESegment]\n</code></pre> <p>Get all segments from root to here.</p> Source code in <code>brainsmith/dse/segment.py</code> <pre><code>def get_path(self) -&gt; list[DSESegment]:\n    \"\"\"Get all segments from root to here.\"\"\"\n    path = []\n    node = self\n    while node:\n        path.append(node)\n        node = node.parent\n    path.reverse()\n    return path\n</code></pre>"},{"location":"api/dse/#see-also","title":"See Also","text":"<ul> <li>Getting Started - Installation and quickstart</li> <li>GitHub Examples - Working code examples</li> </ul>"},{"location":"api/registry/","title":"Component Registry","text":""},{"location":"api/registry/#component-registry","title":"Component Registry","text":"<p>Register components using decorators for hardware kernels, backend implementations (HLS/RTL), and pipeline transformation steps. Components are discovered automatically from brainsmith, FINN, your project, and custom plugins.</p> <p>Example:</p> <pre><code>from brainsmith.dataflow import KernelOp\nfrom brainsmith.registry import kernel\n\n@kernel\nclass ElementwiseBinaryOp(KernelOp):\n    \"\"\"Polymorphic kernel for Add, Mul, Sub, Div operations.\"\"\"\n\n    def get_nodeattr_types(self):\n        return {\"func\": (\"s\", True, \"\")}\n</code></pre> <p>Example:</p> <pre><code>from finn.custom_op.fpgadataflow.hlsbackend import HLSBackend\nfrom brainsmith.registry import backend, get_kernel\n\n# Get the kernel class\nElementwiseBinaryOp = get_kernel('ElementwiseBinaryOp')\n\n@backend(target_kernel='ElementwiseBinaryOp', language='hls')\nclass ElementwiseBinary_hls(ElementwiseBinaryOp, HLSBackend):\n    \"\"\"HLS backend for ElementwiseBinaryOp.\"\"\"\n\n    def get_nodeattr_types(self):\n        # Combine attributes from kernel and backend\n        my_attrs = ElementwiseBinaryOp.get_nodeattr_types(self)\n        my_attrs.update(HLSBackend.get_nodeattr_types(self))\n        return my_attrs\n</code></pre> <p>Example:</p> <pre><code>from brainsmith.registry import step\nfrom qonnx.transformation.infer_shapes import InferShapes\n\n@step(name='qonnx_to_finn')\ndef qonnx_to_finn_step(model, cfg):\n    \"\"\"Convert QONNX to FINN opset.\"\"\"\n    model = model.transform(ConvertQONNXtoFINN())\n    model = model.transform(InferShapes())\n    return model\n</code></pre> <p>Example:</p> <pre><code>from brainsmith.registry import get_kernel\n\n# Get kernel class by name\nkernel_class = get_kernel('ElementwiseBinaryOp')\n</code></pre> <p>Example:</p> <pre><code>from brainsmith.registry import list_kernels\n\n# List all available kernels\nkernels = list_kernels()\nfor name in kernels:\n    print(name)\n</code></pre> <p>Example:</p> <pre><code>from brainsmith.registry import list_backends_for_kernel, get_backend\n\n# Find backends for a kernel\nbackend_names = list_backends_for_kernel('ElementwiseBinaryOp', language='hls')\n\n# Get backend classes\nbackends = [get_backend(name) for name in backend_names]\n</code></pre>"},{"location":"api/registry/#brainsmith.registry.kernel","title":"kernel","text":"<pre><code>kernel(obj=None, **kwargs)\n</code></pre> <p>Register a kernel class.</p> <p>Decorator for hardware kernel implementations. Supports both <code>@kernel</code> and <code>@kernel(name='CustomName')</code> syntax.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <p>Class being decorated (automatic when using @kernel without parens)</p> <code>None</code> <code>**kwargs</code> <p>Optional configuration: name: Kernel name (default: cls.op_type or cls.name) infer_transform: Transform class for ONNX \u2192 kernel inference is_infrastructure: Mark as topology kernel like FIFO (default: False)</p> <code>{}</code> <p>Returns:</p> Type Description <p>The decorated class, unchanged but registered in component index</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from brainsmith.registry import kernel\n&gt;&gt;&gt; from brainsmith.dataflow import KernelOp\n&gt;&gt;&gt;\n&gt;&gt;&gt; @kernel\n... class MyKernel(KernelOp):\n...     op_type = \"MyKernel\"\n...\n&gt;&gt;&gt; @kernel(name=\"CustomName\", is_infrastructure=True)\n... class FIFO(HWCustomOp):\n...     pass\n</code></pre> Source code in <code>brainsmith/registry/_decorators.py</code> <pre><code>def kernel(obj=None, **kwargs):\n    \"\"\"Register a kernel class.\n\n    Decorator for hardware kernel implementations. Supports both `@kernel` and\n    `@kernel(name='CustomName')` syntax.\n\n    Args:\n        obj: Class being decorated (automatic when using @kernel without parens)\n        **kwargs: Optional configuration:\n            name: Kernel name (default: cls.op_type or cls.__name__)\n            infer_transform: Transform class for ONNX \u2192 kernel inference\n            is_infrastructure: Mark as topology kernel like FIFO (default: False)\n\n    Returns:\n        The decorated class, unchanged but registered in component index\n\n    Examples:\n        &gt;&gt;&gt; from brainsmith.registry import kernel\n        &gt;&gt;&gt; from brainsmith.dataflow import KernelOp\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; @kernel\n        ... class MyKernel(KernelOp):\n        ...     op_type = \"MyKernel\"\n        ...\n        &gt;&gt;&gt; @kernel(name=\"CustomName\", is_infrastructure=True)\n        ... class FIFO(HWCustomOp):\n        ...     pass\n    \"\"\"\n\n    def register(cls):\n        kwargs.setdefault(\"name\", getattr(cls, \"op_type\", cls.__name__))\n        _register_kernel(cls, **kwargs)\n        return cls\n\n    return register(obj) if obj is not None else register\n</code></pre>"},{"location":"api/registry/#brainsmith.registry.backend","title":"backend","text":"<pre><code>backend(obj=None, **kwargs)\n</code></pre> <p>Register a backend implementation.</p> <p>Decorator for HLS or RTL backend implementations. Backends must specify their target kernel and implementation language.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <p>Class being decorated (automatic when using @backend without parens)</p> <code>None</code> <code>**kwargs</code> <p>Backend configuration: target_kernel (required): Kernel name this backend implements language (required): Implementation language ('hls' or 'rtl') name: Backend class name (default: cls.name) variant: Optional variant identifier</p> <code>{}</code> <p>Returns:</p> Type Description <p>The decorated class, unchanged but registered in component index</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If target_kernel or language missing</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from brainsmith.registry import backend\n&gt;&gt;&gt;\n&gt;&gt;&gt; @backend(target_kernel='MVAU', language='hls')\n... class MVAU_hls:\n...     @staticmethod\n...     def get_nodeattr_types():\n...         return {...}\n...\n&gt;&gt;&gt; @backend(target_kernel='MVAU', language='rtl', variant='fast')\n... class MVAU_rtl_fast:\n...     pass\n</code></pre> Source code in <code>brainsmith/registry/_decorators.py</code> <pre><code>def backend(obj=None, **kwargs):\n    \"\"\"Register a backend implementation.\n\n    Decorator for HLS or RTL backend implementations. Backends must specify their\n    target kernel and implementation language.\n\n    Args:\n        obj: Class being decorated (automatic when using @backend without parens)\n        **kwargs: Backend configuration:\n            target_kernel (required): Kernel name this backend implements\n            language (required): Implementation language ('hls' or 'rtl')\n            name: Backend class name (default: cls.__name__)\n            variant: Optional variant identifier\n\n    Returns:\n        The decorated class, unchanged but registered in component index\n\n    Raises:\n        ValueError: If target_kernel or language missing\n\n    Examples:\n        &gt;&gt;&gt; from brainsmith.registry import backend\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; @backend(target_kernel='MVAU', language='hls')\n        ... class MVAU_hls:\n        ...     @staticmethod\n        ...     def get_nodeattr_types():\n        ...         return {...}\n        ...\n        &gt;&gt;&gt; @backend(target_kernel='MVAU', language='rtl', variant='fast')\n        ... class MVAU_rtl_fast:\n        ...     pass\n    \"\"\"\n\n    def register(cls):\n        kwargs.setdefault(\"name\", cls.__name__)\n        _register_backend(cls, **kwargs)\n        return cls\n\n    return register(obj) if obj is not None else register\n</code></pre>"},{"location":"api/registry/#brainsmith.registry.step","title":"step","text":"<pre><code>step(obj=None, **kwargs)\n</code></pre> <p>Register a pipeline transformation step.</p> <p>Decorator for model transformation functions. Steps are discovered automatically and can be used in pipeline workflows.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <p>Function or class being decorated (automatic when using @step without parens)</p> <code>None</code> <code>**kwargs</code> <p>Optional configuration: name: Step name (default: function/class name)</p> <code>{}</code> <p>Returns:</p> Type Description <p>The decorated callable, unchanged but registered in component index</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from brainsmith.registry import step\n&gt;&gt;&gt;\n&gt;&gt;&gt; @step\n... def my_optimization(model, config):\n...     # Transform model\n...     return model\n...\n&gt;&gt;&gt; @step(name=\"CustomStepName\")\n... def complex_transform(model, config):\n...     return model\n</code></pre> Source code in <code>brainsmith/registry/_decorators.py</code> <pre><code>def step(obj=None, **kwargs):\n    \"\"\"Register a pipeline transformation step.\n\n    Decorator for model transformation functions. Steps are discovered automatically\n    and can be used in pipeline workflows.\n\n    Args:\n        obj: Function or class being decorated (automatic when using @step without parens)\n        **kwargs: Optional configuration:\n            name: Step name (default: function/class __name__)\n\n    Returns:\n        The decorated callable, unchanged but registered in component index\n\n    Examples:\n        &gt;&gt;&gt; from brainsmith.registry import step\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; @step\n        ... def my_optimization(model, config):\n        ...     # Transform model\n        ...     return model\n        ...\n        &gt;&gt;&gt; @step(name=\"CustomStepName\")\n        ... def complex_transform(model, config):\n        ...     return model\n    \"\"\"\n\n    def register(func_or_class):\n        kwargs.setdefault(\"name\", func_or_class.__name__)\n        _register_step(func_or_class, **kwargs)\n        return func_or_class\n\n    return register(obj) if obj is not None else register\n</code></pre>"},{"location":"api/registry/#brainsmith.registry.source_context","title":"source_context","text":"<pre><code>source_context(source_name: str)\n</code></pre> <p>Context manager for setting component source during discovery.</p> <p>Parameters:</p> Name Type Description Default <code>source_name</code> <code>str</code> <p>Source identifier for components loaded in this context</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; with source_context('project'):\n...     import project_kernels  # Registered with source='project'\n</code></pre> Source code in <code>brainsmith/registry/_decorators.py</code> <pre><code>def __init__(self, source_name: str):\n    self.source_name = source_name\n    self.token = None\n</code></pre>"},{"location":"api/registry/#brainsmith.registry.source_context.__enter__","title":"__enter__","text":"<pre><code>__enter__()\n</code></pre> <p>Enter context, set current source.</p> Source code in <code>brainsmith/registry/_decorators.py</code> <pre><code>def __enter__(self):\n    \"\"\"Enter context, set current source.\"\"\"\n    self.token = _current_source.set(self.source_name)\n    return self\n</code></pre>"},{"location":"api/registry/#brainsmith.registry.source_context.__exit__","title":"__exit__","text":"<pre><code>__exit__(exc_type, exc_val, exc_tb)\n</code></pre> <p>Exit context, restore previous source.</p> Source code in <code>brainsmith/registry/_decorators.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"Exit context, restore previous source.\"\"\"\n    _current_source.reset(self.token)\n    return False\n</code></pre>"},{"location":"api/registry/#brainsmith.registry.discover_components","title":"discover_components","text":"<pre><code>discover_components(use_cache: bool = True, force_refresh: bool = False)\n</code></pre> <p>Discover components from all configured sources.</p> <p>Discovers kernels, backends, and steps from: - Core brainsmith components - Project components (if configured) - Entry points from installed packages (e.g., FINN) - Custom component sources</p> <p>Called automatically on first component lookup.</p> <p>Parameters:</p> Name Type Description Default <code>use_cache</code> <code>bool</code> <p>Use cached manifest if available</p> <code>True</code> <code>force_refresh</code> <code>bool</code> <p>Ignore cache and regenerate manifest</p> <code>False</code> Source code in <code>brainsmith/registry/_discovery.py</code> <pre><code>def discover_components(use_cache: bool = True, force_refresh: bool = False):\n    \"\"\"Discover components from all configured sources.\n\n    Discovers kernels, backends, and steps from:\n    - Core brainsmith components\n    - Project components (if configured)\n    - Entry points from installed packages (e.g., FINN)\n    - Custom component sources\n\n    Called automatically on first component lookup.\n\n    Args:\n        use_cache: Use cached manifest if available\n        force_refresh: Ignore cache and regenerate manifest\n    \"\"\"\n    global _components_discovered\n\n    # Handle force refresh - reset discovery state to allow re-discovery\n    if force_refresh and _components_discovered:\n        logger.debug(\"Force refresh requested - resetting discovery state\")\n        _components_discovered = False\n        _component_index.clear()\n        _discovered_sources.clear()\n\n    # Skip if already discovered (and not forcing refresh)\n    if _components_discovered:\n        return\n\n    with _measure_load(\"discover_components\"):\n        # Check cache_components setting and get project_dir\n        from brainsmith.settings import get_config\n\n        config = get_config()\n        cache_enabled = config.cache_components\n\n        # Use project_dir for manifest location (not CWD)\n        manifest_path = config.project_dir / \".brainsmith\" / \"component_manifest.json\"\n\n        if not cache_enabled:\n            logger.debug(\"Component caching disabled via cache_components setting\")\n            use_cache = False\n\n        # Try cached manifest\n        if use_cache and not force_refresh and manifest_path.exists():\n            try:\n                logger.debug(f\"Loading component manifest from {manifest_path}\")\n                manifest = _load_manifest(manifest_path)\n\n                # Check if cache is stale\n                if _is_manifest_stale(manifest):\n                    logger.debug(\"Manifest is stale - performing full discovery\")\n                    # Don't return, fall through to full discovery\n                else:\n                    _populate_index_from_manifest(manifest)\n                    _components_discovered = True\n\n                    logger.debug(f\"Loaded {len(_component_index)} components from cache\")\n                    return\n            except Exception as e:\n                logger.warning(f\"Failed to load manifest cache: {e}\")\n                logger.debug(\"Falling back to full discovery...\")\n\n        # Full discovery\n        logger.debug(\"Discovering components from all sources...\")\n\n        # Get config for component sources\n        config = get_config()\n\n        # 1. Core brainsmith components (eager imports with source_context)\n        # Decorators fire during import and auto-populate registry + index\n        _discovered_sources.add(SOURCE_BRAINSMITH)\n        with source_context(SOURCE_BRAINSMITH):\n            import brainsmith.kernels  # noqa: F401 - Registers built-in kernels\n            import brainsmith.steps  # noqa: F401 - Registers built-in steps\n\n        logger.debug(\"Loaded core brainsmith components\")\n\n        # 2. Entry point components (FINN, etc.)\n        _load_entry_point_components()\n\n        # 3. Active project components (SOURCE_PROJECT at project root)\n        _load_project_components(config)\n\n        # 4. Other filesystem-based component sources (custom sources)\n        _load_other_component_sources(config)\n\n        _components_discovered = True\n\n        # Link backends to their target kernels after all components indexed\n        _link_backends_to_kernels()\n\n        # Count components by type\n        counts = {\"step\": 0, \"kernel\": 0, \"backend\": 0}\n        for meta in _component_index.values():\n            counts[str(meta.component_type)] += 1\n\n        logger.info(\n            f\"Component discovery complete: \"\n            f\"{counts['step']} steps, \"\n            f\"{counts['kernel']} kernels, \"\n            f\"{counts['backend']} backends\"\n        )\n\n        # Save manifest for next time (only if caching is enabled)\n        if cache_enabled:\n            try:\n                manifest = _build_manifest_from_index()\n                _save_manifest(manifest, manifest_path)\n                logger.debug(f\"Regenerated and saved manifest cache to {manifest_path}\")\n            except Exception as e:\n                logger.warning(f\"Failed to save manifest cache: {e}\")\n        else:\n            logger.debug(\"Skipping manifest save (caching disabled)\")\n</code></pre>"},{"location":"api/registry/#brainsmith.registry.reset_registry","title":"reset_registry","text":"<pre><code>reset_registry() -&gt; None\n</code></pre> <p>Reset registry to uninitialized state.</p> <p>Clears all discovered components and resets discovery flag. Primarily used for testing.</p> Example <p>from brainsmith.registry import reset_registry reset_registry()</p> Source code in <code>brainsmith/registry/__init__.py</code> <pre><code>def reset_registry() -&gt; None:\n    \"\"\"Reset registry to uninitialized state.\n\n    Clears all discovered components and resets discovery flag.\n    Primarily used for testing.\n\n    Example:\n        &gt;&gt;&gt; from brainsmith.registry import reset_registry\n        &gt;&gt;&gt; reset_registry()\n        &gt;&gt;&gt; # ... change configuration ...\n        &gt;&gt;&gt; from brainsmith.registry import discover_components\n        &gt;&gt;&gt; discover_components()  # Re-discover with new config\n    \"\"\"\n    import brainsmith.registry._discovery as discovery_module\n    import brainsmith.registry._state as registry_state\n\n    registry_state._component_index.clear()\n    registry_state._components_discovered = False\n    discovery_module._components_discovered = False\n</code></pre>"},{"location":"api/registry/#brainsmith.registry.reset_registry--change-configuration","title":"... change configuration ...","text":"<p>from brainsmith.registry import discover_components discover_components()  # Re-discover with new config</p>"},{"location":"api/registry/#brainsmith.registry.is_initialized","title":"is_initialized","text":"<pre><code>is_initialized() -&gt; bool\n</code></pre> <p>Check if registry has been initialized.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if component discovery has completed</p> Example <p>from brainsmith.registry import is_initialized is_initialized() False discover_components() is_initialized() True</p> Source code in <code>brainsmith/registry/__init__.py</code> <pre><code>def is_initialized() -&gt; bool:\n    \"\"\"Check if registry has been initialized.\n\n    Returns:\n        True if component discovery has completed\n\n    Example:\n        &gt;&gt;&gt; from brainsmith.registry import is_initialized\n        &gt;&gt;&gt; is_initialized()\n        False\n        &gt;&gt;&gt; discover_components()\n        &gt;&gt;&gt; is_initialized()\n        True\n    \"\"\"\n    from ._state import _components_discovered\n\n    return _components_discovered\n</code></pre>"},{"location":"api/registry/#brainsmith.registry.get_kernel","title":"get_kernel","text":"<pre><code>get_kernel(name: str) -&gt; type\n</code></pre> <p>Get kernel class.</p> <p>Accepts both short names ('LayerNorm') and qualified names ('user:LayerNorm'). Short names are resolved using source priority from settings.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Kernel name - either short ('LayerNorm') or qualified ('user:LayerNorm')</p> required <p>Returns:</p> Type Description <code>type</code> <p>Kernel class</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If kernel not found in any source</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; LayerNorm = get_kernel('LayerNorm')  # Short name, uses source priority\n&gt;&gt;&gt; kernel = LayerNorm(onnx_node)\n&gt;&gt;&gt; CustomKernel = get_kernel('user:CustomKernel')  # Qualified name, explicit source\n</code></pre> Source code in <code>brainsmith/registry/_lookup.py</code> <pre><code>def get_kernel(name: str) -&gt; type:\n    \"\"\"Get kernel class.\n\n    Accepts both short names ('LayerNorm') and qualified names ('user:LayerNorm').\n    Short names are resolved using source priority from settings.\n\n    Args:\n        name: Kernel name - either short ('LayerNorm') or qualified ('user:LayerNorm')\n\n    Returns:\n        Kernel class\n\n    Raises:\n        KeyError: If kernel not found in any source\n\n    Examples:\n        &gt;&gt;&gt; LayerNorm = get_kernel('LayerNorm')  # Short name, uses source priority\n        &gt;&gt;&gt; kernel = LayerNorm(onnx_node)\n        &gt;&gt;&gt; CustomKernel = get_kernel('user:CustomKernel')  # Qualified name, explicit source\n    \"\"\"\n    return _get_component(name, \"kernel\")\n</code></pre>"},{"location":"api/registry/#brainsmith.registry.get_kernel_infer","title":"get_kernel_infer","text":"<pre><code>get_kernel_infer(name: str) -&gt; type\n</code></pre> <p>Get kernel's inference transform class.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Kernel name (short or qualified)</p> required <p>Returns:</p> Type Description <code>type</code> <p>InferTransform class for ONNX \u2192 kernel conversion</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If kernel not found or has no InferTransform</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; InferLayerNorm = get_kernel_infer('LayerNorm')\n&gt;&gt;&gt; model = model.transform(InferLayerNorm())\n</code></pre> Source code in <code>brainsmith/registry/_lookup.py</code> <pre><code>def get_kernel_infer(name: str) -&gt; type:\n    \"\"\"Get kernel's inference transform class.\n\n    Args:\n        name: Kernel name (short or qualified)\n\n    Returns:\n        InferTransform class for ONNX \u2192 kernel conversion\n\n    Raises:\n        KeyError: If kernel not found or has no InferTransform\n\n    Examples:\n        &gt;&gt;&gt; InferLayerNorm = get_kernel_infer('LayerNorm')\n        &gt;&gt;&gt; model = model.transform(InferLayerNorm())\n    \"\"\"\n    if not _components_discovered:\n        discover_components()\n\n    full_name = _resolve_component_name(name, \"kernel\")\n\n    # Lookup in unified component index\n    meta = _component_index.get(full_name)\n    if not meta:\n        available = list_kernels()\n        raise KeyError(_format_not_found_error(\"kernel\", full_name, available))\n\n    # Load component to ensure metadata is populated\n    _load_component(meta)\n\n    # Check metadata for infer transform\n    if meta.kernel_infer is None:\n        raise KeyError(\n            f\"Kernel '{full_name}' has no InferTransform.\\n\"\n            f\"\\nThis kernel does not have an associated shape inference transform.\\n\"\n            f\"Check that the kernel class defines 'infer_transform' attribute.\"\n        )\n\n    # Resolve lazy import specs and cache\n    infer = resolve_lazy_class(meta.kernel_infer)\n    if infer != meta.kernel_infer:\n        # Cache the resolved class\n        meta.kernel_infer = infer\n\n    return infer\n</code></pre>"},{"location":"api/registry/#brainsmith.registry.has_kernel","title":"has_kernel","text":"<pre><code>has_kernel(name: str) -&gt; bool\n</code></pre> <p>Check if kernel exists.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Kernel name (with or without source prefix)</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if kernel exists</p> Source code in <code>brainsmith/registry/_lookup.py</code> <pre><code>def has_kernel(name: str) -&gt; bool:\n    \"\"\"Check if kernel exists.\n\n    Args:\n        name: Kernel name (with or without source prefix)\n\n    Returns:\n        True if kernel exists\n    \"\"\"\n    return _has_component(name, \"kernel\")\n</code></pre>"},{"location":"api/registry/#brainsmith.registry.list_kernels","title":"list_kernels","text":"<pre><code>list_kernels(source: str | None = None) -&gt; list[str]\n</code></pre> <p>List all available kernels.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str | None</code> <p>Optional source filter (e.g., 'user', 'brainsmith')</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>Sorted list of kernel names (with source prefixes)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; kernels = list_kernels()  # All kernels\n&gt;&gt;&gt; user_kernels = list_kernels(source='user')  # Only user kernels\n</code></pre> Source code in <code>brainsmith/registry/_lookup.py</code> <pre><code>def list_kernels(source: str | None = None) -&gt; list[str]:\n    \"\"\"List all available kernels.\n\n    Args:\n        source: Optional source filter (e.g., 'user', 'brainsmith')\n\n    Returns:\n        Sorted list of kernel names (with source prefixes)\n\n    Examples:\n        &gt;&gt;&gt; kernels = list_kernels()  # All kernels\n        &gt;&gt;&gt; user_kernels = list_kernels(source='user')  # Only user kernels\n    \"\"\"\n    return _list_components(\"kernel\", source)\n</code></pre>"},{"location":"api/registry/#brainsmith.registry.get_backend","title":"get_backend","text":"<pre><code>get_backend(name: str) -&gt; type\n</code></pre> <p>Get backend class by name.</p> <p>Accepts both short names ('LayerNorm_HLS') and qualified names ('user:LayerNorm_HLS_Fast'). Short names are resolved using source priority from settings.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Backend name - either short ('LayerNorm_HLS') or qualified ('user:LayerNorm_HLS_Fast')</p> required <p>Returns:</p> Type Description <code>type</code> <p>Backend class</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If backend not found in any source</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; backend = get_backend('LayerNorm_HLS')  # Short name, uses source priority\n&gt;&gt;&gt; custom = get_backend('user:LayerNorm_HLS_Fast')  # Qualified name, explicit source\n</code></pre> Source code in <code>brainsmith/registry/_lookup.py</code> <pre><code>def get_backend(name: str) -&gt; type:\n    \"\"\"Get backend class by name.\n\n    Accepts both short names ('LayerNorm_HLS') and qualified names ('user:LayerNorm_HLS_Fast').\n    Short names are resolved using source priority from settings.\n\n    Args:\n        name: Backend name - either short ('LayerNorm_HLS') or qualified ('user:LayerNorm_HLS_Fast')\n\n    Returns:\n        Backend class\n\n    Raises:\n        KeyError: If backend not found in any source\n\n    Examples:\n        &gt;&gt;&gt; backend = get_backend('LayerNorm_HLS')  # Short name, uses source priority\n        &gt;&gt;&gt; custom = get_backend('user:LayerNorm_HLS_Fast')  # Qualified name, explicit source\n    \"\"\"\n    return _get_component(name, \"backend\")\n</code></pre>"},{"location":"api/registry/#brainsmith.registry.get_backend_metadata","title":"get_backend_metadata","text":"<pre><code>get_backend_metadata(name: str) -&gt; dict[str, Any]\n</code></pre> <p>Get backend metadata.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Backend name (with or without source prefix)</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Backend metadata dict with 'class', 'target_kernel', 'language' keys</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If backend not found</p> Source code in <code>brainsmith/registry/_lookup.py</code> <pre><code>def get_backend_metadata(name: str) -&gt; dict[str, Any]:\n    \"\"\"Get backend metadata.\n\n    Args:\n        name: Backend name (with or without source prefix)\n\n    Returns:\n        Backend metadata dict with 'class', 'target_kernel', 'language' keys\n\n    Raises:\n        KeyError: If backend not found\n    \"\"\"\n    if not _components_discovered:\n        discover_components()\n\n    full_name = _resolve_component_name(name, \"backend\")\n    logger.debug(f\"Looking up backend metadata: {name} \u2192 {full_name}\")\n\n    # Lookup in unified component index\n    meta = _component_index.get(full_name)\n    if not meta:\n        available = list_backends()\n        raise KeyError(_format_not_found_error(\"backend\", full_name, available))\n\n    # Load component to ensure metadata is populated\n    _load_component(meta)\n\n    # Build metadata dict from ComponentMetadata fields\n    return {\n        \"class\": meta.loaded_obj,\n        \"target_kernel\": meta.backend_target,\n        \"language\": meta.backend_language,\n    }\n</code></pre>"},{"location":"api/registry/#brainsmith.registry.list_backends","title":"list_backends","text":"<pre><code>list_backends(source: str | None = None) -&gt; list[str]\n</code></pre> <p>List all available backends.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str | None</code> <p>Optional source filter (e.g., 'user', 'brainsmith')</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>Sorted list of backend names (with source prefixes)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; backends = list_backends()  # All backends\n&gt;&gt;&gt; user_backends = list_backends(source='user')  # Only user backends\n</code></pre> Source code in <code>brainsmith/registry/_lookup.py</code> <pre><code>def list_backends(source: str | None = None) -&gt; list[str]:\n    \"\"\"List all available backends.\n\n    Args:\n        source: Optional source filter (e.g., 'user', 'brainsmith')\n\n    Returns:\n        Sorted list of backend names (with source prefixes)\n\n    Examples:\n        &gt;&gt;&gt; backends = list_backends()  # All backends\n        &gt;&gt;&gt; user_backends = list_backends(source='user')  # Only user backends\n    \"\"\"\n    return _list_components(\"backend\", source)\n</code></pre>"},{"location":"api/registry/#brainsmith.registry.list_backends_for_kernel","title":"list_backends_for_kernel","text":"<pre><code>list_backends_for_kernel(kernel: str, language: str | None = None, sources: list[str] | None = None) -&gt; list[str]\n</code></pre> <p>List backends that target a specific kernel.</p> <p>Backends are stored directly on kernel metadata during discovery, so this is a simple field access followed by optional filtering.</p> <p>Parameters:</p> Name Type Description Default <code>kernel</code> <code>str</code> <p>Kernel name (with or without source prefix)</p> required <code>language</code> <code>str | None</code> <p>Optional language filter ('hls' or 'rtl')</p> <code>None</code> <code>sources</code> <code>list[str] | None</code> <p>Optional list of sources to search (default: all sources)</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>Sorted list of backend names (with source prefixes)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # All backends for LayerNorm\n&gt;&gt;&gt; backends = list_backends_for_kernel('LayerNorm')\n&gt;&gt;&gt; print(backends)  # ['brainsmith:LayerNorm_hls', 'user:LayerNorm_rtl', ...]\n</code></pre> <pre><code>&gt;&gt;&gt; # Only HLS backends for LayerNorm\n&gt;&gt;&gt; hls_backends = list_backends_for_kernel('LayerNorm', language='hls')\n</code></pre> <pre><code>&gt;&gt;&gt; # Only user-provided backends\n&gt;&gt;&gt; user_backends = list_backends_for_kernel('LayerNorm', sources=['user'])\n</code></pre> Source code in <code>brainsmith/registry/_lookup.py</code> <pre><code>def list_backends_for_kernel(\n    kernel: str, language: str | None = None, sources: list[str] | None = None\n) -&gt; list[str]:\n    \"\"\"List backends that target a specific kernel.\n\n    Backends are stored directly on kernel metadata during discovery, so this\n    is a simple field access followed by optional filtering.\n\n    Args:\n        kernel: Kernel name (with or without source prefix)\n        language: Optional language filter ('hls' or 'rtl')\n        sources: Optional list of sources to search (default: all sources)\n\n    Returns:\n        Sorted list of backend names (with source prefixes)\n\n    Examples:\n        &gt;&gt;&gt; # All backends for LayerNorm\n        &gt;&gt;&gt; backends = list_backends_for_kernel('LayerNorm')\n        &gt;&gt;&gt; print(backends)  # ['brainsmith:LayerNorm_hls', 'user:LayerNorm_rtl', ...]\n\n        &gt;&gt;&gt; # Only HLS backends for LayerNorm\n        &gt;&gt;&gt; hls_backends = list_backends_for_kernel('LayerNorm', language='hls')\n\n        &gt;&gt;&gt; # Only user-provided backends\n        &gt;&gt;&gt; user_backends = list_backends_for_kernel('LayerNorm', sources=['user'])\n    \"\"\"\n    if not _components_discovered:\n        discover_components()\n\n    # Resolve kernel name to full source:name format\n    kernel_full = _resolve_component_name(kernel, \"kernel\")\n\n    # Get kernel metadata\n    kernel_meta = _component_index.get(kernel_full)\n    if not kernel_meta:\n        available = list_kernels()\n        raise KeyError(_format_not_found_error(\"kernel\", kernel_full, available))\n\n    # Get backends list from kernel metadata (populated during discovery)\n    candidate_backends = kernel_meta.kernel_backends or []\n\n    matching = []\n\n    # Filter candidates by language and sources (typically small set: 1-5 backends)\n    for backend_name in candidate_backends:\n        meta = _component_index[backend_name]\n\n        # Filter by sources if specified\n        if sources and meta.source not in sources:\n            continue\n\n        # Filter by language if specified\n        # Metadata already populated during discovery - no loading needed!\n        if language and meta.backend_language != language:\n            continue\n\n        matching.append(backend_name)\n\n    return sorted(matching)\n</code></pre>"},{"location":"api/registry/#brainsmith.registry.get_step","title":"get_step","text":"<pre><code>get_step(name: str)\n</code></pre> <p>Get step callable by name.</p> <p>Accepts both short names ('streamline') and qualified names ('user:custom_step'). Short names are resolved using source priority from settings.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Step name - either short ('streamline') or qualified ('user:custom_step')</p> required <p>Returns:</p> Type Description <p>Callable step function or Step instance</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If step not found</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; streamline = get_step('streamline')  # Short name, uses source priority\n&gt;&gt;&gt; custom = get_step('user:custom_step')  # Qualified name, explicit source\n</code></pre> Source code in <code>brainsmith/registry/_lookup.py</code> <pre><code>def get_step(name: str):\n    \"\"\"Get step callable by name.\n\n    Accepts both short names ('streamline') and qualified names ('user:custom_step').\n    Short names are resolved using source priority from settings.\n\n    Args:\n        name: Step name - either short ('streamline') or qualified ('user:custom_step')\n\n    Returns:\n        Callable step function or Step instance\n\n    Raises:\n        KeyError: If step not found\n\n    Examples:\n        &gt;&gt;&gt; streamline = get_step('streamline')  # Short name, uses source priority\n        &gt;&gt;&gt; custom = get_step('user:custom_step')  # Qualified name, explicit source\n    \"\"\"\n    return _get_component(name, \"step\")\n</code></pre>"},{"location":"api/registry/#brainsmith.registry.has_step","title":"has_step","text":"<pre><code>has_step(name: str) -&gt; bool\n</code></pre> <p>Check if step exists without importing it.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Step name (with or without source prefix)</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if step exists</p> Source code in <code>brainsmith/registry/_lookup.py</code> <pre><code>def has_step(name: str) -&gt; bool:\n    \"\"\"Check if step exists without importing it.\n\n    Args:\n        name: Step name (with or without source prefix)\n\n    Returns:\n        True if step exists\n    \"\"\"\n    return _has_component(name, \"step\")\n</code></pre>"},{"location":"api/registry/#brainsmith.registry.list_steps","title":"list_steps","text":"<pre><code>list_steps(source: str | None = None) -&gt; list[str]\n</code></pre> <p>List all available steps.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str | None</code> <p>Optional source filter (e.g., 'user', 'brainsmith')</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>Sorted list of step names (with source prefixes)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; steps = list_steps()  # All steps\n&gt;&gt;&gt; user_steps = list_steps(source='user')  # Only user steps\n</code></pre> Source code in <code>brainsmith/registry/_lookup.py</code> <pre><code>def list_steps(source: str | None = None) -&gt; list[str]:\n    \"\"\"List all available steps.\n\n    Args:\n        source: Optional source filter (e.g., 'user', 'brainsmith')\n\n    Returns:\n        Sorted list of step names (with source prefixes)\n\n    Examples:\n        &gt;&gt;&gt; steps = list_steps()  # All steps\n        &gt;&gt;&gt; user_steps = list_steps(source='user')  # Only user steps\n    \"\"\"\n    return _list_components(\"step\", source)\n</code></pre>"},{"location":"api/registry/#brainsmith.registry.get_component_metadata","title":"get_component_metadata","text":"<pre><code>get_component_metadata(name: str, component_type: str)\n</code></pre> <p>Get component metadata without loading.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Component name (short or qualified)</p> required <code>component_type</code> <code>str</code> <p>Component type ('step', 'kernel', 'backend')</p> required <p>Returns:</p> Type Description <p>ComponentMetadata with source, import spec, and type-specific fields</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If component not found</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; meta = get_component_metadata('LayerNorm', 'kernel')\n&gt;&gt;&gt; print(meta.source, meta.import_spec.module)\nbrainsmith brainsmith.kernels.layernorm\n</code></pre> Source code in <code>brainsmith/registry/_lookup.py</code> <pre><code>def get_component_metadata(name: str, component_type: str):\n    \"\"\"Get component metadata without loading.\n\n    Args:\n        name: Component name (short or qualified)\n        component_type: Component type ('step', 'kernel', 'backend')\n\n    Returns:\n        ComponentMetadata with source, import spec, and type-specific fields\n\n    Raises:\n        KeyError: If component not found\n\n    Examples:\n        &gt;&gt;&gt; meta = get_component_metadata('LayerNorm', 'kernel')\n        &gt;&gt;&gt; print(meta.source, meta.import_spec.module)\n        brainsmith brainsmith.kernels.layernorm\n    \"\"\"\n    if not _components_discovered:\n        discover_components()\n\n    full_name = _resolve_component_name(name, component_type)\n\n    if full_name not in _component_index:\n        available = _list_components(component_type)\n        raise KeyError(_format_not_found_error(component_type, full_name, available))\n\n    return _component_index[full_name]\n</code></pre>"},{"location":"api/registry/#brainsmith.registry.get_all_component_metadata","title":"get_all_component_metadata","text":"<pre><code>get_all_component_metadata() -&gt; dict[str, Any]\n</code></pre> <p>Get all component metadata.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict mapping full_name (source:name) to ComponentMetadata</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; all_components = get_all_component_metadata()\n&gt;&gt;&gt; for name, meta in all_components.items():\n...     print(f\"{name}: {meta.component_type}\")\n</code></pre> Source code in <code>brainsmith/registry/_lookup.py</code> <pre><code>def get_all_component_metadata() -&gt; dict[str, Any]:\n    \"\"\"Get all component metadata.\n\n    Returns:\n        Dict mapping full_name (source:name) to ComponentMetadata\n\n    Examples:\n        &gt;&gt;&gt; all_components = get_all_component_metadata()\n        &gt;&gt;&gt; for name, meta in all_components.items():\n        ...     print(f\"{name}: {meta.component_type}\")\n    \"\"\"\n    if not _components_discovered:\n        discover_components()\n\n    return dict(_component_index)\n</code></pre>"},{"location":"api/registry/#brainsmith.registry.get_domain_for_backend","title":"get_domain_for_backend","text":"<pre><code>get_domain_for_backend(backend_name: str) -&gt; str\n</code></pre> <p>Get ONNX domain for backend by deriving from module path.</p> <p>Derives domain directly from the backend class's module attribute, eliminating the need for reverse lookup through source_module_prefixes.</p> <p>Parameters:</p> Name Type Description Default <code>backend_name</code> <code>str</code> <p>Backend name (short or qualified)</p> required <p>Returns:</p> Type Description <code>str</code> <p>ONNX domain string</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; get_domain_for_backend('brainsmith:LayerNorm_hls')\n'brainsmith.kernels'\n&gt;&gt;&gt; get_domain_for_backend('finn:MVAU_hls')\n'finn.custom_op.fpgadataflow.hls'\n</code></pre> Source code in <code>brainsmith/registry/_lookup.py</code> <pre><code>def get_domain_for_backend(backend_name: str) -&gt; str:\n    \"\"\"Get ONNX domain for backend by deriving from module path.\n\n    Derives domain directly from the backend class's __module__ attribute,\n    eliminating the need for reverse lookup through source_module_prefixes.\n\n    Args:\n        backend_name: Backend name (short or qualified)\n\n    Returns:\n        ONNX domain string\n\n    Examples:\n        &gt;&gt;&gt; get_domain_for_backend('brainsmith:LayerNorm_hls')\n        'brainsmith.kernels'\n        &gt;&gt;&gt; get_domain_for_backend('finn:MVAU_hls')\n        'finn.custom_op.fpgadataflow.hls'\n    \"\"\"\n    from brainsmith.registry._domain_utils import derive_domain_from_module\n\n    # Get backend metadata and load the class\n    meta = get_component_metadata(backend_name, \"backend\")\n\n    # Load the backend class to access its __module__ attribute\n    backend_class = _load_component(meta)\n\n    # Derive domain directly from the backend class's module path\n    domain = derive_domain_from_module(backend_class.__module__)\n\n    logger.debug(\n        f\"Derived domain '{domain}' for backend '{backend_name}' from module '{backend_class.__module__}'\"\n    )\n    return domain\n</code></pre>"},{"location":"api/registry/#brainsmith.registry.ComponentMetadata","title":"ComponentMetadata  <code>dataclass</code>","text":"<pre><code>ComponentMetadata(name: str, source: str, component_type: ComponentType, import_spec: ImportSpec, loaded_obj: Any | None = None, kernel_infer: Any | None = None, kernel_backends: list[str] | None = None, is_infrastructure: bool = False, backend_target: str | None = None, backend_language: str | None = None)\n</code></pre> <p>Metadata for registered component.</p> <p>Contains all information needed to identify, load, and inspect a component.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Component name (without source prefix)</p> <code>source</code> <code>str</code> <p>Source identifier (e.g., 'brainsmith', 'finn', 'project')</p> <code>component_type</code> <code>ComponentType</code> <p>Component type (kernel, backend, or step)</p> <code>import_spec</code> <code>ImportSpec</code> <p>Import specification for lazy loading</p> <code>loaded_obj</code> <code>Any | None</code> <p>Loaded component instance (None if not yet loaded)</p> <code>kernel_infer</code> <code>Any | None</code> <p>InferTransform class (kernels only)</p> <code>kernel_backends</code> <code>list[str] | None</code> <p>List of backend names targeting this kernel (kernels only)</p> <code>is_infrastructure</code> <code>bool</code> <p>True for topology kernels like FIFO (kernels only)</p> <code>backend_target</code> <code>str | None</code> <p>Target kernel name (backends only)</p> <code>backend_language</code> <code>str | None</code> <p>Implementation language 'hls' or 'rtl' (backends only)</p>"},{"location":"api/registry/#brainsmith.registry.ComponentMetadata.full_name","title":"full_name  <code>property</code>","text":"<pre><code>full_name: str\n</code></pre> <p>Get source-prefixed name (e.g., 'brainsmith:LayerNorm').</p>"},{"location":"api/registry/#brainsmith.registry.ComponentType","title":"ComponentType","text":"<p>               Bases: <code>Enum</code></p> <p>Component type enumeration.</p> <p>Attributes:</p> Name Type Description <code>KERNEL</code> <p>Hardware kernel operation</p> <code>BACKEND</code> <p>HLS or RTL backend implementation</p> <code>STEP</code> <p>Pipeline transformation step</p>"},{"location":"api/registry/#brainsmith.registry.ComponentType.__str__","title":"__str__","text":"<pre><code>__str__() -&gt; str\n</code></pre> <p>String representation for display and serialization.</p> Source code in <code>brainsmith/registry/_metadata.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"String representation for display and serialization.\"\"\"\n    return self.name.lower()\n</code></pre>"},{"location":"api/registry/#brainsmith.registry.ComponentType.from_string","title":"from_string  <code>classmethod</code>","text":"<pre><code>from_string(s: str) -&gt; ComponentType\n</code></pre> <p>Parse component type from string.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>Component type string ('kernel', 'backend', or 'step')</p> required <p>Returns:</p> Type Description <code>ComponentType</code> <p>Corresponding ComponentType enum value</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If string doesn't match any component type</p> Example <p>ComponentType.from_string('kernel')  ComponentType.from_string('backend') </p> Source code in <code>brainsmith/registry/_metadata.py</code> <pre><code>@classmethod\ndef from_string(cls, s: str) -&gt; \"ComponentType\":\n    \"\"\"Parse component type from string.\n\n    Args:\n        s: Component type string ('kernel', 'backend', or 'step')\n\n    Returns:\n        Corresponding ComponentType enum value\n\n    Raises:\n        ValueError: If string doesn't match any component type\n\n    Example:\n        &gt;&gt;&gt; ComponentType.from_string('kernel')\n        &lt;ComponentType.KERNEL: 1&gt;\n        &gt;&gt;&gt; ComponentType.from_string('backend')\n        &lt;ComponentType.BACKEND: 2&gt;\n    \"\"\"\n    try:\n        return cls[s.upper()]\n    except KeyError:\n        valid = \", \".join([t.name.lower() for t in cls])\n        raise ValueError(f\"Invalid component type: '{s}'. \" f\"Must be one of: {valid}\")\n</code></pre>"},{"location":"api/registry/#brainsmith.registry.ImportSpec","title":"ImportSpec  <code>dataclass</code>","text":"<pre><code>ImportSpec(module: str, attr: str)\n</code></pre> <p>Import specification for lazy component loading.</p> <p>Attributes:</p> Name Type Description <code>module</code> <code>str</code> <p>Python module path (e.g., 'brainsmith.kernels.layernorm')</p> <code>attr</code> <code>str</code> <p>Attribute name in module (e.g., 'LayerNorm')</p>"},{"location":"api/registry/#see-also","title":"See Also","text":"<ul> <li>Getting Started - Installation and quickstart guide</li> <li>GitHub - Issues and questions</li> </ul>"},{"location":"api/settings/","title":"Settings","text":""},{"location":"api/settings/#settings","title":"Settings","text":"<p>Configuration management for Brainsmith projects with hierarchical loading and type-safe validation using Pydantic.</p> <p>Supports loading from CLI arguments, environment variables (BSMITH_* prefix), project config files (brainsmith.yaml), and built-in defaults.</p> <p>Example:</p> <pre><code>from brainsmith.settings import load_config\n\n# Load with default project config\nconfig = load_config()\n\n# Load with custom project file\nconfig = load_config(project_file=\"custom.yaml\")\n\n# Load with CLI overrides\nconfig = load_config(\n    build_dir=\"./custom-build\",\n    vivado_path=\"/tools/Xilinx/Vivado/2024.2\"\n)\n</code></pre> <p>Example:</p> <pre><code>from brainsmith.settings import get_config\n\n# Get cached config instance\nconfig = get_config()\n\n# Access configuration values\nprint(config.build_dir)\nprint(config.vivado_path)\nprint(config.logging.level)\n</code></pre> <p>Example:</p> <pre><code>from brainsmith.settings import get_default_config\n\n# Get default config without loading from files/env\ndefault_config = get_default_config()\n</code></pre> <p>Example:</p> <pre><code>from brainsmith.settings import SystemConfig\n\n# Create config with custom values\nconfig = SystemConfig(\n    build_dir=\"./build\",\n    vivado_path=\"/tools/Xilinx/Vivado/2024.2\"\n)\n\n# Access nested configuration\nprint(config.logging.level)\nprint(config.netron_port)\n</code></pre> <p>Example:</p> <pre><code>from brainsmith.settings import get_config, EnvironmentExporter\n\nconfig = get_config()\nexporter = EnvironmentExporter(config)\n\n# Export environment variables for external tools\nenv_dict = exporter.to_external_dict()\nprint(env_dict['FINN_ROOT'])\nprint(env_dict['VIVADO_PATH'])\n</code></pre>"},{"location":"api/settings/#brainsmith.settings.load_config","title":"load_config","text":"<pre><code>load_config(project_file: Path | None = None, **cli_overrides) -&gt; SystemConfig\n</code></pre> <p>Load configuration with hierarchical priority.</p> <p>Priority order (highest to lowest): 1. CLI arguments (passed as kwargs) 2. Environment variables (BSMITH_* prefix) 3. Project config file (brainsmith.yaml) 4. Built-in defaults</p> <p>Special handling: - BSMITH_LOG_LEVEL env var overrides logging.level (shorthand for BSMITH_LOGGING__LEVEL)</p> <p>Path Resolution: - Absolute paths: Used as-is - Relative CLI paths: Resolve to current working directory - Relative paths (YAML/env/defaults): Resolve to project directory</p> <p>Parameters:</p> Name Type Description Default <code>project_file</code> <code>Path | None</code> <p>Path to project config file (for non-standard locations)</p> <code>None</code> <code>**cli_overrides</code> <p>CLI argument overrides</p> <code>{}</code> <p>Returns:</p> Type Description <code>SystemConfig</code> <p>SystemConfig object</p> Source code in <code>brainsmith/settings/loader.py</code> <pre><code>def load_config(project_file: Path | None = None, **cli_overrides) -&gt; SystemConfig:\n    \"\"\"Load configuration with hierarchical priority.\n\n    Priority order (highest to lowest):\n    1. CLI arguments (passed as kwargs)\n    2. Environment variables (BSMITH_* prefix)\n    3. Project config file (brainsmith.yaml)\n    4. Built-in defaults\n\n    Special handling:\n    - BSMITH_LOG_LEVEL env var overrides logging.level (shorthand for BSMITH_LOGGING__LEVEL)\n\n    Path Resolution:\n    - Absolute paths: Used as-is\n    - Relative CLI paths: Resolve to current working directory\n    - Relative paths (YAML/env/defaults): Resolve to project directory\n\n    Args:\n        project_file: Path to project config file (for non-standard locations)\n        **cli_overrides: CLI argument overrides\n\n    Returns:\n        SystemConfig object\n    \"\"\"\n    try:\n        cli_overrides = _resolve_cli_paths(cli_overrides)\n\n        # Handle BSMITH_LOG_LEVEL shorthand (if not overridden by CLI)\n        if \"logging\" not in cli_overrides and \"BSMITH_LOG_LEVEL\" in os.environ:\n            log_level = os.environ[\"BSMITH_LOG_LEVEL\"]\n            cli_overrides[\"logging\"] = {\"level\": log_level}\n\n        if project_file:\n            cli_overrides[\"_project_file\"] = project_file\n\n        return SystemConfig(**cli_overrides)\n\n    except ValidationError as e:\n        console.print(\"[bold red]Configuration validation failed:[/bold red]\")\n        for error in e.errors():\n            field = \" \u2192 \".join(str(x) for x in error[\"loc\"])\n            console.print(f\"  [red]{field}: {error['msg']}[/red]\")\n        raise\n</code></pre>"},{"location":"api/settings/#brainsmith.settings.get_config","title":"get_config  <code>cached</code>","text":"<pre><code>get_config() -&gt; SystemConfig\n</code></pre> <p>Get cached configuration instance.</p> Environment must be sourced first <p>source .brainsmith/env.sh  # or: direnv allow</p> Source code in <code>brainsmith/settings/loader.py</code> <pre><code>@lru_cache(maxsize=1)\ndef get_config() -&gt; SystemConfig:\n    \"\"\"Get cached configuration instance.\n\n    Environment must be sourced first:\n        source .brainsmith/env.sh  # or: direnv allow\n    \"\"\"\n    config = load_config()\n    return config\n</code></pre>"},{"location":"api/settings/#brainsmith.settings.get_default_config","title":"get_default_config","text":"<pre><code>get_default_config() -&gt; SystemConfig\n</code></pre> <p>Get a configuration instance with only default values (no files or env vars).</p> Source code in <code>brainsmith/settings/loader.py</code> <pre><code>def get_default_config() -&gt; SystemConfig:\n    \"\"\"Get a configuration instance with only default values (no files or env vars).\"\"\"\n    filtered_env = {k: v for k, v in os.environ.items() if not k.startswith(\"BSMITH_\")}\n\n    with patch.dict(os.environ, filtered_env, clear=True):\n        # Prevent loading config files\n        from pathlib import Path\n\n        return load_config(project_file=Path(\"/dev/null\"))\n</code></pre>"},{"location":"api/settings/#brainsmith.settings.SystemConfig","title":"SystemConfig","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Configuration schema with hierarchical priority.</p> <p>Priority order (highest to lowest): 1. CLI arguments (passed to constructor) 2. Environment variables (BSMITH_* prefix) 3. Project config (brainsmith.yaml) 4. Built-in defaults</p>"},{"location":"api/settings/#brainsmith.settings.SystemConfig.bsmith_dir","title":"bsmith_dir  <code>cached</code> <code>property</code>","text":"<pre><code>bsmith_dir: Path\n</code></pre> <p>Brainsmith repository root containing pyproject.toml.</p> <p>This is the parent of the brainsmith package directory.</p>"},{"location":"api/settings/#brainsmith.settings.SystemConfig.generate_activation_script","title":"generate_activation_script","text":"<pre><code>generate_activation_script(output_path: Path) -&gt; Path\n</code></pre> <p>Generate bash activation script from current configuration.</p> <p>The script can be sourced multiple times safely: - Cleans up old Xilinx/brainsmith paths before adding new ones - Sources Xilinx settings64.sh files for complete tool environment</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>Path</code> <p>Where to write the activation script</p> required <p>Returns:</p> Type Description <code>Path</code> <p>Path to generated script</p> Example <p>config = get_config() config.generate_activation_script(Path(\"~/.brainsmith/env.sh\"))</p> Source code in <code>brainsmith/settings/schema.py</code> <pre><code>def generate_activation_script(self, output_path: Path) -&gt; Path:\n    \"\"\"Generate bash activation script from current configuration.\n\n    The script can be sourced multiple times safely:\n    - Cleans up old Xilinx/brainsmith paths before adding new ones\n    - Sources Xilinx settings64.sh files for complete tool environment\n\n    Args:\n        output_path: Where to write the activation script\n\n    Returns:\n        Path to generated script\n\n    Example:\n        &gt;&gt;&gt; config = get_config()\n        &gt;&gt;&gt; config.generate_activation_script(Path(\"~/.brainsmith/env.sh\"))\n        &gt;&gt;&gt; # User runs: source ~/.brainsmith/env.sh\n    \"\"\"\n    from .env_export import EnvironmentExporter\n\n    env_dict = EnvironmentExporter(self).to_env_dict()\n\n    script_lines = [\n        \"#!/bin/bash\",\n        \"# Auto-generated by brainsmith\",\n        \"# Source this file to set up environment:\",\n        \"#   source .brainsmith/env.sh\",\n        \"\",\n        \"# This script is idempotent - safe to source multiple times\",\n        \"\",\n        self._generate_cleanup_code(),\n        \"\",\n        \"# Export fresh environment variables\",\n    ]\n\n    for key, value in sorted(env_dict.items()):\n        # Skip internal markers\n        if key.startswith(\"_BRAINSMITH\") or key.startswith(\"_OLD_\"):\n            continue\n\n        # Skip PATH - we'll add Xilinx tool paths separately\n        if key == \"PATH\":\n            continue\n\n        # Properly escape quotes in values\n        escaped_value = str(value).replace('\"', '\\\\\"')\n        script_lines.append(f'export {key}=\"{escaped_value}\"')\n\n    # Add Xilinx tool paths to PATH\n    script_lines.extend(\n        [\n            \"\",\n            \"# Add Xilinx tools to PATH\",\n            'if [ -n \"$VIVADO_PATH\" ]; then',\n            '    export PATH=\"$VIVADO_PATH/bin:$PATH\"',\n            \"fi\",\n            \"\",\n            'if [ -n \"$VITIS_PATH\" ]; then',\n            '    export PATH=\"$VITIS_PATH/bin:$PATH\"',\n            \"fi\",\n            \"\",\n            'if [ -n \"$HLS_PATH\" ]; then',\n            '    export PATH=\"$HLS_PATH/bin:$PATH\"',\n            \"fi\",\n        ]\n    )\n\n    # Source Xilinx settings64.sh for complete environment\n    script_lines.extend(\n        [\n            \"\",\n            \"# Source Xilinx tool settings for full environment\",\n            \"# Vitis includes Vivado, so check it first\",\n            'if [ -n \"$VITIS_PATH\" ] &amp;&amp; [ -f \"$VITIS_PATH/settings64.sh\" ]; then',\n            '    source \"$VITIS_PATH/settings64.sh\" 2&gt;/dev/null',\n            'elif [ -n \"$VIVADO_PATH\" ] &amp;&amp; [ -f \"$VIVADO_PATH/settings64.sh\" ]; then',\n            '    source \"$VIVADO_PATH/settings64.sh\" 2&gt;/dev/null',\n            \"fi\",\n            \"\",\n            \"# Source HLS separately (not included in Vitis)\",\n            'if [ -n \"$HLS_PATH\" ] &amp;&amp; [ -f \"$HLS_PATH/settings64.sh\" ]; then',\n            '    source \"$HLS_PATH/settings64.sh\" 2&gt;/dev/null',\n            \"fi\",\n        ]\n    )\n\n    output_path = Path(output_path).expanduser()\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n    output_path.write_text(\"\\n\".join(script_lines))\n    output_path.chmod(0o755)\n\n    return output_path\n</code></pre>"},{"location":"api/settings/#brainsmith.settings.SystemConfig.generate_activation_script--user-runs-source-brainsmithenvsh","title":"User runs: source ~/.brainsmith/env.sh","text":""},{"location":"api/settings/#brainsmith.settings.SystemConfig.generate_direnv_file","title":"generate_direnv_file","text":"<pre><code>generate_direnv_file(output_path: Path) -&gt; Path\n</code></pre> <p>Generate .envrc file for direnv integration.</p> <p>Creates a direnv configuration that: - Watches brainsmith.yaml for changes - Auto-regenerates environment when config changes - Sources .brainsmith/env.sh for all environment variables - Activates virtualenv automatically</p> <p>User must run 'direnv allow' to trust the file.</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>Path</code> <p>Where to write the .envrc file (typically project root)</p> required <p>Returns:</p> Type Description <code>Path</code> <p>Path to generated .envrc file</p> Example <p>config = get_config() config.generate_direnv_file(Path(\".envrc\"))</p> Source code in <code>brainsmith/settings/schema.py</code> <pre><code>    def generate_direnv_file(self, output_path: Path) -&gt; Path:\n        \"\"\"Generate .envrc file for direnv integration.\n\n        Creates a direnv configuration that:\n        - Watches brainsmith.yaml for changes\n        - Auto-regenerates environment when config changes\n        - Sources .brainsmith/env.sh for all environment variables\n        - Activates virtualenv automatically\n\n        User must run 'direnv allow' to trust the file.\n\n        Args:\n            output_path: Where to write the .envrc file (typically project root)\n\n        Returns:\n            Path to generated .envrc file\n\n        Example:\n            &gt;&gt;&gt; config = get_config()\n            &gt;&gt;&gt; config.generate_direnv_file(Path(\".envrc\"))\n            &gt;&gt;&gt; # User runs: direnv allow\n        \"\"\"\n        output_path.parent / \".brainsmith\"\n\n        envrc_content = \"\"\"#!/usr/bin/env bash\n# Auto-generated by brainsmith\n# Enable with: direnv allow\n\n# Watch config file - direnv will reload when it changes\nwatch_file brainsmith.yaml\n\n# Activate virtualenv first (required for brainsmith command)\nif [ -d .venv ]; then\n    export VIRTUAL_ENV=\"$PWD/.venv\"\n    PATH_add \"$VIRTUAL_ENV/bin\"\nfi\n\n# Auto-regenerate environment if config is newer than env.sh\nif [ brainsmith.yaml -nt .brainsmith/env.sh ]; then\n    echo \"Config changed, regenerating environment...\"\n    if command -v brainsmith &amp;&gt; /dev/null; then\n        brainsmith project init &gt; /dev/null 2&gt;&amp;1 || {\n            echo -e \"\\033[33mFailed to regenerate. Run: brainsmith project init\\033[0m\"\n        }\n    else\n        echo -e \"\\033[33mbrainsmith command not found. Check venv activation.\\033[0m\"\n    fi\nfi\n\n# Source Brainsmith environment (sets all variables, sources Xilinx settings64.sh)\nsource_env .brainsmith/env.sh\n\"\"\"\n\n        output_path = Path(output_path).expanduser()\n        output_path.write_text(envrc_content)\n        output_path.chmod(0o644)  # Readable but not executable (direnv sources it)\n\n        return output_path\n</code></pre>"},{"location":"api/settings/#brainsmith.settings.SystemConfig.generate_direnv_file--user-runs-direnv-allow","title":"User runs: direnv allow","text":""},{"location":"api/settings/#brainsmith.settings.SystemConfig.model_post_init","title":"model_post_init","text":"<pre><code>model_post_init(__context: Any) -&gt; None\n</code></pre> <p>Resolve all paths to absolute.</p> <p>At this point: - CLI paths are already absolute (resolved to CWD in load_config) - YAML/env/default paths may be relative (need resolution to project_dir)</p> <p>Resolution steps: 1. Detect project_dir (where config file is, or CWD) 2. Resolve user-facing paths (relative \u2192 project_dir) 3. Force internal paths (deps_dir \u2192 bsmith_dir) 4. Set defaults for unset paths 5. Validate everything is sane 6. Check for deprecated configuration</p> Source code in <code>brainsmith/settings/schema.py</code> <pre><code>def model_post_init(self, __context: Any) -&gt; None:\n    \"\"\"Resolve all paths to absolute.\n\n    At this point:\n    - CLI paths are already absolute (resolved to CWD in load_config)\n    - YAML/env/default paths may be relative (need resolution to project_dir)\n\n    Resolution steps:\n    1. Detect project_dir (where config file is, or CWD)\n    2. Resolve user-facing paths (relative \u2192 project_dir)\n    3. Force internal paths (deps_dir \u2192 bsmith_dir)\n    4. Set defaults for unset paths\n    5. Validate everything is sane\n    6. Check for deprecated configuration\n    \"\"\"\n    self.project_dir = self._detect_project_root()\n    self._resolve_core_paths()\n    self._resolve_xilinx_tools()\n    self._resolve_finn_paths()\n    self._resolve_component_sources()\n    self._resolve_source_priority()\n    self._check_deprecations()\n</code></pre>"},{"location":"api/settings/#brainsmith.settings.SystemConfig.settings_customise_sources","title":"settings_customise_sources  <code>classmethod</code>","text":"<pre><code>settings_customise_sources(settings_cls: type[BaseSettings], init_settings: PydanticBaseSettingsSource, env_settings: PydanticBaseSettingsSource, dotenv_settings: PydanticBaseSettingsSource, file_secret_settings: PydanticBaseSettingsSource) -&gt; tuple[PydanticBaseSettingsSource, ...]\n</code></pre> <p>Customize settings sources.</p> <p>Priority order (first source wins): 1. Init settings (CLI/constructor args) - paths already resolved to CWD 2. Environment variables (BSMITH_*) - paths stay relative, resolved in model_post_init 3. YAML files (custom source) - paths stay relative, resolved in model_post_init 4. Field defaults (built into pydantic)</p> <p>Path Resolution: - CLI paths are resolved to CWD in load_config() before reaching here - All other paths stay relative and are resolved in model_post_init()</p> Source code in <code>brainsmith/settings/schema.py</code> <pre><code>@classmethod\ndef settings_customise_sources(\n    cls,\n    settings_cls: type[BaseSettings],\n    init_settings: PydanticBaseSettingsSource,\n    env_settings: PydanticBaseSettingsSource,\n    dotenv_settings: PydanticBaseSettingsSource,\n    file_secret_settings: PydanticBaseSettingsSource,\n) -&gt; tuple[PydanticBaseSettingsSource, ...]:\n    \"\"\"Customize settings sources.\n\n    Priority order (first source wins):\n    1. Init settings (CLI/constructor args) - paths already resolved to CWD\n    2. Environment variables (BSMITH_*) - paths stay relative, resolved in model_post_init\n    3. YAML files (custom source) - paths stay relative, resolved in model_post_init\n    4. Field defaults (built into pydantic)\n\n    Path Resolution:\n    - CLI paths are resolved to CWD in load_config() before reaching here\n    - All other paths stay relative and are resolved in model_post_init()\n    \"\"\"\n    # Extract file paths from init_settings if provided\n    init_dict = init_settings()\n    project_file = init_dict.get(\"_project_file\")\n\n    return (\n        init_settings,  # CLI args (already resolved to CWD in load_config)\n        env_settings,  # Env vars (stay relative, resolved in model_post_init)\n        YamlSettingsSource(settings_cls, project_file=project_file),\n    )\n</code></pre>"},{"location":"api/settings/#brainsmith.settings.SystemConfig.validate_component_sources","title":"validate_component_sources  <code>classmethod</code>","text":"<pre><code>validate_component_sources(v: Any) -&gt; dict[str, Path | None]\n</code></pre> <p>Validate component sources and warn about reserved source names.</p> <p>Reserved source names: - Core namespace ('brainsmith'): Internal components loaded via direct import - Entry points (e.g., 'finn'): Discovered via pip package entry points</p> <p>Users can configure filesystem-based sources (project, user, custom) but cannot override reserved names.</p> Source code in <code>brainsmith/settings/schema.py</code> <pre><code>@field_validator(\"component_sources\", mode=\"before\")\n@classmethod\ndef validate_component_sources(cls, v: Any) -&gt; dict[str, Path | None]:\n    \"\"\"Validate component sources and warn about reserved source names.\n\n    Reserved source names:\n    - Core namespace ('brainsmith'): Internal components loaded via direct import\n    - Entry points (e.g., 'finn'): Discovered via pip package entry points\n\n    Users can configure filesystem-based sources (project, user, custom) but\n    cannot override reserved names.\n    \"\"\"\n    import logging\n\n    logger = logging.getLogger(__name__)\n\n    default_sources = cls.model_fields[\"component_sources\"].default_factory()\n\n    if v is None or not isinstance(v, dict):\n        return default_sources\n\n    result = default_sources.copy()\n\n    for key, value in v.items():\n        # Warn if trying to override core namespace\n        if key == CORE_NAMESPACE and value is not None:\n            logger.warning(\n                f\"Component source '{key}' is a reserved core namespace and cannot be \"\n                f\"configured. The core brainsmith components are loaded from the package \"\n                f\"installation automatically. This configuration will be ignored.\"\n            )\n            continue  # Skip, don't add to result\n\n        # Warn if trying to override known entry point sources\n        if key in KNOWN_ENTRY_POINTS and value is not None:\n            logger.warning(\n                f\"Component source '{key}' is a registered entry point and cannot be \"\n                f\"configured. Entry point sources are discovered automatically from \"\n                f\"installed packages. This configuration will be ignored.\"\n            )\n            continue  # Skip, don't add to result\n\n        # Add custom or standard filesystem sources\n        result[key] = Path(value) if isinstance(value, str) else value\n\n    return result\n</code></pre>"},{"location":"api/settings/#brainsmith.settings.EnvironmentExporter","title":"EnvironmentExporter","text":"<pre><code>EnvironmentExporter(config: SystemConfig)\n</code></pre> <p>Export configuration as environment variables for shell scripts.</p> <p>Generates environment variable dictionaries for: - FINN (FINN_ROOT, FINN_BUILD_DIR, etc.) - Xilinx tools (VIVADO_PATH, XILINX_VIVADO, etc.) - Visualization tools (NETRON_PORT) - BSMITH_* variables (for YAML ${var} expansion in blueprints)</p> Example <p>config = SystemConfig() exporter = EnvironmentExporter(config) env_dict = exporter.to_external_dict() print(env_dict['FINN_ROOT'])</p> <p>Initialize with system configuration.</p> Source code in <code>brainsmith/settings/env_export.py</code> <pre><code>def __init__(self, config: \"SystemConfig\"):\n    \"\"\"Initialize with system configuration.\"\"\"\n    self.config = config\n</code></pre>"},{"location":"api/settings/#brainsmith.settings.EnvironmentExporter.to_all_dict","title":"to_all_dict","text":"<pre><code>to_all_dict() -&gt; dict[str, str]\n</code></pre> <p>Generate dict of all environment variables.</p> <p>Includes both external tool variables and internal BSMITH_* variables.</p> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Dict of all environment variables</p> Source code in <code>brainsmith/settings/env_export.py</code> <pre><code>def to_all_dict(self) -&gt; dict[str, str]:\n    \"\"\"Generate dict of all environment variables.\n\n    Includes both external tool variables and internal BSMITH_* variables.\n\n    Returns:\n        Dict of all environment variables\n    \"\"\"\n    env_dict = self.to_external_dict()\n\n    env_dict[\"BSMITH_BUILD_DIR\"] = str(self.config.build_dir)\n    env_dict[\"BSMITH_DEPS_DIR\"] = str(self.config.deps_dir)\n    env_dict[\"BSMITH_DIR\"] = str(self.config.bsmith_dir)\n    env_dict[\"BSMITH_PROJECT_DIR\"] = str(self.config.project_dir)\n\n    return env_dict\n</code></pre>"},{"location":"api/settings/#brainsmith.settings.EnvironmentExporter.to_env_dict","title":"to_env_dict","text":"<pre><code>to_env_dict(include_internal: bool = True) -&gt; dict[str, str]\n</code></pre> <p>Generate complete environment for shell script generation.</p> <p>Includes PATH, LD_LIBRARY_PATH, and all configuration variables. Used by generate_activation_script() and generate_direnv_file().</p> <p>Parameters:</p> Name Type Description Default <code>include_internal</code> <code>bool</code> <p>Include internal BSMITH_* variables (default: True)</p> <code>True</code> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Dict of environment variable names to string values</p> Source code in <code>brainsmith/settings/env_export.py</code> <pre><code>def to_env_dict(self, include_internal: bool = True) -&gt; dict[str, str]:\n    \"\"\"Generate complete environment for shell script generation.\n\n    Includes PATH, LD_LIBRARY_PATH, and all configuration variables.\n    Used by generate_activation_script() and generate_direnv_file().\n\n    Args:\n        include_internal: Include internal BSMITH_* variables (default: True)\n\n    Returns:\n        Dict of environment variable names to string values\n    \"\"\"\n    if include_internal:\n        env_dict = self.to_all_dict()\n    else:\n        env_dict = self.to_external_dict()\n\n    path_components = os.environ.get(\"PATH\", \"\").split(\":\")\n\n    new_paths = [\n        str(p) for p in self._collect_path_additions() if str(p) not in path_components\n    ]\n\n    env_dict[\"PATH\"] = \":\".join(path_components + new_paths)\n\n    # FINN XSI no longer requires PYTHONPATH manipulation\n    # The new finn.xsi module handles path management internally\n\n    ld_lib_components = os.environ.get(\"LD_LIBRARY_PATH\", \"\").split(\":\")\n\n    if self.config.vivado_path and Path(_LIBUDEV_PATH).exists():\n        env_dict[\"LD_PRELOAD\"] = _LIBUDEV_PATH\n\n    if self.config.vivado_path:\n        arch = platform.machine()\n        if arch != \"x86_64\":\n            raise RuntimeError(\n                f\"Brainsmith currently only supports x86_64 architecture.\\n\"\n                f\"Detected architecture: {arch}\\n\"\n                f\"Vivado integration has not been tested on this platform.\\n\"\n                f\"If you need ARM support, please open an issue.\"\n            )\n\n        # Add system library path (avoid duplicates)\n        libc_lib = \"/lib/x86_64-linux-gnu/\"\n        if libc_lib not in ld_lib_components:\n            ld_lib_components.append(libc_lib)\n\n        # Add Vivado library path (avoid duplicates)\n        vivado_lib = str(self.config.vivado_path / \"lib\" / \"lnx64.o\")\n        if vivado_lib not in ld_lib_components:\n            ld_lib_components.append(vivado_lib)\n\n    if self.config.vitis_path:\n        vitis_fpo_lib = str(self.config.vitis_path / \"lnx64\" / \"tools\" / \"fpo_v7_1\")\n        if vitis_fpo_lib not in ld_lib_components:\n            ld_lib_components.append(vitis_fpo_lib)\n\n    env_dict[\"LD_LIBRARY_PATH\"] = \":\".join(filter(None, ld_lib_components))\n\n    # The actual HOME override is handled at container level in entrypoint scripts\n    if self.config.vivado_path:\n        # Ensure XILINX_LOCAL_USER_DATA is set to prevent network operations\n        env_dict[\"XILINX_LOCAL_USER_DATA\"] = \"no\"\n\n    return env_dict\n</code></pre>"},{"location":"api/settings/#brainsmith.settings.EnvironmentExporter.to_external_dict","title":"to_external_dict","text":"<pre><code>to_external_dict() -&gt; dict[str, str]\n</code></pre> <p>Generate dict of external tool environment variables.</p> <p>Returns variables for external tools (FINN, Xilinx, etc.). Excludes internal BSMITH_* variables.</p> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Dict of environment variable names to string values</p> Source code in <code>brainsmith/settings/env_export.py</code> <pre><code>def to_external_dict(self) -&gt; dict[str, str]:\n    \"\"\"Generate dict of external tool environment variables.\n\n    Returns variables for external tools (FINN, Xilinx, etc.).\n    Excludes internal BSMITH_* variables.\n\n    Returns:\n        Dict of environment variable names to string values\n    \"\"\"\n    env = {}\n    cfg = self.config\n\n    # Xilinx tool paths (dual naming for FINN compatibility)\n    # Both XILINX_* and *_PATH variants are exported for maximum FINN compatibility.\n    # - XILINX_* variants: Used by FINN's Python runtime and internal scripts\n    # - *_PATH variants: Used by FINN's TCL scripts during Vivado/Vitis integration\n    if cfg.vivado_path:\n        vivado_str = str(cfg.vivado_path)\n        env[\"XILINX_VIVADO\"] = vivado_str\n        env[\"VIVADO_PATH\"] = vivado_str\n\n        if cfg.vivado_ip_cache:\n            env[\"VIVADO_IP_CACHE\"] = str(cfg.vivado_ip_cache)\n\n    if cfg.vitis_path:\n        vitis_str = str(cfg.vitis_path)\n        env[\"XILINX_VITIS\"] = vitis_str\n        env[\"VITIS_PATH\"] = vitis_str\n\n    if cfg.vitis_hls_path:\n        hls_str = str(cfg.vitis_hls_path)\n        env[\"XILINX_HLS\"] = hls_str\n        env[\"HLS_PATH\"] = hls_str\n\n    env[\"PLATFORM_REPO_PATHS\"] = cfg.vendor_platform_paths\n    env[\"OHMYXILINX\"] = str(cfg.deps_dir / \"oh-my-xilinx\")\n\n    env[\"NETRON_PORT\"] = str(cfg.netron_port)\n\n    if cfg.finn_root:\n        env[\"FINN_ROOT\"] = str(cfg.finn_root)\n    if cfg.finn_build_dir:\n        env[\"FINN_BUILD_DIR\"] = str(cfg.finn_build_dir)\n    if cfg.finn_deps_dir:\n        env[\"FINN_DEPS_DIR\"] = str(cfg.finn_deps_dir)\n\n    if cfg.default_workers:\n        env[\"NUM_DEFAULT_WORKERS\"] = str(cfg.default_workers)\n\n    return env\n</code></pre>"},{"location":"api/settings/#see-also","title":"See Also","text":"<ul> <li>Getting Started - Installation and project setup</li> <li>GitHub - Issues and questions</li> </ul>"},{"location":"developer-guide/","title":"Overview","text":""},{"location":"developer-guide/#developer-guide","title":"Developer Guide","text":"<p>Technical documentation for extending Brainsmith and understanding its architecture.</p> <p>Hardware Kernels - What kernels are, design principles, layer-level granularity, and complete kernel examples.</p> <p>Component Registry - Plugin system for registering custom kernels, backends, and pipeline steps.</p> <p>Blueprint Schema - Complete YAML schema for design space configuration files.</p> <p>Dataflow Modeling - Theoretical foundations: TENSOR/BLOCK/STREAM hierarchy, inter-kernel composition, streaming interfaces.</p> <p>Multi-Layer Offload - Using weight streaming to implement large models (experimental).</p>"},{"location":"developer-guide/blueprint-schema/","title":"Blueprint Schema","text":""},{"location":"developer-guide/blueprint-schema/#blueprint-schema-reference","title":"Blueprint Schema Reference","text":"<p>Blueprints are YAML files defining the design space for FPGA accelerator generation.</p>"},{"location":"developer-guide/blueprint-schema/#quick-reference","title":"Quick Reference","text":"Field Type Required Description <code>board</code> string Yes Target FPGA board <code>clock_ns</code> float Yes Target clock period (nanoseconds) <code>design_space.kernels</code> list Yes Hardware kernels to use <code>design_space.steps</code> list Yes Transformation pipeline <code>description</code> string No Blueprint description <code>extends</code> path No Parent blueprint for inheritance <code>finn_config</code> dict No FINN parameter overrides <code>name</code> string No Blueprint name <code>output</code> string No Output type: <code>estimates</code> | <code>rtl</code> | <code>bitfile</code> (default: <code>estimates</code>) <code>start_step</code> string No Pipeline start step (inclusive) <code>stop_step</code> string No Pipeline stop step (inclusive)"},{"location":"developer-guide/blueprint-schema/#minimal-blueprint","title":"Minimal Blueprint","text":"<pre><code>name: \"My Accelerator\"\ndescription: \"Minimal blueprint for resource estimates\"\nboard: \"Pynq-Z1\"\nclock_ns: 5.0\noutput: \"estimates\"  # Default: generates estimates only\n\ndesign_space:\n  kernels:\n    - MVAU\n    - Thresholding\n\n  steps:\n    - \"qonnx_to_finn\"\n    - \"build_dataflow_graph\"    # Infers and builds kernel graph\n    - \"build_hw_graph\"           # Partitions and specializes backends\n    - \"generate_estimate_reports\"\n</code></pre> <p>Note: This minimal blueprint generates resource estimates only. For RTL or bitfile generation, change <code>output</code> to <code>\"rtl\"</code> or <code>\"bitfile\"</code> (see Core Configuration).</p> <p>See Also: - <code>examples/blueprints/base.yaml</code> - Baseline FINN pipeline (estimates only) - <code>examples/blueprints/bert.yaml</code> - Complete BERT blueprint (bitfile generation)</p>"},{"location":"developer-guide/blueprint-schema/#execution-semantics","title":"Execution Semantics","text":"<p>Brainsmith builds an execution tree where:</p> <ul> <li>Nodes = execution segments (sequential steps)</li> <li>Branches = variation points (lists)</li> <li>Leaves = complete execution paths</li> </ul> <p>Segment-based execution: Steps between branch points form single segments. Each segment executes as one FINN build. Artifacts are shared at branch points to avoid redundant computation.</p> <p>Example tree: </p><pre><code>steps:\n  - \"tidy_up\"\n  - [\"streamline\", \"streamline_aggressive\"]\n  - \"convert_to_hw\"\n</code></pre><p></p> <p>Creates 2 paths:</p> <ol> <li><code>tidy_up \u2192 streamline \u2192 convert_to_hw</code></li> <li><code>tidy_up \u2192 streamline_aggressive \u2192 convert_to_hw</code></li> </ol> <p>Both paths share the <code>tidy_up</code> segment.</p>"},{"location":"developer-guide/blueprint-schema/#core-configuration","title":"Core Configuration","text":""},{"location":"developer-guide/blueprint-schema/#clock_ns-required","title":"clock_ns (required)","text":"<p>Target clock period in nanoseconds. Determines timing constraints for synthesis.</p> <pre><code>clock_ns: 5.0    # 200MHz (1/5ns)\nclock_ns: 4.0    # 250MHz (1/4ns)\n</code></pre>"},{"location":"developer-guide/blueprint-schema/#output","title":"output","text":"<p>How far to proceed in the build pipeline:</p> <pre><code>output: \"estimates\"    # Resource estimates only (default)\noutput: \"rtl\"          # Generate RTL + IP blocks\noutput: \"bitfile\"      # Full synthesis to bitstream\n</code></pre>"},{"location":"developer-guide/blueprint-schema/#board","title":"board","text":"<p>Target FPGA board. Required when <code>output</code> is <code>rtl</code> or <code>bitfile</code>.</p> <pre><code>board: \"Pynq-Z1\"\nboard: \"ZCU104\"\nboard: \"V80\"\n</code></pre>"},{"location":"developer-guide/blueprint-schema/#start_step-stop_step","title":"start_step / stop_step","text":"<p>Control execution range for debugging or incremental builds:</p> <pre><code>start_step: \"streamline\"           # Start from this step (inclusive)\nstop_step: \"generate_estimates\"    # Stop at this step (inclusive)\n</code></pre> <p>CLI overrides (take precedence): </p><pre><code>smith model.onnx blueprint.yaml --start-step streamline --stop-step streamline\n</code></pre><p></p>"},{"location":"developer-guide/blueprint-schema/#finn_config","title":"finn_config","text":"<p>Direct FINN parameter overrides (deep-merged during inheritance):</p> <pre><code>finn_config:\n  target_fps: 3000\n  standalone_thresholds: true\n  rtlsim_batch_size: 100\n</code></pre>"},{"location":"developer-guide/blueprint-schema/#kernels","title":"Kernels","text":"<p>Define hardware implementations available for layer mapping.</p> <p>All backends (auto-sorted: RTL \u2192 HLS): </p><pre><code>kernels:\n  - MVAU\n  - LayerNorm\n</code></pre><p></p> <p>Specific backends (explicit priority order): </p><pre><code>kernels:\n  - MVAU: [MVAU_hls, MVAU_rtl]              # HLS first\n  - Softmax: Softmax_hls                     # Single backend\n  - Crop: [brainsmith:Crop_rtl]              # Fully-qualified name\n</code></pre><p></p> <p>Backend resolution:</p> <ul> <li>String format \u2192 all registered backends, sorted by priority</li> <li>Dict format \u2192 only specified backends, in given order</li> <li>Supports short names (<code>MVAU_hls</code>) and qualified names (<code>brainsmith:MVAU_hls</code>)</li> </ul>"},{"location":"developer-guide/blueprint-schema/#steps","title":"Steps","text":"<p>Transformation pipeline with support for variations and optional steps.</p> <p>Linear pipeline: </p><pre><code>steps:\n  - \"qonnx_to_finn\"\n  - \"build_dataflow_graph\"\n  - \"build_hw_graph\"\n</code></pre><p></p> <p>Branch points (design space exploration): </p><pre><code>steps:\n  - \"tidy_up\"\n  - [\"streamline\", \"streamline_aggressive\"]    # Try both\n  - \"convert_to_hw\"\n  - [\"minimize_bit_width\", ~]                  # Optional step\n</code></pre><p></p> <p>The second example creates 4 execution paths (2 \u00d7 2 combinations).</p> <ol> <li>tidy_up \u2192 streamline \u2192 convert_to_hw \u2192 minimize_bit_width</li> <li>tidy_up \u2192 streamline \u2192 convert_to_hw (skip minimize_bit_width)</li> <li>tidy_up \u2192 streamline_aggressive \u2192 convert_to_hw \u2192 minimize_bit_width</li> <li>tidy_up \u2192 streamline_aggressive \u2192 convert_to_hw (skip minimize_bit_width)</li> </ol> <p>Skip indicators: <code>~</code>, <code>null</code>, <code>\"\"</code> (all equivalent)</p> <p>Constraints:</p> <ul> <li>Maximum 1 skip per branch point</li> <li>No nested lists in branch points (use double brackets <code>[[...]]</code> for operations)</li> </ul>"},{"location":"developer-guide/blueprint-schema/#inheritance","title":"Inheritance","text":"<p>Reuse and extend existing blueprints via <code>extends</code>:</p> <pre><code># parent.yaml\nname: \"Base Pipeline\"\nclock_ns: 5.0\n\ndesign_space:\n  kernels:\n    - MVAU\n  steps:\n    - \"qonnx_to_finn\"\n    - \"streamline\"\n</code></pre> <pre><code># child.yaml\nextends: \"parent.yaml\"\nname: \"Extended Pipeline\"\noutput: \"rtl\"\nboard: \"Pynq-Z1\"\n\ndesign_space:\n  kernels:\n    - MVAU\n    - Thresholding    # Replaces parent kernels entirely\n\n  steps:\n    - after: \"streamline\"\n      insert: \"custom_step\"\n</code></pre> <p>Inheritance rules:</p> <ol> <li>Simple fields (name, clock_ns, etc.) \u2192 Child overrides parent</li> <li><code>finn_config</code> \u2192 Deep merge (child fields override parent fields)</li> <li><code>kernels</code> \u2192 Child replaces parent entirely (or inherits if not specified)</li> <li><code>steps</code> \u2192 Child replaces parent entirely (or inherits if not specified)</li> <li>Step operations (<code>after</code>, <code>before</code>, etc.) \u2192 Applied after determining base steps</li> </ol>"},{"location":"developer-guide/blueprint-schema/#step-operations","title":"Step Operations","text":"<p>Modify inherited or complex step lists:</p> <p>Insert after/before: </p><pre><code>steps:\n  - after: \"streamline\"\n    insert: \"custom_optimization\"\n\n  - before: \"build_hw_graph\"\n    insert:\n      - \"validation_step\"\n      - [\"option1\", \"option2\"]    # Insert branch point\n</code></pre><p></p> <p>Replace/remove: </p><pre><code>steps:\n  - replace: \"old_step\"\n    with: \"new_step\"\n\n  - replace: \"branch_step\"\n    with: [[\"new_option1\", \"new_option2\"]]    # Replace with branch\n\n  - remove: \"unwanted_step\"\n</code></pre><p></p> <p>Insert at start/end: </p><pre><code>steps:\n  - at_start:\n      insert: \"initialization\"\n\n  - at_end:\n      insert: [\"package_ip\", \"validate\"]\n</code></pre><p></p>"},{"location":"developer-guide/blueprint-schema/#environment-variables","title":"Environment Variables","text":"<p>Use <code>${VAR}</code> syntax for dynamic path resolution:</p> <pre><code>extends: \"${BSMITH_DIR}/examples/blueprints/base.yaml\"\nboard: \"${TARGET_BOARD}\"\n</code></pre> <p>Available variables:</p> <ul> <li><code>${BLUEPRINT_DIR}</code> - Directory containing current blueprint</li> <li><code>${BSMITH_DIR}</code> - Brainsmith installation directory</li> <li>Any shell environment variable</li> </ul> <p>Notes:</p> <ul> <li>Context variables override environment variables</li> <li>Undefined variables remain unexpanded (safe substitution)</li> <li>Thread-safe (no <code>os.environ</code> mutation)</li> </ul>"},{"location":"developer-guide/blueprint-schema/#design-space-size","title":"Design Space Size","text":"<p>Design space size = product of all branch point sizes.</p> <p>Limits:</p> <ul> <li>Default: 100,000 combinations</li> <li>Environment override: <code>export BRAINSMITH_MAX_COMBINATIONS=500000</code></li> <li>Validation: Exceeding limit raises <code>ValueError</code> before execution</li> </ul> <p>Example: </p><pre><code>steps:\n  - [\"opt1\", \"opt2\"]           # 2 options\n  - [\"opt3\", \"opt4\", \"opt5\"]   # 3 options\n  - [\"opt6\", ~]                # 2 options (with skip)\n# Total: 2 \u00d7 3 \u00d7 2 = 12 combinations\n</code></pre><p></p>"},{"location":"developer-guide/dataflow-modeling/","title":"Dataflow Modeling","text":""},{"location":"developer-guide/dataflow-modeling/#dataflow-modeling","title":"Dataflow Modeling","text":"<p>How Brainsmith systematically bridges the semantic gap between ONNX's model-level abstractions and RTL's cycle-accurate streaming implementation.</p>"},{"location":"developer-guide/dataflow-modeling/#introduction","title":"Introduction","text":"<p>ONNX describes neural network computation in terms of complete tensors\u2014a LayerNorm operates on an entire <code>(1, 224, 224, 64)</code> activation map. Hardware executes on bit-streams cycling through circuits\u201416 INT8 elements processed per clock cycle, streamed over hundreds of cycles.</p> <p>This semantic gap must be bridged systematically. Without a formal model linking tensor operations to streaming execution, you cannot:</p> <ul> <li>Automatically estimate performance - Calculate cycle counts, throughput, and latency from ONNX graph structure</li> <li>Generate infrastructure - Derive FIFO depths, width converters, and buffering requirements from tensor shapes</li> <li>Enable design space exploration - Determine valid parallelization ranges and resource tradeoffs without manual analysis</li> <li>Compose heterogeneous kernels - Connect elementwise, reduction, and attention operations with different processing patterns</li> </ul>"},{"location":"developer-guide/dataflow-modeling/#data-hierarchy-tensorblockstream","title":"Data Hierarchy: TENSOR/BLOCK/STREAM","text":""},{"location":"developer-guide/dataflow-modeling/#three-tiers-of-refinement","title":"Three Tiers of Refinement","text":"<p>Brainsmith bridges this gap through a three-tier data hierarchy: TENSOR \u2192 BLOCK \u2192 STREAM. Each tier refines how data is represented and processed:</p> <p>TENSOR - Complete dimensions from ONNX graph (e.g., <code>(1, 224, 224, 64)</code>)</p> <ul> <li>Defines functional correctness and accuracy requirements</li> <li>Extracted from ModelWrapper's tensor shape information</li> <li>Represents the semantic unit of computation from the model</li> </ul> <p>BLOCK - Kernel's atomic computation unit (e.g., <code>(1, 7, 7, 64)</code>)</p> <ul> <li>Data required for one cycle of the kernel's computation state</li> <li>Controls memory footprint, pipeline depth, and latency</li> <li>Defined by <code>block_tiling</code> template in schema</li> <li>Determines valid parallelization ranges (STREAM cannot exceed BLOCK)</li> </ul> <p>STREAM - Elements processed per clock cycle (e.g., <code>16</code> 8-bit Integers)</p> <ul> <li>Determines throughput and resource usage</li> <li>Defined by <code>stream_tiling</code> template in schema</li> <li>Constrained by BLOCK shape (cannot exceed block dimensions)</li> <li>Resolved during DSE configuration (SIMD, PE parameters)</li> </ul>"},{"location":"developer-guide/dataflow-modeling/#kernel-specific-block-semantics","title":"Kernel-Specific Block Semantics","text":"<p>The BLOCK abstraction level is necessary because some kernels cannot arbitrarily increase parallelism up to the full TENSOR shape. Consider the following examples, and how the BLOCK shape adapts to kernel computation characteristics:</p> <p>Simple kernels (elementwise operations like Add, Multiply):</p> <ul> <li>BLOCK = TENSOR (entire input processed as one unit)</li> <li>No intermediate computation state between elements</li> <li>STREAM parallelization limited only by resource availability</li> <li>Example: Add with input <code>(1, 224, 224, 64)</code> \u2192 BLOCK <code>(1, 224, 224, 64)</code>, STREAM <code>(1, 1, 1, 16)</code></li> </ul> <p>Complex kernels (reduction operations like MatMul, LayerNorm):</p> <ul> <li>BLOCK = one quantum of the calculation state</li> <li>Example: Dot product BLOCK = one input vector</li> <li>Kernel must accumulate/reduce across BLOCK before producing output</li> <li>STREAM parallelization cannot exceed BLOCK dimensions</li> <li>Example: LayerNorm with input <code>(1, 224, 224, 64)</code> \u2192 BLOCK <code>(1, 1, 1, 64)</code>, STREAM <code>(1, 1, 1, 16)</code></li> </ul>"},{"location":"developer-guide/dataflow-modeling/#lowering-from-onnx-to-rtl","title":"Lowering from ONNX to RTL","text":"<p>The streaming data hierarchy enables automatic derivation of hardware execution characteristics:</p> <pre><code># Spatial decomposition\ntensor_blocks = ceil(tensor_dim / block_dim)\n\n# Temporal execution\nstream_cycles = ceil(block_dim / stream_dim)\n\n# Total latency\ntotal_cycles = prod(tensor_blocks) \u00d7 prod(stream_cycles)\n</code></pre> <p>Example: For tensor <code>(100, 64)</code>, block <code>(32, 16)</code>, stream <code>(8, 4)</code>:</p> <ul> <li>Tensor blocks: <code>(4, 4)</code> \u2192 16 blocks cover the full tensor</li> <li>Stream cycles: <code>(4, 4)</code> \u2192 16 cycles stream each block</li> <li>Total cycles: 256 (16 blocks \u00d7 16 cycles per block)</li> </ul> <p>This calculation forms the basis for: - Performance estimation - Cycle counts and throughput - Resource estimation - Memory requirements from block sizes - DSE constraints - Valid parallelization ranges</p>"},{"location":"developer-guide/dataflow-modeling/#inter-kernel-dataflow","title":"Inter-Kernel Dataflow","text":"<p>Kernels communicate via streaming interfaces, producing and consuming data cycle-by-cycle. Elastic FIFOs between kernels accumulate these streams as data blocks for buffering, then stream them out to downstream consumers. This infrastructure automatically adapts to different kernel semantics through shape-driven buffering.</p>"},{"location":"developer-guide/dataflow-modeling/#composing-kernels-with-different-block-semantics","title":"Composing Kernels with Different Block Semantics","text":"<p>Consider a simple pipeline: <code>Add \u2192 LayerNorm \u2192 Softmax</code></p> <pre><code>Add (elementwise)           LayerNorm (reduction)       Softmax (reduction)\nBLOCK = TENSOR (1,224,64)   BLOCK = (1,1,64)           BLOCK = (1,1,N_classes)\nSTREAM = (1,1,16)           STREAM = (1,1,16)          STREAM = (1,1,8)\n</code></pre> <p>What happens at kernel boundaries:</p> <ol> <li> <p>Add \u2192 LayerNorm: Producer outputs (1,224,64) blocks, consumer expects (1,1,64) blocks</p> </li> <li> <p>FIFO buffers shape transformation</p> </li> <li>Add streams 14 blocks \u00d7 16 cycles each = 224 cycles</li> <li> <p>LayerNorm consumes in (1,1,64) chunks, computing normalization per spatial position</p> </li> <li> <p>LayerNorm \u2192 Softmax: Block shapes may differ based on computation semantics</p> </li> <li> <p>Each kernel's BLOCK reflects its reduction domain</p> </li> <li>FIFOs provide elastic buffering for rate adaptation</li> </ol>"},{"location":"developer-guide/dataflow-modeling/#automatic-infrastructure-derivation","title":"Automatic Infrastructure Derivation","text":"<p>Block and stream shapes drive hardware generation:</p> <p>FIFO Depths - Determined by producer/consumer rate mismatch: </p><pre><code>producer_rate = prod(block_shape) / prod(stream_shape)  # cycles per block\nconsumer_rate = # depends on consumer's internal pipeline depth\nfifo_depth = max(producer_burst, consumer_backpressure_tolerance)\n</code></pre><p></p> <p>Implementation Reality</p> <p>The formula above describes the theoretical basis for FIFO sizing. In practice, the current implementation uses iterative cycle-accurate RTL simulation to converge on efficient FIFO depths. This will be expanded on in future releases.</p> <p>Stream Width Matching - Interface widths must align:</p> <ul> <li>Producer STREAM=(1,1,16) \u00d7 INT8 \u2192 128-bit AXI-Stream</li> <li>Consumer expects matching width or automatic width conversion</li> <li>Datatype changes (INT8 \u2192 FLOAT32) insert conversion logic</li> </ul> <p>Rate Mismatch Handling - Kernels may have different throughputs:</p> <ul> <li>Elementwise: 1 output per cycle (after initial latency)</li> <li>Reduction: Multiple cycles per output (accumulation phase)</li> <li>FIFOs absorb transient rate differences, prevent pipeline stalls</li> </ul>"},{"location":"developer-guide/dataflow-modeling/#schema-driven-interface-resolution","title":"Schema-Driven Interface Resolution","text":"<p>Schemas declare interfaces using templates that adapt to runtime shapes:</p> <pre><code># Producer (Add kernel)\nOutputSchema(\n    block_tiling=[FULL_DIM, FULL_DIM, FULL_DIM],  # Process entire spatial dims\n    stream_tiling=[1, 1, \"PE\"]                      # Parallelize over channels\n)\n\n# Consumer (LayerNorm kernel)\nInputSchema(\n    block_tiling=[FULL_DIM, FULL_DIM, FULL_DIM],  # Same spatial processing\n    stream_tiling=[1, 1, \"SIMD\"]                    # Match channel parallelism\n)\n</code></pre> <p>At compile time:</p> <ol> <li>ONNX tensor shapes resolve <code>FULL_DIM</code> \u2192 actual dimensions</li> <li>DSE parameters (PE=16, SIMD=16) resolve stream tiling</li> <li>Infrastructure generates FIFOs matching computed shapes</li> <li>Validation ensures producer/consumer compatibility</li> </ol> <p>This declarative approach separates what (tensor semantics) from how (hardware implementation), enabling design space exploration while maintaining correctness by construction. The compiler automatically inserts width converters, reshaping logic, and elastic buffering as needed.</p>"},{"location":"developer-guide/dataflow-modeling/#design-space-implications","title":"Design Space Implications","text":"<p>The dataflow modeling hierarchy directly constrains design space exploration:</p> <p>Valid Parallelization Ranges:</p> <ul> <li>STREAM dimensions must be divisors of BLOCK dimensions</li> <li>For BLOCK=(64,), valid STREAM values are {1, 2, 4, 8, 16, 32, 64}</li> <li>Invalid: STREAM=128 (exceeds block size)</li> </ul> <p>Performance vs Resource Tradeoffs:</p> <ul> <li>Increasing STREAM \u2192 Higher throughput, more DSP/LUT usage</li> <li>Increasing BLOCK \u2192 More on-chip memory, potentially higher latency</li> <li>Heterogeneous pipelines: Balance per-kernel parallelization against total resource budget</li> </ul> <p>Configuration Constraints:</p> <ul> <li>Producer/consumer STREAM widths must match (or have automatic conversion)</li> <li>Total pipeline throughput limited by slowest kernel</li> <li>FIFO depths must accommodate worst-case rate mismatches</li> </ul>"},{"location":"developer-guide/hardware-kernels/","title":"Hardware Kernels","text":""},{"location":"developer-guide/hardware-kernels/#hardware-kernels","title":"Hardware Kernels","text":"<p>Hardware kernels are layer-level operations (LayerNorm, MatMul, Attention) implemented as self-contained streaming circuits for FPGAs.</p> <p></p> <p>Brainsmith constructs dataflow accelerators by iteratively applying graph transformations to lower ONNX nodes to matching kernels, connected via streaming interfaces. During this process, kernels are modeled by the relationship between their input and output streams with the internal architecture largely abstracted away.</p>"},{"location":"developer-guide/hardware-kernels/#designed-for-agility","title":"Designed for Agility","text":"<p>The kernel system is the foundation of Brainsmith's agility, enabling key architectural capabilities:</p> <p>1. Schema-Driven Automation - All integration code, constraint validation, and DSE parameters are auto-derived from the kernel's schema, allowing hardware engineers to create efficient kernels without onerous compiler integration work.</p> <p>2. Standardized DSE Interface - Structural and optimization design parameters construct a kernel design space with unified functions for efficient exploration and testing.</p> <p>3. Automated Testing Framework - Validate parity against target ONNX operators at various levels of abstraction with a simple testbench.</p> <p>See Dataflow Modeling for theoretical foundations.</p>"},{"location":"developer-guide/hardware-kernels/#layer-level-granularity","title":"Layer-Level Granularity","text":"<p>Unlike MLIR-based or fully HLS toolchains that fully decompose the model into primitive operations, Brainsmith maintains layer-level design abstraction throughout the compiler. This has several key advantages that further serve the goals of agility and separation of concerns:</p> <ol> <li> <p>Preserve Design Space - Neural networks are expressed as layers in PyTorch/ONNX. Preserving this granularity enables natural extension from AI frameworks while maintaining semantic meaning through the compilation pipeline.</p> </li> <li> <p>Prevent Exponential Explosion - Decomposing layers into individual operations (add, multiply, shift) creates thousands of tiny blocks, making design space exploration computationally intractable.</p> </li> <li> <p>Enable Hand Optimization - Hardware engineers can optimize kernels at the layer scale without deep AI model knowledge, achieving performance that auto-generated designs cannot match while maintaining flexibility through automated composition.</p> </li> </ol>"},{"location":"developer-guide/hardware-kernels/#three-file-structure","title":"Three-File Structure","text":"<p>Each kernel consists of:</p> <ol> <li>Schema + KernelOp (<code>kernel.py</code>) - Interface definitions, validation constraints, and ONNX-to-hardware transformation logic</li> <li>Backend (<code>kernel_hls.py</code> or <code>kernel_rtl.py</code>) - Code generation that bridges schema to implementation</li> <li>Hardware (<code>kernel.hpp</code> or <code>kernel.v</code>) - Actual RTL/HLS implementation with standard interfaces</li> </ol> <p>This separation enables:</p> <ul> <li>Multiple backends for the same kernel (HLS vs RTL, vendor-specific optimizations)</li> <li>Schema-driven automation (DSE parameter derivation, validation, interface generation)</li> <li>Hardware expertise isolation (implement kernels without compiler knowledge)</li> </ul>"},{"location":"developer-guide/hardware-kernels/#quick-start-minimum-kernel","title":"Quick Start: Minimum Kernel","text":"<pre><code>project/kernels/my_kernel/\n\u251c\u2500\u2500 my_kernel.py          # Schema + transformation logic\n\u251c\u2500\u2500 my_kernel_hls.py      # Code generation backend\n\u2514\u2500\u2500 my_kernel.hpp         # Hardware implementation\n</code></pre> <p>my_kernel.py - Schema + KernelOp: </p><pre><code>import brainsmith.dataflow as df\nfrom brainsmith.registry import kernel\nfrom onnx import helper\n\nMY_KERNEL_SCHEMA = df.KernelSchema(\n    name=\"MyKernel\",\n    inputs=[df.InputSchema(name=\"input\", block_tiling=df.FULL_SHAPE, stream_tiling=[\"SIMD\"])],\n    outputs=[df.OutputSchema(name=\"output\", block_tiling=df.FULL_SHAPE, stream_tiling=[\"PE\"])],\n)\n\n@kernel(description=\"Custom kernel\", author=\"Your Name\")\nclass MyKernel(df.KernelOp):\n    @classmethod\n    def build_schema(cls, node, model):\n        return MY_KERNEL_SCHEMA\n\n    @classmethod\n    def can_infer_from(cls, node, model):\n        return node.op_type == \"MyOnnxOp\"  # Pattern match ONNX nodes\n\n    @classmethod\n    def infer_from(cls, node, model, insert_index):\n        hw_node = helper.make_node(\"MyKernel\", inputs=list(node.input),\n                                    outputs=list(node.output), domain=\"brainsmith.kernels\")\n        return df.TransformationResult(nodes_to_insert=[hw_node], nodes_to_remove=[node])\n\n    def execute_node(self, context, graph):\n        # Reference numpy implementation for validation\n        pass\n</code></pre><p></p> <p>my_kernel_hls.py - Backend: </p><pre><code>from finn.custom_op.fpgadataflow.hlsbackend import HLSBackend\nfrom brainsmith.registry import backend\n\n@backend(target_kernel=\"brainsmith:MyKernel\", language=\"hls\")\nclass MyKernel_hls(MyKernel, HLSBackend):\n    def defines(self, var):\n        # Extract parameters from self.design_point\n        point = self.design_point\n        simd = point.inputs[\"input\"].stream_shape[-1]\n        self.code_gen_dict[\"$DEFINES$\"] = [f\"#define SIMD {simd}\"]\n\n    def docompute(self):\n        self.code_gen_dict[\"$DOCOMPUTE$\"] = [\"my_kernel&lt;SIMD&gt;(in0, out);\"]\n</code></pre><p></p> <p>my_kernel.hpp - Hardware: </p><pre><code>template&lt;unsigned SIMD, typename TI, typename TO&gt;\nvoid my_kernel(hls::stream&lt;hls::vector&lt;TI, SIMD&gt;&gt;&amp; in,\n               hls::stream&lt;hls::vector&lt;TO, SIMD&gt;&gt;&amp; out) {\n    // Your hardware implementation\n}\n</code></pre><p></p> <p>Register in <code>__init__.py</code>: </p><pre><code>from .my_kernel import MyKernel\nfrom .my_kernel_hls import MyKernel_hls\n__all__ = [\"MyKernel\", \"MyKernel_hls\"]\n</code></pre><p></p> <p>Use in blueprint: </p><pre><code>design_space:\n  kernels:\n    - MyKernel\n</code></pre><p></p>"},{"location":"developer-guide/hardware-kernels/#complete-example-layernorm-kernel","title":"Complete Example: LayerNorm Kernel","text":"<p>The LayerNorm kernel demonstrates a complete production implementation with schema definition, ONNX transformation, HLS backend, and hardware code.</p> <p>Implementation files:</p> <ul> <li><code>layernorm.py</code> - Schema definition with <code>FULL_DIM</code> tiling, <code>derive_dim()</code> for matching parallelization, epsilon parameter, and ONNX-to-hardware transformation</li> <li><code>layernorm_hls.py</code> - HLS backend extracting parameters from design_point and generating C++ defines/docompute</li> <li><code>layernorm.hpp</code> - HLS C++ implementation with AXI-Stream interfaces and normalization logic</li> </ul> <p>Key patterns demonstrated:</p> <ul> <li>Schema-driven automation (SIMD parameter auto-extracted from <code>stream_tiling</code>)</li> <li>Dimension derivation (<code>derive_dim()</code>) to match input/output parallelization</li> <li>Validation constraints (<code>AttrCompare</code> for epsilon &gt; 0)</li> <li>Pattern matching (<code>can_infer_from()</code> checks for FuncLayerNorm with axis=-1)</li> <li>Multiple inheritance (LayerNorm + HLSBackend) for code generation</li> <li>Reference implementation (<code>execute_node()</code>) for cppsim validation</li> </ul>"},{"location":"developer-guide/multi-layer-offload/","title":"Multi-Layer Offload","text":""},{"location":"developer-guide/multi-layer-offload/#multilayer-offload-mlo","title":"Multilayer Offload (MLO)","text":"<p>Multilayer Offload (MLO) is a powerful feature recently added to FINN that enables the implementation of much larger neural networks by implementing a repeating slice of the model (such as a single transformer encoder layer) in hardware and cycling model weights through external memory (DRAM/HBM). This technique allows models that would otherwise be too large to be mapped to the FPGA.</p> <p>MLO is currently an experimental feature is not yet available on the main branch.</p>"},{"location":"developer-guide/multi-layer-offload/#overview","title":"Overview","text":"<p>In many cases large Deep Learning models such as transformers and SLMs (and LLMs for that matter) have millions or billions of parameters processed over several identical repeating layers. One solution would be to map these layers to multiple FPGAs but the sheer quantity of layers (e.g. 32 layers in the PHI-4 Mini) makes it impractical to spread the design across so many devices. MLO overcomes this limitation by:</p> <ol> <li>Implementing a single repeating layer (e.g., one transformer encoder) in hardware</li> <li>Storing weights off-chip in high-bandwidth memory (HBM/DRAM)</li> <li>Streaming weights into the accelerator as needed for each layer</li> <li>Reusing the same hardware to process multiple layers sequentially</li> </ol> <p>This approach trades some throughput for the ability to handle much larger models, making it ideal for larger transformer models such as SLMs, vision transformers, and other deep architectures.</p>"},{"location":"developer-guide/multi-layer-offload/#how-it-works","title":"How It Works","text":""},{"location":"developer-guide/multi-layer-offload/#loop-body-hierarchy","title":"Loop Body Hierarchy","text":"<p>MLO works by identifying a repeating structure in the neural network and implementing only that structure in hardware. Currently, loop body discovery is not automated - users must manually identify one iteration of the repeating pattern and specify it using the <code>loop_body_hierarchy</code> parameter:</p> <pre><code>finn_config:\n  loop_body_hierarchy: [['encoder', 'encoder.layer.0']]\n</code></pre> <p>Manual Loop Body Identification: The <code>loop_body_hierarchy</code> configuration must match the hierarchical naming structure in your ONNX model, which corresponds to the <code>pkg.torch.onnx.name_scopes</code> field used during model export. The loop rolling transformation uses these name scopes to determine which levels of hierarchy to include in the loop body.</p> <p>\u26a0\ufe0f Important: You must use <code>dynamo=True</code> when exporting your PyTorch model to ONNX. Exporting with <code>dynamo=True</code> generates the metadata (name scopes) that MLO requires to identify repeating structures. Without this flag, the ONNX model will lack the hierarchical metadata needed for loop body discovery, and the MLO transformation will fail to locate the repeating patterns.</p> <p>Technical Implementation: The node extraction mechanism is implemented in FINN's loop rolling transformations:</p> <ul> <li>Step Location: <code>deps/finn/src/finn/builder/build_dataflow_steps.py</code></li> <li>Extraction Process: <code>deps/finn/src/finn/transformation/fpgadataflow/loop_rolling.py</code> (LoopExtraction class)</li> <li>Hierarchy Matching: <code>deps/finn/src/finn/util/onnxscript_helpers.py</code> (PytorchHierarchyNode class)</li> </ul> <p>The extraction works by:</p> <ol> <li>Creating a hierarchy parser from PyTorch metadata (<code>pkg.torch.onnx.name_scopes</code>)</li> <li>Adding each ONNX node to the parser based on its hierarchy path</li> <li>Using prefix matching to find all nodes under the specified hierarchy paths</li> <li>Extracting matching nodes to create loop templates and removing originals from the main graph</li> </ol> <p>This process requires the PyTorch exporter metadata generated by <code>dynamo=True</code>, which contains the module instance hierarchies that map ONNX nodes back to their originating PyTorch modules.</p> <p>This configuration tells Brainsmith:</p> <ul> <li>Look for a repeating pattern called 'encoder' (top-level hierarchy)</li> <li>The repeating unit is 'encoder.layer.0' (one complete encoder layer)</li> <li>All encoder layers (layer.0, layer.1, layer.2, etc.) will be processed using the same hardware</li> <li>The name scopes must exactly match the ONNX node names for proper identification</li> </ul>"},{"location":"developer-guide/multi-layer-offload/#multiple-hierarchy-groups","title":"Multiple Hierarchy Groups","text":"<p>For models with multiple independent repeating structures, you can specify multiple hierarchy groups in the <code>loop_body_hierarchy</code> configuration:</p> <pre><code>finn_config:\n  loop_body_hierarchy: [\n    ['encoder', 'encoder.layer.0'],\n    ['encoder', 'encoder.layer.1']\n  ]\n</code></pre> <p>This advanced configuration enables the following:</p> <ul> <li>Multiple Loop Iterations in a Single Body - Include nodes from consecutive layers (e.g., layer.0 and layer.1) to unroll multiple iterations into the hardware implementation</li> <li>Fine-tuning Node Selection - Adjust which nodes are included in the loop body when metadata is lost or inexact during ONNX export</li> </ul> <p>Multiple Group Behavior:</p> <ul> <li>The loop body will include all of the nodes belonging to each hierarchy region within the loop body.</li> </ul>"},{"location":"developer-guide/multi-layer-offload/#hierarchy-level-specification","title":"Hierarchy Level Specification","text":"<p>The <code>loop_body_hierarchy</code> can specify multiple levels of hierarchy to precisely control what gets included in the loop body:</p> <p>Two-level hierarchy (simple case): </p><pre><code>loop_body_hierarchy: [['encoder', 'encoder.layer.0']]\n</code></pre> - Includes all nodes under <code>encoder.layer.0.*</code> - Good for simple transformer architectures<p></p> <p>Three-level hierarchy (precise control): </p><pre><code>loop_body_hierarchy: [\n  ['bert', 'bert.encoder', 'bert.encoder.layer.0']\n]\n</code></pre> - Specifies the full path: model \u2192 encoder stack \u2192 specific layer - Provides more precise control over node selection - Useful for complex models with nested structures<p></p> <p>The FINN loop rolling step will find all ONNX nodes whose names start with the final hierarchy level (e.g., <code>bert.encoder.layer.0</code>) and extract them as the loop body.</p>"},{"location":"developer-guide/multi-layer-offload/#loop-rolling-process","title":"Loop Rolling Process","text":"<p>The loop rolling transformation (<code>step_loop_rolling</code> in FINN) performs these key operations:</p> <ol> <li>Parses the <code>loop_body_hierarchy</code> to identify which nodes belong to the repeating structure</li> <li>Extracts nodes by name scope matching - finds all ONNX nodes whose names match the specified hierarchy pattern (e.g., nodes starting with 'bert.encoder.layer.0')</li> <li>Generates loop iteration logic - creates control structures to iterate through all layers using the same hardware</li> <li>Sets up weight streaming infrastructure - configures memory interfaces to stream different weights for each iteration</li> <li>Updates folding configuration - modifies parallelization parameters to account for the loop structure</li> </ol>"},{"location":"developer-guide/multi-layer-offload/#loop-body-extraction-details","title":"Loop Body Extraction Details","text":"<p>The specific extraction logic is implemented in the FINN library (<code>finn.builder.build_dataflow_steps.step_loop_rolling</code>). While the exact source code lines are not visible in this repository, the process performs these operations based on observable behavior:</p> <p>Node Selection Process: </p><pre><code># Conceptual extraction logic (actual implementation in FINN)\ndef extract_loop_body_nodes(model, loop_body_hierarchy):\n    \"\"\"Extract nodes matching the loop body hierarchy pattern.\"\"\"\n    extracted_nodes = []\n\n    # Get the target pattern from hierarchy (e.g., 'bert.encoder.layer.0')\n    target_pattern = loop_body_hierarchy[0][-1]  # Final level\n\n    # Find all nodes whose names start with the target pattern\n    for node in model.graph.node:\n        if node.name.startswith(target_pattern):\n            extracted_nodes.append(node)\n\n    return extracted_nodes\n</code></pre><p></p> <p>The metadata fields exported by PyTorch Dynamo are not always reliable and in some cases can be removed by optimization passes. When encountered, these issues are reported to the onnxscript team and are often resolved. However, we have tried to make the Loop Body Extraction process as robust as possible in the presence of missing metadata.</p> <p>In some cases, the Loop Body Extraction process can identify nodes with missing metadata fields. For example, if a node is missing its metadata field, Loop Extract attempts to infer the missing information for that node by checking the metadata of its input and output nodes.</p>"},{"location":"developer-guide/multi-layer-offload/#configuration","title":"Configuration","text":""},{"location":"developer-guide/multi-layer-offload/#basic-mlo-setup","title":"Basic MLO Setup","text":"<p>To enable MLO in your blueprint, add the <code>loop_body_hierarchy</code> configuration:</p> <pre><code>name: \"BERT with MLO\"\ndescription: \"BERT model with Multilayer Offload\"\n\nfinn_config:\n  loop_body_hierarchy: [['encoder', 'encoder.layer.0']]\n  split_large_fifos: true\n  fifosim_n_inferences: 2  # Speed up FIFO simulation\n\ndesign_space:\n  steps:\n    - \"qonnx_to_finn\"\n    - \"bert_streamlining\"\n    - \"infer_kernels\"\n    - \"create_dataflow_partition\"\n    - \"specialize_layers\"\n    - \"loop_rolling\"        # This step implements MLO\n    - \"target_fps_parallelization\"\n    - \"apply_folding_config\"\n    # ... rest of pipeline\n</code></pre> <p>The easiest way to identify the proper loop body hierarchy is to open the model in Netron and check the values of the node metadata that you'd like to include in the loop body.</p>"},{"location":"developer-guide/multi-layer-offload/#bert-mlo-example","title":"BERT MLO Example","text":"<p>For BERT models, a typical MLO configuration looks like:</p> <pre><code># bert_mlo_demo.yaml\nname: \"BERT Demo\"\ndescription: \"Hugging face BERT model with MLO\"\n\nextends: \"../../brainsmith/blueprints/bert.yaml\"\n\nfinn_config:\n  loop_body_hierarchy: [['encoder', 'encoder.layer.0']]\n  split_large_fifos: true\n  fifosim_n_inferences: 2\n  verify_steps: ['folded_hls_cppsim', 'stitched_ip_rtlsim']\n\ndesign_space:\n  steps:\n    - at_start:\n        insert:\n          - \"bert_cleanup\"\n          - \"remove_head\"\n          - \"remove_tail\"\n          - \"generate_reference_io\"\n    - at_end:\n        insert: \"shell_metadata_handover\"\n</code></pre>"},{"location":"developer-guide/multi-layer-offload/#example-bert-mlo-demo","title":"Example: BERT MLO Demo","text":"<p>The <code>examples/bert/bert_mlo_demo.sh</code> demonstrates a complete MLO workflow:</p> <pre><code>#!/bin/bash\n# BERT MLO Demo\n\n# Generate folding configuration\npython gen_folding_config.py \\\n    --simd 4 \\\n    --pe 4 \\\n    --num_layers 2 \\\n    -t 1 \\\n    -o ./configs/bert_mlo_demo.json\n\n# Run BERT demo with MLO\npython bert_demo.py \\\n    -o bert_mlo_demo \\\n    -n 4 \\                    # 4 attention heads\n    -l 2 \\                    # 2 layers total\n    -z 64 \\                   # Hidden size 64\n    -i 256 \\                  # Intermediate size 256\n    -b 8 \\                    # 8-bit quantization\n    -q 32 \\                   # Sequence length 32\n    --blueprint ./bert_mlo_demo.yaml\n</code></pre> <p>This creates a BERT model with 2 encoder layers where only the first layer is implemented in hardware, and the second layer reuses the same hardware with different weights.</p> <p>CRITICAL: ONNX Export Requirements </p><pre><code># When exporting your model to ONNX, you MUST use dynamo=True\n# This generates the metadata (name scopes) that MLO requires for loop body discovery\nimport brevitas.onnx as bo\n\nbo.export_qonnx(\n    model,\n    inputs,\n    output_path,\n    dynamo=True,              # Generates name scope metadata for MLO\n    input_names=['input_ids'],\n    opset_version=18,\n    do_constant_folding=True\n)\n</code></pre><p></p> <p>Alternative: Custom Loop Rolling for Non-Dynamo Export</p> <p>If you cannot use <code>dynamo=True</code> (due to compatibility issues, model complexity, or other constraints), you can either add the metadata manually or you can implement a custom loop rolling step.</p> <p>Adding Metadata Manually</p> <p>If your ONNX model was exported without <code>dynamo=True</code> or the metadata was lost during optimization, you can manually add the required <code>pkg.torch.onnx.name_scopes</code> metadata to enable MLO. This approach requires modifying the ONNX model's metadata properties directly.</p> <p>Step 1: Understanding the Metadata Structure</p> <p>The <code>pkg.torch.onnx.name_scopes</code> metadata field contains hierarchical naming information that maps each ONNX node back to its originating PyTorch module. The metadata is stored as a list of strings representing the hierarchy path from the root module to the specific operation.</p> <p>For example, in a BERT model: </p><pre><code># Layer 0 attention query node\n['bert', 'bert.encoder', 'bert.encoder.layer.0', 'bert.encoder.layer.0.attention.self.query']\n\n# Layer 0 attention key node\n['bert', 'bert.encoder', 'bert.encoder.layer.0', 'bert.encoder.layer.0.attention.self.key']\n\n# Layer 1 attention query node\n['bert', 'bert.encoder', 'bert.encoder.layer.1', 'bert.encoder.layer.1.attention.self.query']\n</code></pre><p></p> <p>Step 2: Identify Your Model's Hierarchy</p> <p>First, determine the hierarchical structure of your model:</p> <pre><code>import torch\n\n# Example: Print your PyTorch model structure\nmodel = YourModel()\nfor name, module in model.named_modules():\n    print(name)\n\n# Output might look like:\n# encoder\n# encoder.layer.0\n# encoder.layer.0.attention\n# encoder.layer.0.attention.self\n# encoder.layer.1.attention\n# encoder.layer.1.attention.self\n</code></pre> <p>Step 3: Add Metadata to ONNX Nodes</p> <p>Use the following script to add metadata to your ONNX model:</p> <pre><code>import onnx\nfrom onnx import helper\n\ndef add_name_scope_metadata(model_path, output_path, node_hierarchy_map):\n    \"\"\"\n    Add pkg.torch.onnx.name_scopes metadata to ONNX nodes.\n\n    Args:\n        model_path: Path to input ONNX model\n        output_path: Path to save modified ONNX model\n        node_hierarchy_map: Dict mapping node names to hierarchy paths (as list of strings)\n                           e.g., {'MatMul_0': ['encoder', 'encoder.layer.0', 'encoder.layer.0.attention']}\n    \"\"\"\n    model = onnx.load(model_path)\n\n    for node in model.graph.node:\n        if node.name in node_hierarchy_map:\n            hierarchy_list = node_hierarchy_map[node.name]\n            # Convert list to the string format expected by ONNX metadata\n            # Format: serialized list of strings\n            hierarchy_str = str(hierarchy_list)\n\n            # Add or update the metadata attribute\n            metadata_found = False\n            for attr in node.attribute:\n                if attr.name == \"pkg.torch.onnx.name_scopes\":\n                    attr.s = hierarchy_str.encode('utf-8')\n                    metadata_found = True\n                    break\n\n            if not metadata_found:\n                # Create new metadata attribute\n                metadata_attr = helper.make_attribute(\n                    \"pkg.torch.onnx.name_scopes\",\n                    hierarchy_str\n                )\n                node.attribute.append(metadata_attr)\n\n    onnx.save(model, output_path)\n    print(f\"Model with metadata saved to {output_path}\")\n\n# Example usage for a BERT model\nnode_hierarchy_map = {\n    # Attention layer nodes\n    'MatMul_0': ['bert', 'bert.encoder', 'bert.encoder.layer.0', 'bert.encoder.layer.0.attention.self.query'],\n    'MatMul_1': ['bert', 'bert.encoder', 'bert.encoder.layer.0', 'bert.encoder.layer.0.attention.self.key'],\n    'MatMul_2': ['bert', 'bert.encoder', 'bert.encoder.layer.0', 'bert.encoder.layer.0.attention.self.value'],\n    'MatMul_3': ['bert', 'bert.encoder', 'bert.encoder.layer.0', 'bert.encoder.layer.0.attention.output.dense'],\n\n    # Intermediate layer nodes\n    'MatMul_4': ['bert', 'bert.encoder', 'bert.encoder.layer.0', 'bert.encoder.layer.0.intermediate.dense'],\n    'MatMul_5': ['bert', 'bert.encoder', 'bert.encoder.layer.0', 'bert.encoder.layer.0.output.dense'],\n\n    # LayerNorm nodes\n    'LayerNormalization_0': ['bert', 'bert.encoder', 'bert.encoder.layer.0', 'bert.encoder.layer.0.attention.output.LayerNorm'],\n    'LayerNormalization_1': ['bert', 'bert.encoder', 'bert.encoder.layer.0', 'bert.encoder.layer.0.output.LayerNorm'],\n\n    # You only need to add metadata for the nodes used in the loop body template\n}\n\nadd_name_scope_metadata(\n    'model_without_metadata.onnx',\n    'model_with_metadata.onnx',\n    node_hierarchy_map\n)\n</code></pre> <p>Step 4: Verify Metadata with Netron</p> <p>After adding metadata, open the modified model in Netron and inspect node properties to verify the <code>pkg.torch.onnx.name_scopes</code> field appears correctly.</p> <p>Step 5: Use in MLO Configuration</p> <p>Once metadata is added, configure your blueprint with the appropriate <code>loop_body_hierarchy</code>:</p> <pre><code>finn_config:\n  loop_body_hierarchy: [['encoder', 'encoder.layer.0']]  # Must match your hierarchy paths\n</code></pre> <p>Important Notes: - Metadata must accurately reflect the repeating structure of your model - All nodes within a layer should have consistent hierarchy prefixes - Test with a small model (2-3 layers) before applying to larger models - Incorrect metadata will cause loop body extraction to fail or extract wrong nodes</p> <p>Custom Loop Rolling Step</p> <p>If you cannot export via PyTorch Dynamo, you can write your own Loop Extraction transform and then leverage the existing Loop Rolling transform to create the FINNLoop ONNX node. At present, you'll need to copy the Loop Rolling step in FINN and replace the Loop Extraction functionality. In the future, we plan to update the Loop Rolling step to accept a custom Loop Extraction function.</p> <p>The standard Loop Rolling build step consists of two transformations: Loop Body Extraction and Loop Rolling. Loop Body Extraction returns a LoopBodyTemplate object which is used by the LoopRolling transformation to as a pattern to identify individual instances of each loop body. The LoopBody template object is created using an ONNX file that contains one copy of the LoopBody you'd like to create.</p> <p>If you have a graph of the loop body or can easily create one, then you can simply create a custom Loop Rolling step in BrainSmith that creates the LoopBodyTemplate object from the ONNX file and passes it to the LoopRolling transformation as shown in the example code below.</p> <p>Example: Custom Loop Rolling Step with Pre-built Loop Body Template</p> <pre><code>from brainsmith.core.plugins import step\nfrom finn.transformation.fpgadataflow.loop_rolling import LoopBodyTemplate, LoopRolling\n\n@step(name=\"custom_loop_rolling_with_template\")\ndef custom_loop_rolling_with_template(model, cfg):\n    \"\"\"\n    Custom loop rolling step that uses a pre-created loop body ONNX file.\n\n    Use this approach when you have manually created or extracted the loop body\n    graph and saved it to an ONNX file.\n    \"\"\"\n    # Load the loop body template from a pre-created ONNX file\n    # This file should contain one complete iteration of your loop body\n    loop_body_template_path = \"path/to/your/loop_body_template.onnx\"\n    loop_body_template = LoopBodyTemplate(loop_body_template_path)\n\n    # Apply the loop rolling transformation using your custom template\n    model = model.transform(LoopRolling(loop_body_template))\n\n    return model\n</code></pre> <p>In this approach, you need to manually create <code>loop_body_template.onnx</code> containing one instance of your repeating layer structure. You can create this file by: 1. Extracting a subgraph from your full model using ONNX tools 2. Building it programmatically using ONNX IR or onnxscript 3. Exporting a single layer model from PyTorch</p> <p>Otherwise, you can create a custom LoopBodyExtraction transform. One approach to creating this transform is to create a python list of ONNX nodes within the model that fully comprise an iteration of the LoopBody. Then you can use that list to create a SubGraphView object which can in turn be saved to an ONNX file and then used to create the LoopBodyTemplate as shown in the example code below.</p> <p>Example: Custom Loop Extraction and Rolling</p> <pre><code>from brainsmith.core.plugins import step\nfrom finn.transformation.fpgadataflow.loop_rolling import LoopBodyTemplate, LoopRolling\nfrom finn.util import onnxscript_helpers as osh\nimport onnxscript\nfrom onnxscript import ir\nimport onnx\n\nclass CustomLoopExtraction:\n    \"\"\"\n    Custom loop body extraction that identifies loop body nodes\n    without relying on PyTorch metadata.\n    \"\"\"\n\n    def __init__(self, loop_body_hierarchy):\n        self.loop_body_hierarchy = loop_body_hierarchy\n        self.loop_body_template = None\n\n    def extract_loop_body_nodes(self, graph, target_pattern):\n        \"\"\"\n        Identify nodes that belong to the loop body.\n\n        This is where you implement your custom logic to find the nodes.\n        You can use pattern matching, graph analysis, or any other method.\n        \"\"\"\n        extracted_nodes = []\n\n        # Strategy 1: Simple name prefix matching\n        for node in graph._nodes:\n            if node.name.startswith(target_pattern):\n                extracted_nodes.append(node)\n\n        # Strategy 2: If prefix matching fails, try pattern in node name\n        if not extracted_nodes:\n            layer_id = target_pattern.split('.')[-1]\n            for node in graph._nodes:\n                if f\".{layer_id}.\" in node.name or f\"_{layer_id}_\" in node.name:\n                    extracted_nodes.append(node)\n\n        return extracted_nodes\n\n    def apply(self, model):\n        \"\"\"Extract loop body and create template file.\"\"\"\n        # Deserialize the model to ONNX IR\n        model_ir = onnxscript.ir.serde.deserialize_model(model.model)\n        graph = model_ir.graph\n\n        # Get the target pattern from hierarchy\n        target_pattern = self.loop_body_hierarchy[0][-1]\n\n        # Extract nodes belonging to the loop body\n        nodes = self.extract_loop_body_nodes(graph, target_pattern)\n\n        if not nodes:\n            raise ValueError(f\"No nodes found matching pattern: {target_pattern}\")\n\n        print(f\"Extracted {len(nodes)} nodes for loop body\")\n\n        # Create a SubGraphView containing only the loop body nodes\n        loop_body_graph_view = osh.SubGraphView(graph, \"loop-body\", nodes)\n\n        # Create an ONNX model from the subgraph\n        loop_body_model = onnxscript.ir.Model(\n            loop_body_graph_view,\n            ir_version=model.model.ir_version\n        )\n\n        # Serialize and save the loop body template\n        proto = onnxscript.ir.serde.serialize_model(loop_body_model)\n        template_path = \"loop-body-template.onnx\"\n        onnx.save(proto, template_path)\n\n        print(f\"Loop body template saved to: {template_path}\")\n\n        # Create the LoopBodyTemplate object\n        self.loop_body_template = LoopBodyTemplate(template_path)\n\n        return model\n\n@step(name=\"custom_loop_rolling_full\")\ndef custom_loop_rolling_full(model, cfg):\n    \"\"\"\n    Complete custom loop rolling step with custom extraction.\n\n    This approach:\n    1. Uses custom logic to identify loop body nodes\n    2. Creates a loop body template from those nodes\n    3. Applies FINN's LoopRolling transformation\n    \"\"\"\n    # Get loop body hierarchy from config\n    hierarchy = cfg.loop_body_hierarchy if hasattr(cfg, 'loop_body_hierarchy') \\\n                else [['encoder', 'encoder.layer.0']]\n\n    # Step 1: Custom extraction to create loop body template\n    extractor = CustomLoopExtraction(hierarchy)\n    model = extractor.apply(model)\n\n    # Step 2: Apply FINN's loop rolling with the custom template\n    if extractor.loop_body_template is None:\n        raise ValueError(\"Loop body extraction failed - no template created\")\n\n    model = model.transform(LoopRolling(extractor.loop_body_template))\n\n    print(\"Custom loop rolling completed successfully\")\n\n    return model\n</code></pre> <p>Key Points:</p> <ol> <li> <p>CustomLoopExtraction.extract_loop_body_nodes(): This is where you implement your custom logic to identify which nodes belong to the loop body. The example shows simple name matching, but you can implement more sophisticated graph analysis.</p> </li> <li> <p>SubGraphView: This FINN utility class creates a view of a subgraph given a list of nodes. It automatically handles:</p> </li> <li>Finding all necessary inputs/outputs</li> <li>Maintaining graph connectivity</li> <li> <p>Preserving node attributes and metadata</p> </li> <li> <p>LoopBodyTemplate: This class (from FINN) wraps the loop body ONNX file and provides the pattern matching infrastructure that LoopRolling needs.</p> </li> <li> <p>LoopRolling transformation: This is FINN's standard transformation that:</p> </li> <li>Finds all instances of the loop body pattern in your model</li> <li>Replaces them with a single FINNLoop node</li> <li>Sets up weight streaming infrastructure</li> <li>Handles I/O normalization and type checking</li> </ol> <p>Usage in Blueprint:</p> <pre><code>design_space:\n  steps:\n    - \"qonnx_to_finn\"\n    - \"bert_streamlining\"\n    - \"infer_kernels\"\n    - \"create_dataflow_partition\"\n    - \"specialize_layers\"\n    - \"custom_loop_rolling_full\"  # Your custom step\n    - \"target_fps_parallelization\"\n    - \"apply_folding_config\"\n</code></pre>"},{"location":"developer-guide/multi-layer-offload/#debugging-mlo-issues","title":"Debugging MLO Issues","text":""},{"location":"developer-guide/multi-layer-offload/#common-problems","title":"Common Problems","text":"<p>Missing or incorrect metadata (most common): - Ensure ONNX export used <code>dynamo=True</code> to generate name scope metadata - Verify the ONNX model contains proper hierarchical node names - If unable to use dynamo export, implement custom loop rolling step (see Loop Body Identification section)</p> <p>Missing Loop Body Nodes</p> <p>If a node that should be in the loop body is not included during Loop Extraction, this can appear in <code>loopbody_template.onnx</code> as unexpected inputs and outputs to the loop body graph. Further, this can result in loop rolling failure or errors in subsequent build steps like <code>step_create_dataflow_partition</code>.</p> <p>Sometimes a node in the middle of the loop body will be excluded from the loop body. This can result in a self-referencing loop error in <code>step_create_dataflow_partition</code>, where the partitioning process detects invalid circular dependencies.</p> <p>Debugging Steps: 1. Open <code>loopbody_template.onnx</code> in your build directory using Netron 2. Check for unexpected graph inputs/outputs that should be internal to the loop body 3. Identify which nodes are missing by comparing against your expected layer structure 4. Adjust the <code>loop_body_hierarchy</code> configuration to include missing nodes:    - Try adding an additional hierarchy group for the missing node's namespace    - Use a broader hierarchy prefix to capture more nodes    - If using custom loop extraction, verify your node matching patterns 5. Verify metadata on the missing nodes (check <code>pkg.torch.onnx.name_scopes</code> field in Netron) 6. Rebuild and verify the <code>loopbody_template.onnx</code> contains all expected nodes</p> <p>Incorrect loop body identification: - Check <code>loop_body_hierarchy</code> matches your model structure - Verify layer naming conventions in ONNX graph</p>"},{"location":"developer-guide/multi-layer-offload/#debug-tools","title":"Debug Tools","text":"<ol> <li>Save intermediate models - Use <code>save_intermediate_models: true</code></li> <li>Enable verification - Use RTL simulation to check correctness</li> <li>Memory tracing - Monitor weight loading patterns</li> <li>Performance counters - Track cycles, bandwidth utilization</li> </ol>"},{"location":"developer-guide/registry/","title":"Component Registry","text":""},{"location":"developer-guide/registry/#component-registry","title":"Component Registry","text":"<p>The Component Registry enables declarative blueprint construction for FPGA accelerators:</p> <pre><code># blueprint.yaml\ndesign_space:\n  kernels: [LayerNorm, Softmax]\n  steps: [streamline, infer_kernels]\n</code></pre> <p>Component Types: Steps (pipeline transforms), Kernels (hardware operators), Backends (HLS/RTL code generators)</p> <p>Key Benefits: Parse-time validation (fail fast via <code>has_step()</code>), source priority (project overrides core), automatic discovery via decorators</p>"},{"location":"developer-guide/registry/#component-sources","title":"Component Sources","text":"Source Type Configuration Caching <code>brainsmith</code> Core framework Automatic (direct import) Yes <code>finn</code> (or custom) Entry point plugins <code>setup.cfg</code>: <code>brainsmith.plugins = pkg.module:func</code> Yes <code>project</code> Filesystem Automatic (<code>kernels/</code>, <code>steps/</code> subdirs) Yes e.g. <code>team</code> Filesystem <code>component_sources.team = \"/path\"</code> in config Yes <code>custom</code> Runtime registration Programmatic (no namespace/domain) No <p>Source priority (configurable): <code>['project', 'brainsmith', 'finn', 'custom']</code> - first match wins for short names</p> <p>Check registered components with <code>brainsmith registry</code></p>"},{"location":"developer-guide/registry/#registering-components","title":"Registering Components","text":"<p>Using components: </p><pre><code>from brainsmith.registry import get_step, get_kernel, list_backends_for_kernel\n\nstreamline = get_step('streamline')\nLayerNorm = get_kernel('LayerNorm')\nbackends = list_backends_for_kernel('LayerNorm', language='hls')\n</code></pre><p></p> <p>Registering components via decorators: </p><pre><code>from brainsmith.registry import kernel, backend, step\n\n@kernel\nclass MyKernel(HWCustomOp):\n    op_type = \"MyKernel\"\n\n@backend(target_kernel='MyKernel', language='hls')\nclass MyKernel_hls:\n    pass\n\n@step\ndef my_step(model, **config):\n    return model\n</code></pre><p></p> <p>Registering via entry points (for pip packages): </p><pre><code># setup.cfg\n[options.entry_points]\nbrainsmith.plugins =\n    myplugin = myplugin.registry:register_components\n</code></pre><p></p> <p>Discovery: Automatic on first use. Manual refresh: <code>discover_components(force_refresh=True)</code></p> <p>Configuration (<code>brainsmith.yaml</code>): <code>cache_components</code> (default: true), <code>components_strict</code> (default: true), <code>source_priority</code>, <code>component_sources</code></p>"},{"location":"developer-guide/registry/#api-patterns","title":"API Patterns","text":"<p>All components follow <code>get_*/has_*/list_*</code> pattern:</p> <ul> <li>get - Load component (imports module): <code>get_step()</code>, <code>get_kernel()</code>, <code>get_backend()</code></li> <li>has - Check existence (no import): <code>has_step()</code>, <code>has_kernel()</code></li> <li>list - Enumerate: <code>list_steps()</code>, <code>list_kernels()</code>, <code>list_backends()</code></li> </ul> <p>Name resolution: Short names use source priority (<code>LayerNorm</code> \u2192 project &gt; brainsmith &gt; finn). Qualified names force source (<code>brainsmith:LayerNorm</code>).</p> <p>Special: <code>get_kernel_infer()</code> for ONNX transforms, <code>list_backends_for_kernel(name, language='hls')</code> for backend discovery, <code>get_component_metadata()</code> for inspection.</p>"},{"location":"developer-guide/registry/#advanced-patterns","title":"Advanced Patterns","text":"<p>Dynamic backend selection: </p><pre><code>backends = list_backends_for_kernel('LayerNorm', language='hls')\nbackend_cls = get_backend(backends[0]) if backends else None\n</code></pre><p></p> <p>Plugin inventory: </p><pre><code>for src in ['brainsmith', 'finn', 'project']:\n    print(f\"{src}: {len(list_kernels(source=src))} kernels\")\n</code></pre><p></p> <p>Component introspection: Use <code>get_component_metadata(name, type)</code> to inspect without loading.</p>"},{"location":"tutorials/","title":"Tutorials (Coming Soon)","text":""},{"location":"tutorials/#tutorials-coming-soon","title":"Tutorials (Coming Soon)","text":"<p>Hands-on guides for building, optimizing, and deploying neural network accelerators with Brainsmith.</p>"},{"location":"tutorials/#coming-soon","title":"Coming Soon","text":"<p>This section will include step-by-step tutorials covering:</p> <ul> <li>End-to-End Workflows - From PyTorch model to FPGA bitstream</li> <li>Design Space Exploration - Finding optimal configurations</li> <li>Kernel Development - Building and integrating custom hardware kernels</li> <li>Performance Optimization - Maximizing throughput and efficiency</li> </ul>"},{"location":"tutorials/#available-resources","title":"Available Resources","text":"<p>While tutorials are in development, you can explore:</p> <ul> <li>Getting Started Guide - Installation and running the BERT example</li> <li>Example Projects - Working code examples in the repository</li> <li>GitHub Discussions - Ask questions and share experiences</li> </ul>"},{"location":"tutorials/#contributing-tutorials","title":"Contributing Tutorials","text":"<p>Is there a topic our documentation and examples don't cover? Please see our contribution guidelines  or open an issue to discuss your idea.</p>"}]}