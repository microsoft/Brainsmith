"""
Comprehensive Week 3 Integration Test Suite.

This test suite demonstrates the complete functionality of all Week 3 components
working together: Generator Factory, Pipeline Orchestrator, Workflow Engine,
Generator Management, and Integration Orchestrator.
"""

import pytest
import asyncio
import time
from unittest.mock import Mock, patch, MagicMock
from pathlib import Path

from brainsmith.tools.hw_kernel_gen.enhanced_config import PipelineConfig, GeneratorType
from brainsmith.tools.hw_kernel_gen.enhanced_data_structures import (
    RTLModule, RTLInterface, RTLSignal
)
from brainsmith.tools.hw_kernel_gen.enhanced_generator_base import (
    GenerationResult, GeneratedArtifact
)

# Week 3 orchestration imports
from brainsmith.tools.hw_kernel_gen.orchestration.generator_factory import (
    GeneratorFactory, GeneratorConfiguration, GeneratorCapability, GeneratorPriority
)
from brainsmith.tools.hw_kernel_gen.orchestration.pipeline_orchestrator import (
    PipelineOrchestrator, GeneratorStage, ValidationStage, ExecutionMode
)
from brainsmith.tools.hw_kernel_gen.orchestration.generation_workflow import (
    WorkflowEngine, WorkflowDefinition, GeneratorWorkflowStep, TransformationWorkflowStep,
    WorkflowContext, ConditionType, WorkflowCondition
)
from brainsmith.tools.hw_kernel_gen.orchestration.generator_management import (
    GeneratorManager, PoolConfiguration
)
from brainsmith.tools.hw_kernel_gen.orchestration.integration_orchestrator import (
    IntegrationOrchestrator, IntegrationConfiguration, IntegrationMode, ValidationLevel
)
from brainsmith.tools.hw_kernel_gen.orchestration.workflow_definitions import (
    StandardWorkflows, create_standard_workflow
)

# Import base classes
from brainsmith.tools.hw_kernel_gen.enhanced_generator_base import GeneratorBase


class MockEnhancedGenerator(GeneratorBase):
    """Enhanced mock generator for comprehensive testing."""
    
    def __init__(self, config=None, generator_name="mock", capabilities=None):
        super().__init__(config)
        self.generator_name = generator_name
        self.capabilities = capabilities or set()
        self.generation_count = 0
        self.last_inputs = None
    
    def get_template_name(self) -> str:
        """Get the primary template name for this generator."""
        return f"{self.generator_name}_template.py.j2"
    
    def get_artifact_type(self) -> str:
        """Get the artifact type produced by this generator."""
        return f"{self.generator_name}_artifact"
    
    def generate(self, inputs):
        self.generation_count += 1
        self.last_inputs = inputs
        
        # Create realistic artifacts
        artifacts = [
            GeneratedArtifact(
                file_name=f"{self.generator_name}_output.py",
                content=f"# Generated by {self.generator_name}\nclass Generated:\n    pass",
                artifact_type="python_code"
            )
        ]
        
        return GenerationResult(
            success=True,
            artifacts=artifacts,
            metadata={
                "generator": self.generator_name,
                "generation_time": 0.1,
                "input_count": len(inputs)
            }
        )


class TestWeek3ComponentIntegration:
    """Test integration between all Week 3 components."""
    
    def setup_method(self):
        """Set up comprehensive test environment."""
        self.config = PipelineConfig()
        self.config.generator_type = GeneratorType.AUTO_HW_CUSTOM_OP
        
        # Create realistic RTL module
        self.rtl_module = self.create_comprehensive_rtl_module()
        
        # Initialize all Week 3 components
        self.generator_factory = GeneratorFactory(self.config)
        self.generator_manager = GeneratorManager(self.config, self.generator_factory)
        self.pipeline_orchestrator = PipelineOrchestrator(
            self.config, self.generator_factory, ExecutionMode.HYBRID
        )
        self.workflow_engine = WorkflowEngine(self.config)
        self.integration_orchestrator = IntegrationOrchestrator(self.config)
        
        # Register mock generators
        self.register_mock_generators()
    
    def create_comprehensive_rtl_module(self) -> RTLModule:
        """Create a comprehensive RTL module for testing."""
        # Input AXI-Stream interface
        input_signals = [
            RTLSignal("s_axis_input_tdata", "input", 64, interface_role="tdata"),
            RTLSignal("s_axis_input_tvalid", "input", 1, interface_role="tvalid"),
            RTLSignal("s_axis_input_tready", "output", 1, interface_role="tready"),
            RTLSignal("s_axis_input_tlast", "input", 1, interface_role="tlast")
        ]
        input_interface = RTLInterface("s_axis_input", "axi_stream", input_signals)
        
        # Output AXI-Stream interface
        output_signals = [
            RTLSignal("m_axis_output_tdata", "output", 32, interface_role="tdata"),
            RTLSignal("m_axis_output_tvalid", "output", 1, interface_role="tvalid"),
            RTLSignal("m_axis_output_tready", "input", 1, interface_role="tready"),
            RTLSignal("m_axis_output_tlast", "output", 1, interface_role="tlast")
        ]
        output_interface = RTLInterface("m_axis_output", "axi_stream", output_signals)
        
        # Control interface
        control_signals = [
            RTLSignal("ap_clk", "input", 1, interface_role="clock"),
            RTLSignal("ap_rst_n", "input", 1, interface_role="reset"),
            RTLSignal("ap_start", "input", 1, interface_role="enable"),
            RTLSignal("ap_done", "output", 1, interface_role="interrupt")
        ]
        control_interface = RTLInterface("control", "control", control_signals)
        
        return RTLModule(
            name="comprehensive_accelerator",
            interfaces=[input_interface, output_interface, control_interface],
            parameters={
                "INPUT_WIDTH": 64,
                "OUTPUT_WIDTH": 32,
                "BUFFER_DEPTH": 1024
            }
        )
    
    def register_mock_generators(self):
        """Register mock generators with different capabilities."""
        generators = [
            ("hw_custom_op_generator", {GeneratorCapability.HW_CUSTOM_OP, GeneratorCapability.DATAFLOW_INTEGRATION}),
            ("rtl_backend_generator", {GeneratorCapability.RTL_BACKEND, GeneratorCapability.DATAFLOW_INTEGRATION}),
            ("documentation_generator", {GeneratorCapability.DOCUMENTATION}),
            ("test_generator", {GeneratorCapability.TEST_GENERATION}),
            ("validation_generator", {GeneratorCapability.VALIDATION}),
        ]
        
        for name, capabilities in generators:
            # Create a proper generator class for each one
            class SpecificMockGenerator(MockEnhancedGenerator):
                def __init__(self, config=None):
                    super().__init__(config, name, capabilities)
            
            SpecificMockGenerator.__name__ = f"Mock{name.replace('_', '').title()}Generator"
            
            self.generator_factory.registry.register_generator(
                name=name,
                generator_class=SpecificMockGenerator,
                capabilities=capabilities,
                priority=GeneratorPriority.MEDIUM
            )
    
    def test_generator_factory_integration(self):
        """Test generator factory integration with all components."""
        # Test generator creation with different configurations
        configs = [
            GeneratorConfiguration(
                generator_type=GeneratorType.AUTO_HW_CUSTOM_OP,
                config=self.config,
                capabilities_required={GeneratorCapability.HW_CUSTOM_OP}
            ),
            GeneratorConfiguration(
                generator_type=GeneratorType.AUTO_RTL_BACKEND,
                config=self.config,
                capabilities_required={GeneratorCapability.RTL_BACKEND}
            )
        ]
        
        generators = []
        for config in configs:
            generator = self.generator_factory.create_generator(config, self.rtl_module)
            generators.append(generator)
            assert generator is not None
            assert isinstance(generator, MockEnhancedGenerator)
        
        # Test that generators are cached properly
        cached_generator = self.generator_factory.create_generator(configs[0], self.rtl_module)
        assert cached_generator is generators[0]
        
        # Verify factory statistics
        stats = self.generator_factory.get_factory_statistics()
        assert stats["factory_stats"]["creations"] >= 2
        assert stats["factory_stats"]["cache_hits"] >= 1
    
    @pytest.mark.asyncio
    async def test_pipeline_orchestrator_integration(self):
        """Test pipeline orchestrator with generator and validation stages."""
        # Create generator stages
        hw_config = GeneratorConfiguration(
            generator_type=GeneratorType.AUTO_HW_CUSTOM_OP,
            config=self.config,
            capabilities_required={GeneratorCapability.HW_CUSTOM_OP}
        )
        
        rtl_config = GeneratorConfiguration(
            generator_type=GeneratorType.AUTO_RTL_BACKEND,
            config=self.config,
            capabilities_required={GeneratorCapability.RTL_BACKEND}
        )
        
        # Add stages to pipeline
        hw_stage = GeneratorStage("hw_generation", hw_config)
        rtl_stage = GeneratorStage("rtl_generation", rtl_config)
        
        # Add validation stage
        def validation_func(context):
            return {
                "success": True,
                "artifacts_validated": len(context.global_artifacts),
                "interfaces_count": len(context.rtl_module.interfaces)
            }
        
        validation_stage = ValidationStage(
            "validation",
            validation_func,
            dependencies={"hw_generation", "rtl_generation"}
        )
        
        self.pipeline_orchestrator.add_stage(hw_stage)
        self.pipeline_orchestrator.add_stage(rtl_stage)
        self.pipeline_orchestrator.add_stage(validation_stage)
        
        # Execute pipeline
        results = await self.pipeline_orchestrator.execute_pipeline(self.rtl_module)
        
        # Verify all stages completed successfully
        assert len(results) == 3
        for stage_name in ["hw_generation", "rtl_generation", "validation"]:
            assert results[stage_name].status.value == "completed"
        
        # Verify artifacts were generated
        total_artifacts = sum(len(result.artifacts) for result in results.values())
        assert total_artifacts >= 2  # At least one from each generator stage
        
        # Verify pipeline statistics
        stats = self.pipeline_orchestrator.get_pipeline_statistics()
        assert stats["pipeline_executions"] == 1
        assert stats["successful_pipelines"] == 1
    
    def test_workflow_engine_integration(self):
        """Test workflow engine with custom workflows."""
        # Create comprehensive workflow
        workflow = WorkflowDefinition(
            name="comprehensive_test_workflow",
            description="Test workflow with multiple step types",
            version="1.0.0"
        )
        
        # Step 1: Analysis transformation
        analysis_step = TransformationWorkflowStep(
            name="analysis",
            transformation_func=lambda ctx: {
                "interfaces_analyzed": len(ctx.rtl_module.interfaces),
                "parameters_analyzed": len(ctx.rtl_module.parameters),
                "analysis_completed": True
            }
        )
        workflow.add_step(analysis_step)
        
        # Step 2: HW Generation
        hw_step = GeneratorWorkflowStep(
            name="hw_generation",
            generator_name="hw_custom_op_generator",
            dependencies={"analysis"}
        )
        workflow.add_step(hw_step)
        
        # Step 3: RTL Generation
        rtl_step = GeneratorWorkflowStep(
            name="rtl_generation",
            generator_name="rtl_backend_generator",
            dependencies={"analysis"}
        )
        workflow.add_step(rtl_step)
        
        # Step 4: Conditional documentation (only if both generations succeed)
        def doc_condition_func(ctx):
            hw_success = ctx.get_step_result("hw_generation") is not None
            rtl_success = ctx.get_step_result("rtl_generation") is not None
            return hw_success and rtl_success
        
        doc_condition = WorkflowCondition(
            ConditionType.CUSTOM,
            custom_evaluator=doc_condition_func
        )
        
        doc_step = GeneratorWorkflowStep(
            name="documentation",
            generator_name="documentation_generator",
            dependencies={"hw_generation", "rtl_generation"},
            condition=doc_condition
        )
        workflow.add_step(doc_step)
        
        # Create workflow context
        context = WorkflowContext(
            rtl_module=self.rtl_module,
            config=self.config,
            generator_factory=self.generator_factory
        )
        
        # Execute workflow
        result = self.workflow_engine.execute_workflow(workflow, context)
        
        # Verify workflow execution
        assert result.status == "completed"
        assert len(result.step_results) == 4
        
        # Verify all steps completed
        for step_name in ["analysis", "hw_generation", "rtl_generation", "documentation"]:
            assert step_name in result.step_results
            step_result = result.step_results[step_name]
            assert step_result.status.value in ["completed", "skipped"]
        
        # Verify artifacts were generated
        assert len(result.global_artifacts) >= 2
    
    def test_generator_management_integration(self):
        """Test generator management with pooling and metrics."""
        # Create pool configuration
        pool_config = PoolConfiguration(
            min_instances=1,
            max_instances=3,
            preload_instances=True
        )
        
        # Create generator configuration
        generator_config = GeneratorConfiguration(
            generator_type=GeneratorType.AUTO_HW_CUSTOM_OP,
            config=self.config,
            capabilities_required={GeneratorCapability.HW_CUSTOM_OP}
        )
        
        # Create pool
        pool = self.generator_manager.create_pool(
            "hw_generator_pool",
            generator_config,
            pool_config
        )
        
        # Test instance acquisition and release
        instances = []
        for i in range(2):
            instance = self.generator_manager.acquire_generator(
                "hw_generator_pool",
                generator_config,
                pool_config
            )
            assert instance is not None
            instances.append(instance)
        
        # Use generators
        for i, instance in enumerate(instances):
            inputs = {"rtl_module": self.rtl_module, "config": self.config}
            result = instance.generator.generate(inputs)
            assert result.success
            
            # Release instance
            self.generator_manager.release_generator(instance, 0.1, True)
        
        # Verify pool statistics
        pool_stats = pool.get_pool_statistics()
        assert pool_stats["created_instances"] >= 1
        assert pool_stats["total_requests"] >= 2
        
        # Verify manager statistics
        manager_stats = self.generator_manager.get_manager_statistics()
        assert manager_stats["total_acquisitions"] >= 2
        assert manager_stats["total_releases"] >= 2
    
    @pytest.mark.asyncio
    async def test_integration_orchestrator_full_workflow(self):
        """Test integration orchestrator with complete workflow."""
        # Configure integration
        integration_config = IntegrationConfiguration(
            mode=IntegrationMode.ANALYSIS_ONLY,  # Use simpler mode for testing
            validation_level=ValidationLevel.BASIC,
            enable_parallel_processing=False,
            enable_performance_monitoring=True
        )
        
        orchestrator = IntegrationOrchestrator(self.config, integration_config, self.generator_factory)
        
        # Execute analysis-only generation (simpler test)
        result = await orchestrator.orchestrate_complete_generation(self.rtl_module)
        
        # Verify integration result (relaxed expectations for analysis-only mode)
        assert result is not None
        assert result.execution_time >= 0
        assert hasattr(result, 'metadata')
        
        # Verify orchestration statistics
        stats = orchestrator.get_orchestration_statistics()
        assert stats["orchestrations"] >= 1
    
    def test_standard_workflows_integration(self):
        """Test standard workflows with all components."""
        # Test basic generation workflow
        basic_workflow = create_standard_workflow("basic")
        assert basic_workflow.name == "basic_generation"
        assert len(basic_workflow.steps) >= 3
        
        # Test comprehensive workflow
        comprehensive_workflow = create_standard_workflow("comprehensive")
        assert comprehensive_workflow.name == "comprehensive_generation"
        assert len(comprehensive_workflow.steps) >= 7
        
        # Test dataflow optimized workflow
        dataflow_workflow = create_standard_workflow("dataflow_optimized")
        assert dataflow_workflow.name == "dataflow_optimized"
        assert dataflow_workflow.global_config.get("enable_dataflow") == True
        
        # Verify workflow validation
        for workflow in [basic_workflow, comprehensive_workflow, dataflow_workflow]:
            validation_errors = workflow.validate()
            assert len(validation_errors) == 0  # All workflows should be valid
    
    def test_performance_and_scalability(self):
        """Test performance characteristics of integrated system."""
        # Test multiple concurrent generator requests
        start_time = time.time()
        
        generators = []
        for i in range(5):
            config = GeneratorConfiguration(
                generator_type=GeneratorType.AUTO_HW_CUSTOM_OP,
                config=self.config,
                capabilities_required={GeneratorCapability.HW_CUSTOM_OP}
            )
            generator = self.generator_factory.create_generator(config, self.rtl_module)
            generators.append(generator)
        
        creation_time = time.time() - start_time
        
        # Should be fast due to caching
        assert creation_time < 1.0
        
        # Test concurrent generation
        start_time = time.time()
        
        for generator in generators:
            inputs = {"rtl_module": self.rtl_module, "config": self.config}
            result = generator.generate(inputs)
            assert result.success
        
        generation_time = time.time() - start_time
        
        # Should complete reasonably quickly
        assert generation_time < 2.0
        
        # Verify caching effectiveness
        cache_stats = self.generator_factory.get_factory_statistics()["cache_stats"]
        assert cache_stats["hit_rate"] > 0.5  # At least 50% cache hit rate
    
    def test_error_handling_and_recovery(self):
        """Test error handling across all components."""
        # Test with invalid generator configuration
        invalid_config = GeneratorConfiguration(
            generator_type=GeneratorType.AUTO_HW_CUSTOM_OP,
            config=self.config,
            capabilities_required={GeneratorCapability.OPTIMIZATION}  # Not available
        )
        
        # Should handle gracefully
        with pytest.raises(Exception):  # Should raise appropriate error
            self.generator_factory.create_generator(invalid_config, self.rtl_module)
        
        # Test workflow with failing step
        failing_workflow = WorkflowDefinition(
            name="failing_workflow",
            description="Workflow with failing step",
            version="1.0.0"
        )
        
        def failing_transform(ctx):
            raise Exception("Simulated step failure")
        
        failing_step = TransformationWorkflowStep(
            name="failing_step",
            transformation_func=failing_transform,
            continue_on_error=True  # Should continue despite error
        )
        failing_workflow.add_step(failing_step)
        
        success_step = TransformationWorkflowStep(
            name="success_step",
            transformation_func=lambda ctx: {"success": True},
            dependencies=set()  # No dependency on failing step
        )
        failing_workflow.add_step(success_step)
        
        context = WorkflowContext(
            rtl_module=self.rtl_module,
            config=self.config,
            generator_factory=self.generator_factory
        )
        
        # Should handle failure gracefully
        result = self.workflow_engine.execute_workflow(failing_workflow, context)
        
        # Should have both failed and successful steps
        assert len(result.step_results) == 2
        assert result.step_results["failing_step"].status.value == "failed"
        assert result.step_results["success_step"].status.value == "completed"
    
    def test_memory_and_resource_management(self):
        """Test memory and resource management across components."""
        # Test cache cleanup
        initial_cache_size = self.generator_factory.get_factory_statistics()["cache_stats"]["cache_size"]
        
        # Create many generators to fill cache
        for i in range(10):
            config = GeneratorConfiguration(
                generator_type=GeneratorType.AUTO_HW_CUSTOM_OP,
                config=self.config,
                capabilities_required={GeneratorCapability.HW_CUSTOM_OP},
                cache_enabled=True
            )
            rtl_variant = RTLModule(f"module_{i}", [], {})
            self.generator_factory.create_generator(config, rtl_variant)
        
        # Cache should have managed size appropriately
        final_cache_size = self.generator_factory.get_factory_statistics()["cache_stats"]["cache_size"]
        assert final_cache_size <= self.generator_factory.cache.max_size
        
        # Test cache clearing
        self.generator_factory.clear_cache()
        cleared_cache_size = self.generator_factory.get_factory_statistics()["cache_stats"]["cache_size"]
        assert cleared_cache_size == 0
        
        # Test generator manager cleanup
        self.generator_manager.shutdown()
        
        # Verify cleanup
        manager_stats = self.generator_manager.get_manager_statistics()
        # Manager should still provide statistics but pools should be cleaned up


class TestWeek3EndToEndScenarios:
    """End-to-end scenarios testing complete Week 3 functionality."""
    
    def setup_method(self):
        """Set up end-to-end test environment."""
        self.config = PipelineConfig()
        self.rtl_module = self.create_test_rtl_module()
    
    def create_test_rtl_module(self) -> RTLModule:
        """Create test RTL module."""
        signals = [
            RTLSignal("data_in", "input", 32),
            RTLSignal("data_out", "output", 32),
            RTLSignal("clk", "input", 1),
            RTLSignal("rst", "input", 1)
        ]
        interface = RTLInterface("simple_interface", "custom", signals)
        
        return RTLModule(
            name="simple_module",
            interfaces=[interface],
            parameters={"WIDTH": 32}
        )
    
    @pytest.mark.asyncio
    async def test_complete_generation_pipeline(self):
        """Test complete generation pipeline from RTL to artifacts."""
        # Initialize orchestrator
        orchestrator = IntegrationOrchestrator(self.config)
        
        # Mock the analysis orchestrator for this test
        mock_analysis_results = Mock()
        mock_analysis_results.success = True
        mock_analysis_results.rtl_module = self.rtl_module
        mock_analysis_results.interface_results = []
        mock_analysis_results.pragma_results = None
        mock_analysis_results.total_analysis_time = 0.1
        mock_analysis_results.errors = []
        mock_analysis_results.warnings = []
        
        with patch.object(orchestrator.analysis_orchestrator, 'analyze_rtl_module', return_value=mock_analysis_results):
            # Execute complete generation
            result = await orchestrator.orchestrate_complete_generation(self.rtl_module)
            
            # Verify result
            assert isinstance(result.rtl_module, RTLModule)
            assert result.execution_time > 0
    
    def test_workflow_customization_and_execution(self):
        """Test custom workflow creation and execution."""
        from brainsmith.tools.hw_kernel_gen.orchestration.workflow_definitions import CustomWorkflowBuilder
        
        # Create custom workflow using builder
        workflow = (CustomWorkflowBuilder("custom_test_workflow", "Test custom workflow")
                   .add_analysis_step("custom_analysis")
                   .add_generation_step("hw_custom_op_generator", "hw_gen", dependencies={"custom_analysis"})
                   .add_validation_step("final_validation", dependencies={"hw_gen"})
                   .set_global_config({"custom_mode": True})
                   .build())
        
        # Verify workflow structure
        assert workflow.name == "custom_test_workflow"
        assert len(workflow.steps) == 3
        assert workflow.global_config["custom_mode"] == True
        
        # Validate workflow
        validation_errors = workflow.validate()
        assert len(validation_errors) == 0
    
    def test_performance_monitoring_integration(self):
        """Test performance monitoring across all components."""
        factory = GeneratorFactory(self.config)
        
        # Register mock generator
        factory.registry.register_generator(
            "perf_test_gen",
            MockEnhancedGenerator,
            {GeneratorCapability.HW_CUSTOM_OP}
        )
        
        # Test performance tracking
        config = GeneratorConfiguration(
            generator_type=GeneratorType.AUTO_HW_CUSTOM_OP,
            config=self.config,
            capabilities_required={GeneratorCapability.HW_CUSTOM_OP}
        )
        
        # Create and use generator multiple times
        start_time = time.time()
        for i in range(3):
            generator = factory.create_generator(config, self.rtl_module)
            inputs = {"rtl_module": self.rtl_module}
            result = generator.generate(inputs)
            assert result.success
        
        total_time = time.time() - start_time
        
        # Verify performance statistics
        stats = factory.get_factory_statistics()
        factory_stats = stats["factory_stats"]
        
        assert factory_stats["creations"] >= 1  # At least one creation (others cached)
        assert factory_stats["creation_time"] > 0
        assert factory_stats["selection_time"] > 0
        
        # Verify registry statistics
        registry_stats = stats["registry_stats"]
        assert registry_stats["total_generators"] >= 1
        assert registry_stats["lookups"] >= 3  # One per generator request


if __name__ == "__main__":
    pytest.main([__file__, "-v", "--tb=short"])