# ðŸš€ Getting Started with Brainsmith
## Comprehensive Guide for New Users

---

## ðŸ“‹ Table of Contents

1. [Installation and Setup](#-installation-and-setup)
2. [First Steps](#-first-steps)
3. [Basic Usage Examples](#-basic-usage-examples)
4. [Advanced Workflows](#-advanced-workflows)
5. [API Reference](#-api-reference)
6. [Troubleshooting](#-troubleshooting)
7. [Best Practices](#-best-practices)

---

## ðŸ”§ Installation and Setup

### Prerequisites

Before installing Brainsmith, ensure you have the following prerequisites:

#### System Requirements
- **Operating System**: Linux (Ubuntu 18.04+, CentOS 7+) or WSL2 on Windows
- **Python**: 3.8 or higher
- **Memory**: Minimum 8GB RAM (16GB+ recommended for large models)
- **Storage**: 50GB+ free space for tools and builds

#### Required Dependencies
```bash
# Core Python packages
pip install numpy>=1.19.0
pip install pyyaml>=5.4.0
pip install packaging>=20.0

# Optional but recommended for advanced features
pip install scipy>=1.7.0          # For Latin Hypercube sampling
pip install scikit-learn>=1.0.0   # For Gaussian Process models
pip install matplotlib>=3.3.0     # For visualization
```

#### FPGA Tool Dependencies
- **Xilinx Vivado**: 2020.1 or later (for synthesis and implementation)
- **FINN Framework**: Latest version (automatically configured by Brainsmith)

### Installation Methods

#### Method 1: Development Installation (Recommended)
```bash
# Clone the repository
git clone https://github.com/your-org/brainsmith-1.git
cd brainsmith-1

# Install in development mode
pip install -e .

# Verify installation
python -c "import brainsmith; print('Brainsmith installed successfully')"
```

#### Method 2: Package Installation (Future)
```bash
# When available on PyPI
pip install brainsmith

# Verify installation
brainsmith --version
```

### Environment Configuration

#### Set up environment variables:
```bash
# Add to ~/.bashrc or ~/.zshrc
export BRAINSMITH_ROOT=/path/to/brainsmith-1
export PYTHONPATH=$BRAINSMITH_ROOT:$PYTHONPATH

# FPGA tool paths (adjust according to your installation)
export VIVADO_ROOT=/tools/Xilinx/Vivado/2020.1
export PATH=$VIVADO_ROOT/bin:$PATH
```

#### Verify FPGA tools:
```bash
# Test Vivado installation
vivado -version

# Test FINN availability (will be configured automatically)
python -c "import finn; print('FINN available')"
```

---

## ðŸŽ¯ First Steps

### Basic Platform Test

Create a simple test to verify your installation:

```python
# test_installation.py
import brainsmith
from brainsmith.core.design_space import DesignSpace, ParameterDefinition, ParameterType
from brainsmith.core.config import DSEConfig

def test_basic_functionality():
    """Test basic platform functionality."""
    
    # Create a simple design space
    design_space = DesignSpace("test_space")
    
    # Add a parameter
    param = ParameterDefinition(
        "test_param", 
        ParameterType.INTEGER, 
        range_min=1, 
        range_max=10
    )
    design_space.add_parameter(param)
    
    # Create DSE configuration
    dse_config = DSEConfig(
        strategy="random",
        max_evaluations=5
    )
    
    print(f"âœ… Design space created: {design_space.name}")
    print(f"âœ… Parameters: {len(design_space.parameters)}")
    print(f"âœ… DSE strategy: {dse_config.strategy}")
    print("ðŸŽ‰ Installation test successful!")

if __name__ == "__main__":
    test_basic_functionality()
```

Run the test:
```bash
python test_installation.py
```

### Understanding the Directory Structure

```
brainsmith-1/
â”œâ”€â”€ brainsmith/                 # Main package
â”‚   â”œâ”€â”€ core/                  # Core platform components
â”‚   â”œâ”€â”€ dse/                   # Design space exploration
â”‚   â”œâ”€â”€ libraries/             # Library ecosystem
â”‚   â”œâ”€â”€ blueprints/            # Blueprint system
â”‚   â””â”€â”€ tools/                 # Utility tools
â”œâ”€â”€ docs/                      # Documentation
â”‚   â”œâ”€â”€ architecture/          # Architecture guides
â”‚   â”œâ”€â”€ examples/              # Usage examples
â”‚   â””â”€â”€ tutorials/             # Step-by-step tutorials
â”œâ”€â”€ demos/                     # Complete demo projects
â”œâ”€â”€ tests/                     # Test suites
â””â”€â”€ examples/                  # Simple examples
```

---

## ðŸ’¡ Basic Usage Examples

### Example 1: Simple Design Space Exploration

```python
# simple_dse_example.py
import brainsmith
from brainsmith.core.design_space import DesignSpace, ParameterDefinition, ParameterType
from brainsmith.core.config import DSEConfig
from brainsmith.dse.simple import SimpleDSEEngine

def simple_dse_example():
    """Demonstrate basic design space exploration."""
    
    # Step 1: Define design space
    design_space = DesignSpace("simple_fpga_design")
    
    # Add optimization parameters
    design_space.add_parameter(
        ParameterDefinition("pe_count", ParameterType.INTEGER, range_min=2, range_max=16)
    )
    design_space.add_parameter(
        ParameterDefinition("frequency_mhz", ParameterType.FLOAT, range_min=100.0, range_max=300.0)
    )
    design_space.add_parameter(
        ParameterDefinition("memory_type", ParameterType.CATEGORICAL, values=["DDR", "HBM"])
    )
    
    # Step 2: Configure exploration
    dse_config = DSEConfig(
        strategy="random",
        max_evaluations=20,
        objectives=["performance", "power"],
        random_seed=42
    )
    
    # Step 3: Create DSE engine
    dse_engine = SimpleDSEEngine("random", design_space, dse_config)
    
    # Step 4: Run exploration loop
    results = []
    for iteration in range(dse_config.max_evaluations):
        # Get design point suggestions
        suggested_points = dse_engine.suggest(n_points=1)
        
        for point in suggested_points:
            # Simulate evaluation (replace with actual FPGA build)
            mock_results = simulate_fpga_evaluation(point)
            
            # Update engine with results
            dse_engine.update(point, mock_results)
            results.append((point, mock_results))
            
            print(f"Iteration {iteration}: PE={point.parameters['pe_count']}, "
                  f"Freq={point.parameters['frequency_mhz']:.1f}MHz, "
                  f"Performance={mock_results['performance']:.1f}")
    
    # Step 5: Analyze results
    best_result = max(results, key=lambda x: x[1]['performance'])
    print(f"\nðŸ† Best configuration:")
    print(f"   Parameters: {best_result[0].parameters}")
    print(f"   Performance: {best_result[1]['performance']:.2f}")

def simulate_fpga_evaluation(design_point):
    """Simulate FPGA evaluation (replace with real implementation)."""
    import random
    
    # Mock performance based on parameters
    pe_count = design_point.parameters['pe_count']
    frequency = design_point.parameters['frequency_mhz']
    
    # Simple performance model
    performance = pe_count * frequency * random.uniform(0.8, 1.2)
    power = pe_count * 0.5 + frequency * 0.01 + random.uniform(-1, 1)
    
    return {
        'performance': performance,
        'power': power,
        'success': True
    }

if __name__ == "__main__":
    simple_dse_example()
```

### Example 2: Blueprint-Based Configuration

Create a blueprint file:

```yaml
# example_blueprint.yaml
name: "cnn_edge_optimization"
description: "CNN optimization for edge deployment"
version: "1.0"

model:
  name: "mobilenet_v2"
  type: "cnn"
  source:
    format: "onnx"
    path: "./models/mobilenet_v2.onnx"

targets:
  performance:
    throughput_ops_sec: 500000
    latency_ms: 20
  resources:
    max_lut_utilization: 0.75
  power:
    max_total_power_w: 5.0

design_space:
  parameters:
    pe_count:
      type: integer
      range: [4, 16]
      default: 8
    simd_factor:
      type: integer
      range: [1, 4]
      default: 2
    memory_mode:
      type: categorical
      values: ["internal", "external"]
      default: "external"

dse:
  strategy: "adaptive"
  max_evaluations: 50
  objectives:
    - name: "throughput_ops_sec"
      direction: "maximize"
      weight: 0.6
    - name: "power_efficiency"
      direction: "maximize"
      weight: 0.4

libraries:
  transforms:
    enabled: true
    pipeline: ["quantization", "folding", "streamlining"]
  hw_optim:
    enabled: true
    strategy: "genetic"
  analysis:
    enabled: true
```

Use the blueprint:

```python
# blueprint_example.py
import brainsmith
from brainsmith.blueprints.manager import BlueprintManager
from brainsmith.core.api import brainsmith_explore

def blueprint_example():
    """Demonstrate blueprint-based workflow."""
    
    # Step 1: Load blueprint
    blueprint_manager = BlueprintManager()
    blueprint = blueprint_manager.load_blueprint("example_blueprint.yaml")
    
    print(f"ðŸ“‹ Loaded blueprint: {blueprint.name}")
    print(f"ðŸ“Š Design space parameters: {len(blueprint.design_space.parameters)}")
    
    # Step 2: Run optimization using enhanced API
    result = brainsmith_explore(
        blueprint=blueprint,
        output_dir="./build/cnn_optimization"
    )
    
    # Step 3: Analyze results
    if result.success:
        print(f"âœ… Optimization completed successfully")
        print(f"ðŸ“Š Total evaluations: {len(result.dse_results.results)}")
        print(f"ðŸ† Best configurations found: {len(result.dse_results.pareto_frontier)}")
    else:
        print(f"âŒ Optimization failed: {result.errors}")

if __name__ == "__main__":
    blueprint_example()
```

### Example 3: Library Integration

```python
# library_integration_example.py
from brainsmith.libraries.transforms.library import TransformsLibrary
from brainsmith.libraries.hw_optim.library import HwOptimLibrary
from brainsmith.libraries.analysis.library import AnalysisLibrary

def library_integration_example():
    """Demonstrate library ecosystem usage."""
    
    # Step 1: Initialize libraries
    transforms_lib = TransformsLibrary()
    hw_optim_lib = HwOptimLibrary()
    analysis_lib = AnalysisLibrary()
    
    print("ðŸ“š Libraries initialized:")
    print(f"   Transforms: {len(transforms_lib.get_capabilities())} capabilities")
    print(f"   HW Optimization: {len(hw_optim_lib.get_capabilities())} capabilities")
    print(f"   Analysis: {len(analysis_lib.get_capabilities())} capabilities")
    
    # Step 2: Configure transformation pipeline
    model_config = {
        "model_type": "cnn",
        "layers": 50,
        "quantization": "INT8"
    }
    
    pipeline_id = transforms_lib.configure_pipeline(
        model_config, 
        ["quantization", "folding", "streamlining"]
    )
    
    print(f"ðŸ”„ Transform pipeline configured: {pipeline_id}")
    
    # Step 3: Run hardware optimization
    initial_design = {
        "pe_count": 8,
        "simd_factor": 4,
        "memory_mode": "external"
    }
    
    optimization_result = hw_optim_lib.optimize_design(
        initial_design,
        strategy="genetic",
        objectives=["performance", "resources"],
        max_generations=20
    )
    
    print(f"âš™ï¸ Optimization completed: {len(optimization_result['solutions'])} solutions")
    print(f"ðŸ“ˆ Pareto frontier: {len(optimization_result['pareto_front'])} points")
    
    # Step 4: Analyze best solution
    best_solution = optimization_result['pareto_front'][0]
    
    analysis_result = analysis_lib.analyze_implementation({
        "design_parameters": best_solution,
        "performance_data": {
            "ops_per_sec": 1e6,
            "memory_bandwidth_gbps": 100,
            "arithmetic_intensity": 2.0
        },
        "resource_data": {
            "luts": 32000,
            "dsps": 120,
            "brams": 30
        }
    })
    
    print(f"ðŸ“Š Analysis completed: {len(analysis_result['categories'])} analysis types")
    
    # Step 5: Generate report
    try:
        report = analysis_lib.generate_report(analysis_result, format_type="html")
        print(f"ðŸ“‹ Report generated ({len(report)} characters)")
    except Exception as e:
        print(f"âš ï¸ Report generation failed: {e}")

if __name__ == "__main__":
    library_integration_example()
```

---

## ðŸ”¬ Advanced Workflows

### Multi-Objective Optimization with Pareto Analysis

```python
# advanced_optimization.py
import brainsmith
from brainsmith.core.design_space import DesignSpace, ParameterDefinition, ParameterType
from brainsmith.core.config import DSEConfig
from brainsmith.dse.simple import SimpleDSEEngine
from brainsmith.dse.analysis import ParetoAnalyzer

def advanced_optimization_example():
    """Demonstrate advanced multi-objective optimization."""
    
    # Create complex design space
    design_space = DesignSpace("advanced_fpga_design")
    
    # Multiple optimization parameters
    parameters = [
        ("pe_count", ParameterType.INTEGER, {"range_min": 4, "range_max": 32}),
        ("simd_factor", ParameterType.INTEGER, {"range_min": 1, "range_max": 8}),
        ("memory_hierarchy", ParameterType.CATEGORICAL, {"values": ["L1", "L2", "L3"]}),
        ("clock_frequency", ParameterType.FLOAT, {"range_min": 150.0, "range_max": 400.0}),
        ("pipeline_depth", ParameterType.INTEGER, {"range_min": 3, "range_max": 10}),
        ("quantization", ParameterType.CATEGORICAL, {"values": ["INT8", "INT16", "FP16"]})
    ]
    
    for param_name, param_type, kwargs in parameters:
        param_def = ParameterDefinition(param_name, param_type, **kwargs)
        design_space.add_parameter(param_def)
    
    # Multi-objective configuration
    dse_config = DSEConfig(
        strategy="genetic",  # Better for multi-objective
        max_evaluations=100,
        objectives=["throughput_ops_sec", "power_efficiency", "resource_efficiency"],
        objective_directions=["maximize", "maximize", "maximize"],
        random_seed=42
    )
    
    # Run optimization
    results = run_multi_objective_optimization(design_space, dse_config)
    
    # Pareto analysis
    pareto_analyzer = ParetoAnalyzer()
    pareto_frontier = pareto_analyzer.compute_pareto_frontier(
        results, 
        dse_config.objectives,
        dse_config.objective_directions
    )
    
    print(f"ðŸŽ¯ Multi-objective optimization completed")
    print(f"ðŸ“Š Total evaluations: {len(results)}")
    print(f"ðŸ† Pareto frontier size: {len(pareto_frontier)}")
    
    # Analyze trade-offs
    trade_off_analysis = pareto_analyzer.analyze_trade_offs(
        pareto_frontier, 
        dse_config.objectives
    )
    
    print(f"ðŸ“ˆ Trade-off analysis:")
    print(f"   Diversity: {trade_off_analysis['diversity']:.3f}")
    print(f"   Trade-offs identified: {len(trade_off_analysis['trade_offs'])}")
    
    # Show best solutions for each objective
    for i, objective in enumerate(dse_config.objectives):
        best_point = max(pareto_frontier, key=lambda p: p.objectives.get(objective, 0))
        print(f"\nðŸ¥‡ Best {objective}:")
        print(f"   Value: {best_point.objectives.get(objective, 0):.2f}")
        print(f"   Parameters: {best_point.parameters}")

def run_multi_objective_optimization(design_space, dse_config):
    """Run the optimization and return results."""
    # Implementation would create DSE engine and run optimization
    # For brevity, returning mock results
    results = []
    # ... optimization implementation
    return results
```

### Custom DSE Strategy Integration

```python
# custom_strategy_example.py
from brainsmith.dse.interface import DSEEngine
from brainsmith.core.design_space import DesignSpace, DesignPoint
from brainsmith.core.config import DSEConfig
import numpy as np

class CustomOptimizationStrategy(DSEEngine):
    """Custom optimization strategy example."""
    
    def __init__(self, design_space: DesignSpace, config: DSEConfig):
        super().__init__("custom_strategy", design_space, config)
        self.evaluated_points = []
        self.best_points = []
        
    def suggest(self, n_points: int = 1) -> List[DesignPoint]:
        """Custom suggestion algorithm."""
        points = []
        
        if len(self.evaluated_points) < 5:
            # Initial exploration phase
            points = self._random_exploration(n_points)
        else:
            # Exploitation phase based on best points
            points = self._local_search(n_points)
        
        return points
    
    def _random_exploration(self, n_points: int) -> List[DesignPoint]:
        """Random exploration for initial phase."""
        points = []
        for _ in range(n_points):
            point_params = {}
            for param_name, param_def in self.design_space.parameters.items():
                if param_def.type == ParameterType.INTEGER:
                    value = np.random.randint(param_def.range_min, param_def.range_max + 1)
                elif param_def.type == ParameterType.FLOAT:
                    value = np.random.uniform(param_def.range_min, param_def.range_max)
                elif param_def.type == ParameterType.CATEGORICAL:
                    value = np.random.choice(param_def.values)
                point_params[param_name] = value
            
            points.append(DesignPoint(point_params))
        return points
    
    def _local_search(self, n_points: int) -> List[DesignPoint]:
        """Local search around best points."""
        if not self.best_points:
            return self._random_exploration(n_points)
        
        points = []
        for _ in range(n_points):
            # Select best point as baseline
            base_point = np.random.choice(self.best_points)
            
            # Create variation
            new_params = base_point.parameters.copy()
            
            # Mutate one parameter
            param_name = np.random.choice(list(new_params.keys()))
            param_def = self.design_space.parameters[param_name]
            
            if param_def.type == ParameterType.INTEGER:
                mutation_range = max(1, (param_def.range_max - param_def.range_min) // 10)
                delta = np.random.randint(-mutation_range, mutation_range + 1)
                new_value = np.clip(
                    new_params[param_name] + delta,
                    param_def.range_min, 
                    param_def.range_max
                )
                new_params[param_name] = new_value
            
            points.append(DesignPoint(new_params))
        
        return points
    
    def update(self, point: DesignPoint, results: Dict[str, Any]):
        """Update strategy with evaluation results."""
        point.results.update(results)
        self.evaluated_points.append(point)
        
        # Update best points based on primary objective
        primary_obj = self.config.objectives[0]
        if primary_obj in results:
            point.set_objective(primary_obj, results[primary_obj])
            
            # Maintain top 10 best points
            self.best_points.append(point)
            self.best_points.sort(
                key=lambda p: p.objectives.get(primary_obj, 0), 
                reverse=True
            )
            self.best_points = self.best_points[:10]
    
    def is_converged(self) -> bool:
        """Check convergence based on improvement rate."""
        if len(self.evaluated_points) < self.config.max_evaluations:
            return False
        
        # Check if recent evaluations show improvement
        recent_count = min(20, len(self.evaluated_points) // 4)
        if recent_count < 5:
            return False
        
        recent_points = self.evaluated_points[-recent_count:]
        older_points = self.evaluated_points[-(recent_count*2):-recent_count]
        
        if not older_points:
            return False
        
        primary_obj = self.config.objectives[0]
        recent_avg = np.mean([p.objectives.get(primary_obj, 0) for p in recent_points])
        older_avg = np.mean([p.objectives.get(primary_obj, 0) for p in older_points])
        
        improvement = (recent_avg - older_avg) / (older_avg + 1e-8)
        return improvement < 0.01  # Less than 1% improvement

def custom_strategy_example():
    """Demonstrate custom strategy usage."""
    
    # Create design space
    design_space = DesignSpace("custom_strategy_test")
    design_space.add_parameter(
        ParameterDefinition("param1", ParameterType.INTEGER, range_min=1, range_max=20)
    )
    design_space.add_parameter(
        ParameterDefinition("param2", ParameterType.FLOAT, range_min=0.1, range_max=2.0)
    )
    
    # Configure DSE
    config = DSEConfig(
        strategy="custom",
        max_evaluations=50,
        objectives=["performance"]
    )
    
    # Use custom strategy
    strategy = CustomOptimizationStrategy(design_space, config)
    
    # Run optimization
    for iteration in range(config.max_evaluations):
        points = strategy.suggest(1)
        for point in points:
            # Mock evaluation
            results = {"performance": point.parameters["param1"] * point.parameters["param2"]}
            strategy.update(point, results)
        
        if strategy.is_converged():
            print(f"ðŸŽ¯ Converged after {iteration + 1} iterations")
            break
    
    print(f"ðŸ† Best result: {max(strategy.best_points, key=lambda p: p.objectives['performance']).objectives['performance']:.2f}")
```

---

## ðŸ“– API Reference

### Core API Functions

#### Enhanced API
```python
def brainsmith_explore(
    blueprint: Union[str, Blueprint] = None,
    model_path: str = None,
    config: CompilerConfig = None,
    design_space: DesignSpace = None,
    dse_config: DSEConfig = None,
    output_dir: str = "./build",
    **kwargs
) -> BrainsmithResult:
    """
    Enhanced Brainsmith exploration API.
    
    Args:
        blueprint: Blueprint file path or Blueprint object
        model_path: Path to model file (ONNX, PyTorch, etc.)
        config: Compiler configuration object
        design_space: Design space specification
        dse_config: DSE configuration
        output_dir: Output directory for results
        **kwargs: Additional configuration parameters
    
    Returns:
        BrainsmithResult: Comprehensive result object
    """
```

#### Legacy API
```python
def explore_design_space(
    model_path: str,
    blueprint_name: str = "default",
    output_dir: str = "./build",
    target_fps: int = 3000,
    board: str = "V80",
    **kwargs
) -> Any:
    """
    Legacy API for backward compatibility.
    
    Args:
        model_path: Path to model file
        blueprint_name: Blueprint identifier
        output_dir: Output directory
        target_fps: Target frames per second
        board: Target FPGA board
        **kwargs: Additional legacy parameters
    
    Returns:
        Legacy result format
    """
```

### Configuration Classes

#### DSEConfig
```python
@dataclass
class DSEConfig:
    strategy: str = "random"
    max_evaluations: int = 50
    objectives: List[str] = field(default_factory=lambda: ["throughput_ops_sec"])
    objective_directions: List[str] = field(default_factory=lambda: ["maximize"])
    random_seed: Optional[int] = None
    external_tool_interface: Optional[str] = None
    early_stopping: bool = False
    convergence_patience: int = 10
    min_improvement: float = 0.01
```

#### ParameterDefinition
```python
class ParameterDefinition:
    def __init__(self, name: str, param_type: ParameterType, 
                 range_min: float = None, range_max: float = None,
                 values: List[Any] = None, default: Any = None):
        """
        Define an optimization parameter.
        
        Args:
            name: Parameter name
            param_type: Type (INTEGER, FLOAT, CATEGORICAL, BOOLEAN)
            range_min: Minimum value (for numeric types)
            range_max: Maximum value (for numeric types)
            values: Valid values (for categorical type)
            default: Default value
        """
```

---

## ðŸ”§ Troubleshooting

### Common Issues and Solutions

#### Installation Issues

**Problem**: Import errors after installation
```
ImportError: No module named 'brainsmith'
```

**Solution**:
```bash
# Check Python path
echo $PYTHONPATH

# Reinstall in development mode
pip uninstall brainsmith
pip install -e .

# Verify installation
python -c "import brainsmith; print(brainsmith.__file__)"
```

**Problem**: FINN dependency issues
```
ModuleNotFoundError: No module named 'finn'
```

**Solution**:
```bash
# FINN will be automatically configured
# Ensure you have internet access for first run
python -c "import brainsmith; brainsmith.verify_dependencies()"
```

#### Runtime Issues

**Problem**: Blueprint validation errors
```
BlueprintValidationError: Blueprint validation failed
```

**Solution**:
```python
# Enable detailed validation
from brainsmith.blueprints.manager import BlueprintManager

manager = BlueprintManager()
try:
    blueprint = manager.load_blueprint("your_blueprint.yaml")
except Exception as e:
    print(f"Validation details: {e}")
    # Check blueprint syntax and required fields
```

**Problem**: DSE engine creation failures
```
ValueError: Unknown optimization strategy: 'invalid_strategy'
```

**Solution**:
```python
# Check available strategies
from brainsmith.dse.strategies import get_available_strategies

strategies = get_available_strategies()
print(f"Available strategies: {list(strategies.keys())}")

# Use valid strategy name
dse_config = DSEConfig(strategy="random")  # or "adaptive", "genetic", etc.
```

#### Performance Issues

**Problem**: Slow optimization convergence

**Solution**:
- Reduce design space size
- Use appropriate strategy for problem size
- Increase evaluation budget
- Check parameter ranges for realism

```python
# Strategy selection guidance
def select_strategy(param_count, evaluation_budget, objective_count):
    if param_count <= 3 and evaluation_budget >= 100:
        return "grid"
    elif param_count <= 8 and objective_count == 1:
        return "adaptive"
    elif objective_count > 1:
        return "genetic"
    else:
        return "latin_hypercube"
```

### Debug Mode

Enable detailed logging:
```python
import logging
logging.basicConfig(level=logging.DEBUG)

# Enable Brainsmith debug mode
import brainsmith
brainsmith.set_debug_mode(True)
```

### Getting Help

1. **Documentation**: Check `/docs/` directory for detailed guides
2. **Examples**: See `/examples/` and `/demos/` for working code
3. **Issues**: Report bugs and request features on GitHub
4. **Community**: Join discussions and get support

---

## âœ¨ Best Practices

### Design Space Definition

1. **Start Small**: Begin with 2-3 parameters, expand gradually
2. **Realistic Ranges**: Use parameter ranges based on actual hardware constraints
3. **Meaningful Constraints**: Add constraints that reflect real design limitations
4. **Objective Selection**: Choose objectives that truly matter for your application

### Optimization Strategy Selection

1. **Problem Size Matters**:
   - Small spaces (â‰¤3 params): Grid search or random
   - Medium spaces (4-8 params): Adaptive or genetic
   - Large spaces (>8 params): Latin hypercube or genetic

2. **Budget Considerations**:
   - Low budget (<50 evals): Latin hypercube
   - Medium budget (50-200 evals): Adaptive or genetic  
   - High budget (>200 evals): Any strategy

3. **Multi-objective**: Always use genetic algorithm for >1 objective

### Blueprint Organization

1. **Use Templates**: Create reusable templates for model families
2. **Version Control**: Keep blueprints in version control
3. **Modular Design**: Split complex blueprints into composable parts
4. **Documentation**: Add detailed descriptions and comments

### Performance Optimization

1. **Parallel Builds**: Use `parallel_builds` parameter for multiple evaluations
2. **Early Stopping**: Enable convergence detection for automatic stopping
3. **Caching**: Reuse evaluations when possible
4. **Resource Management**: Monitor disk space and memory usage

### Example Best Practice Blueprint

```yaml
# best_practices_example.yaml
name: "production_cnn_optimization"
description: "Production-ready CNN optimization with best practices"
version: "2.1"

# Use template inheritance
extends: "templates/cnn_base.yaml"

# Clear, documented parameters
design_space:
  parameters:
    pe_count:
      type: integer
      range: [8, 32]  # Based on target board resources
      default: 16
      description: "Processing elements (2-4x expected optimal)"
      
    memory_bandwidth:
      type: categorical
      values: ["standard", "high_bandwidth"]  # Clear semantic choices
      default: "standard"
      description: "Memory subsystem configuration"

# Realistic objectives with priorities
dse:
  strategy: "genetic"  # Multi-objective optimization
  max_evaluations: 100  # Sufficient budget for convergence
  objectives:
    - name: "throughput_ops_sec"
      direction: "maximize"
      weight: 0.5  # Primary objective
    - name: "power_efficiency"
      direction: "maximize"
      weight: 0.3  # Secondary objective
    - name: "resource_efficiency"
      direction: "maximize"
      weight: 0.2  # Tertiary objective
  
  convergence:
    enabled: true
    patience: 15  # Allow sufficient convergence time
    min_improvement: 0.02  # 2% improvement threshold

# Production settings
build:
  parallel_builds: 4  # Utilize available cores
  verification_enabled: true  # Always verify in production
  synthesis_enabled: true  # Full implementation
  
reporting:
  formats: ["html", "json"]  # Multiple formats for different users
  comparative_analysis: true  # Enable trend analysis
  research_export: true  # Support reproducibility
```

---

## ðŸŽ¯ Next Steps

### Learning Path

1. **Beginner**: Start with simple DSE examples
2. **Intermediate**: Create custom blueprints and use library ecosystem
3. **Advanced**: Develop custom strategies and integrate external tools
4. **Expert**: Contribute new libraries and optimization algorithms

### Additional Resources

- **Examples Directory**: `/examples/` - Simple, focused examples
- **Demos Directory**: `/demos/` - Complete, real-world projects
- **Architecture Documentation**: `/docs/architecture/` - Deep technical details
- **API Documentation**: Generated API docs (when available)

### Community Contribution

- **Bug Reports**: Help improve platform stability
- **Feature Requests**: Suggest new capabilities
- **Library Development**: Create specialized libraries
- **Documentation**: Improve guides and examples

---

*Congratulations! You're now ready to use Brainsmith for advanced FPGA accelerator design and optimization.* ðŸš€

---

## ðŸ“š Complete Architecture Guide Index

1. [Platform Overview](01_PLATFORM_OVERVIEW.md) - High-level introduction and capabilities
2. [Architecture Fundamentals](02_ARCHITECTURE_FUNDAMENTALS.md) - Core design principles and system architecture
3. [Core Components](03_CORE_COMPONENTS.md) - Detailed component architecture and implementation
4. [Library Ecosystem](04_LIBRARY_ECOSYSTEM.md) - Extensible library architecture
5. [Design Space Exploration](05_DESIGN_SPACE_EXPLORATION.md) - Advanced optimization framework
6. [Blueprint System](06_BLUEPRINT_SYSTEM.md) - Configuration-driven design framework
7. [Getting Started Guide](07_GETTING_STARTED.md) - Comprehensive user guide (this document)