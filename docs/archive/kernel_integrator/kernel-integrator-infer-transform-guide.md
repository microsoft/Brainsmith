# Kernel Integrator: Infer Transform Implementation Guide

This guide provides detailed examples and patterns for implementing the `infer_transform.py` file generated by the kernel integrator.

## Overview

The infer transform is responsible for converting ONNX graph nodes into your custom HWCustomOp during the compilation flow. It bridges between the high-level ML framework representation and your hardware-optimized kernel.

## Key Methods to Implement

### 1. `matches_node()` - Node Pattern Matching

This method determines which ONNX nodes should be converted to your kernel.

#### Example Patterns

**Single Op Type Matching:**
```python
def matches_node(self, model: ModelWrapper, node: NodeProto) -> bool:
    return node.op_type == "Conv"
```

**Multiple Op Types:**
```python
def matches_node(self, model: ModelWrapper, node: NodeProto) -> bool:
    return node.op_type in ["Conv", "ConvTranspose", "DepthwiseConv"]
```

**With Attribute Checking:**
```python
def matches_node(self, model: ModelWrapper, node: NodeProto) -> bool:
    if node.op_type == "Conv":
        kernel_shape = get_by_name(node.attribute, "kernel_shape")
        # Only match 3x3 convolutions
        return kernel_shape and list(kernel_shape.ints) == [3, 3]
    return False
```

**Custom QONNX Ops:**
```python
def matches_node(self, model: ModelWrapper, node: NodeProto) -> bool:
    return node.op_type == "MultiThreshold"
```

**Complex Pattern Matching:**
```python
def matches_node(self, model: ModelWrapper, node: NodeProto) -> bool:
    if node.op_type != "MatMul":
        return False
    
    # Check if followed by activation
    consumers = model.find_consumers(node.output[0])
    if consumers and consumers[0].op_type in ["Relu", "Sigmoid"]:
        return True
    return False
```

### 2. `can_convert_node()` - Additional Validation

This method performs deeper validation beyond basic pattern matching.

#### Common Validation Patterns

**Check Tensor Shapes:**
```python
def can_convert_node(self, model: ModelWrapper, node: NodeProto) -> bool:
    if not self.validate_integer_datatypes(model, node):
        return False
    
    # Ensure 4D tensors (NCHW or NHWC)
    input_shape = model.get_tensor_shape(node.input[0])
    if len(input_shape) != 4:
        return False
    
    # Check for square kernels
    if node.op_type == "Conv":
        kernel_shape = get_by_name(node.attribute, "kernel_shape").ints
        if kernel_shape[0] != kernel_shape[1]:
            return False
    
    return True
```

**Verify Static Weights:**
```python
def can_convert_node(self, model: ModelWrapper, node: NodeProto) -> bool:
    # For operations with weights, ensure they're available as initializers
    if len(node.input) > 1:
        weight_tensor = model.get_initializer(node.input[1])
        if weight_tensor is None:
            return False
    return True
```

### 3. `extract_node_attributes()` - Parameter Extraction

This method maps ONNX node attributes and context to your kernel's parameters.

#### Attribute Extraction Examples

**From Node Attributes:**
```python
def extract_node_attributes(self, model: ModelWrapper, node: NodeProto) -> Dict[str, Any]:
    attrs = {}
    
    # Direct attribute extraction
    attrs["stride"] = list(get_by_name(node.attribute, "strides").ints)
    attrs["padding"] = list(get_by_name(node.attribute, "pads").ints)
    attrs["dilation"] = list(get_by_name(node.attribute, "dilations").ints)
    
    return attrs
```

**From Tensor Shapes:**
```python
def extract_node_attributes(self, model: ModelWrapper, node: NodeProto) -> Dict[str, Any]:
    attrs = {}
    
    # Extract dimensions from input tensor
    input_shape = model.get_tensor_shape(node.input[0])
    attrs["batch_size"] = input_shape[0]
    attrs["height"] = input_shape[1]
    attrs["width"] = input_shape[2]
    attrs["in_channels"] = input_shape[3]  # Assuming NHWC
    
    # Extract from weight tensor
    if len(node.input) > 1:
        weight_shape = model.get_tensor_shape(node.input[1])
        attrs["out_channels"] = weight_shape[0]
        attrs["kernel_size"] = weight_shape[2]  # Assuming square kernel
    
    return attrs
```

**Computing Derived Values:**
```python
def extract_node_attributes(self, model: ModelWrapper, node: NodeProto) -> Dict[str, Any]:
    attrs = {}
    
    # Compute output dimensions
    in_shape = model.get_tensor_shape(node.input[0])
    kernel_size = get_by_name(node.attribute, "kernel_shape").ints[0]
    stride = get_by_name(node.attribute, "strides").ints[0]
    padding = get_by_name(node.attribute, "pads").ints[0]
    
    out_dim = (in_shape[2] + 2*padding - kernel_size) // stride + 1
    attrs["output_size"] = out_dim
    
    # Set parallelism based on channel count
    attrs["PE"] = min(in_shape[3], 16)  # Process up to 16 channels in parallel
    attrs["SIMD"] = 1  # Start conservative
    
    return attrs
```

### 4. `create_basic_finn_attributes()` - Interface Mapping

Map ONNX tensors to your kernel's named interfaces.

```python
def create_basic_finn_attributes(self, model: ModelWrapper, node: NodeProto) -> Dict[str, Any]:
    attrs = {"backend": "fpgadataflow"}
    
    # Map specific tensors to named interfaces
    attrs["inputDataType"] = model.get_tensor_datatype(node.input[0]).name
    
    if len(node.input) > 1:
        attrs["weightDataType"] = model.get_tensor_datatype(node.input[1]).name
    
    if len(node.input) > 2:
        attrs["biasDataType"] = model.get_tensor_datatype(node.input[2]).name
    
    attrs["outputDataType"] = model.get_tensor_datatype(node.output[0]).name
    
    # Enable runtime weight updates if using AXI-Lite config
    attrs["runtime_writeable_weights"] = True
    
    return attrs
```

## Complete Example: MultiThreshold Implementation

Here's a complete example for converting QONNX MultiThreshold nodes:

```python
class InferThresholdingAxi(InferAutoHWCustomOp):
    
    def matches_node(self, model: ModelWrapper, node: NodeProto) -> bool:
        # Match MultiThreshold nodes
        return node.op_type == "MultiThreshold"
    
    def can_convert_node(self, model: ModelWrapper, node: NodeProto) -> bool:
        # Ensure integer datatypes
        if not self.validate_integer_datatypes(model, node):
            return False
        
        # Check threshold tensor is available
        if model.get_initializer(node.input[1]) is None:
            return False
        
        # Verify 2D threshold tensor
        thresholds_shape = model.get_tensor_shape(node.input[1])
        if len(thresholds_shape) != 2:
            return False
            
        return True
    
    def extract_node_attributes(self, model: ModelWrapper, node: NodeProto) -> Dict[str, Any]:
        attrs = {}
        
        # Get custom op instance for attribute access
        mt_inst = getCustomOp(node)
        
        # Extract algorithm parameters
        thresholds_shape = model.get_tensor_shape(node.input[1])
        attrs["num_steps"] = thresholds_shape[1]
        attrs["activation_value"] = int(mt_inst.get_nodeattr("out_bias"))
        
        # Extract shape information
        in_shape = model.get_tensor_shape(node.input[0])
        attrs["num_channels"] = in_shape[-1]  # Assuming NHWC
        attrs["numInputVectors"] = list(in_shape[:-1])
        
        # Set parallelism parameters
        attrs["PE"] = 1  # Processing elements
        attrs["SIMD"] = 1  # SIMD lanes
        
        return attrs
    
    def create_basic_finn_attributes(self, model: ModelWrapper, node: NodeProto) -> Dict[str, Any]:
        attrs = super().create_basic_finn_attributes(model, node)
        
        # Map interfaces to tensors
        attrs["inputDataType"] = model.get_tensor_datatype(node.input[0]).name
        attrs["thresholdDataType"] = model.get_tensor_datatype(node.input[1]).name  
        attrs["outputDataType"] = model.get_tensor_datatype(node.output[0]).name
        
        return attrs
```

## Shape and Tiling Parameters

Shape parameters control how data flows through your kernel:

### Block Dimensions (BDIM)
Define how data is tiled/blocked for processing:
- Set based on available on-chip memory
- Consider kernel computation patterns
- Balance between parallelism and resource usage

### Stream Dimensions (SDIM)
Define parallelism - elements processed per cycle:
- `SDIM = 1`: Minimal resources, sequential processing
- `SDIM = channels`: Full parallelism, maximum throughput
- `SDIM = power_of_2`: Balanced parallelism

Example:
```python
# Conservative settings
attrs["in_block_channels"] = 8    # Process 8 channels at a time
attrs["in_stream_channels"] = 1   # One channel per cycle

# Aggressive settings
attrs["in_block_channels"] = num_channels  # All channels in block
attrs["in_stream_channels"] = 16           # 16 channels per cycle
```

## Advanced Patterns

### Absorbing Adjacent Nodes

```python
def convert_to_auto_hw_custom_op(self, model: ModelWrapper, node: NodeProto) -> NodeProto:
    # Check if followed by activation
    consumer = model.find_consumer(node.output[0])
    if consumer and consumer.op_type == "Relu":
        # Create new node that outputs to activation's output
        new_node = super().convert_to_auto_hw_custom_op(model, node)
        new_node.output[0] = consumer.output[0]
        
        # Remember to remove the absorbed node in apply()
        self._absorbed_nodes.append(consumer.name)
        
        return new_node
    
    return super().convert_to_auto_hw_custom_op(model, node)
```

### Multi-Input/Output Handling

```python
def extract_node_attributes(self, model: ModelWrapper, node: NodeProto) -> Dict[str, Any]:
    attrs = {}
    
    # Handle multiple inputs
    for i, inp in enumerate(node.input):
        if model.get_initializer(inp):
            attrs[f"weight_{i}_shape"] = list(model.get_tensor_shape(inp))
        else:
            attrs[f"input_{i}_shape"] = list(model.get_tensor_shape(inp))
    
    # Handle multiple outputs
    for i, out in enumerate(node.output):
        attrs[f"output_{i}_shape"] = list(model.get_tensor_shape(out))
    
    return attrs
```

## Testing Your Implementation

1. **Start Simple**: Test with minimal graphs containing just your target node
2. **Verify Attributes**: Print extracted attributes to ensure correctness
3. **Check Datatypes**: Ensure all interfaces have proper datatype mappings
4. **Test Parallelism**: Start with PE=1, SIMD=1, then increase gradually
5. **Validate Shapes**: Ensure input/output shapes match expectations

## Common Issues and Solutions

### Issue: Node not being matched
- Check `matches_node()` logic
- Print node.op_type to verify
- Ensure graph has expected structure

### Issue: Conversion fails
- Check `can_convert_node()` validation
- Verify all required initializers exist
- Ensure integer datatypes

### Issue: Missing attributes
- Print all node attributes with `node.attribute`
- Check tensor shapes are available
- Verify attribute names match exactly

### Issue: Runtime errors
- Start with conservative parallelism
- Verify all required attributes are set
- Check interface name mapping