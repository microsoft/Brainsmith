# Multilayer Offload (MLO)

Multilayer Offload (MLO) is a powerful feature developed by the FINN and Brainsmith teams that enables the implementation of much larger neural networks by implementing a repeating slice of the model (such as a single transformer encoder layer) in hardware and cycling model weights through external memory (DRAM/HBM). This technique allows the acceleration of models that would otherwise be too large to fit on the FPGA.

## Overview


have stored all model weights on-chip in BRAM or UltraRAM, which severely limits the size of models that can be implemented. MLO overcomes this limitation by:

1. **Implementing a single repeating layer** (e.g., one transformer encoder) in hardware
2. **Storing weights off-chip** in high-bandwidth memory (HBM/DRAM)
3. **Streaming weights** into the accelerator as needed for each layer
4. **Reusing the same hardware** to process multiple layers sequentially

This approach trades some throughput for the ability to handle much larger models, making it ideal for large language models, vision transformers, and other deep architectures.

## How It Works

### Loop Body Hierarchy

MLO works by identifying a repeating structure in the neural network and implementing only that structure in hardware. **Currently, loop body discovery is not automated** - users must manually identify one iteration of the repeating pattern and specify it using the `loop_body_hierarchy` parameter:

```yaml
finn_config:
  loop_body_hierarchy: [['encoder', 'encoder.layer.0']]
```

**Manual Loop Body Identification:**
The `loop_body_hierarchy` configuration must match the hierarchical naming structure in your ONNX model, which corresponds to the `pkg.torch.onnx.name_scopes` field used during model export. The loop rolling transformation uses these name scopes to determine which levels of hierarchy to include in the loop body.

> **⚠️ Important:** You must use `dynamo=True` when exporting your PyTorch model to ONNX. Exporting with `dynamo=True` generates the metadata (name scopes) that MLO requires to identify repeating structures. Without this flag, the ONNX model will lack the hierarchical metadata needed for loop body discovery, and the MLO transformation will fail to locate the repeating patterns.

**Technical Implementation:**
The node extraction mechanism is implemented in FINN's loop rolling transformations:

- **Step Location**: `deps/finn/src/finn/builder/build_dataflow_steps.py` (line 984)
- **Extraction Process**: `deps/finn/src/finn/transformation/fpgadataflow/loop_rolling.py` (LoopExtraction class)
- **Hierarchy Matching**: `deps/finn/src/finn/util/onnxscript_helpers.py` (PytorchHierarchyNode class)

The extraction works by:
1. Creating a hierarchy parser from PyTorch metadata (`pkg.torch.onnx.name_scopes`)
2. Adding each ONNX node to the parser based on its hierarchy path
3. Using prefix matching to find all nodes under the specified hierarchy paths
4. Extracting matching nodes to create loop templates and removing originals from the main graph

This process requires the PyTorch exporter metadata generated by `dynamo=True`, which contains the module instance hierarchies that map ONNX nodes back to their originating PyTorch modules.

This configuration tells Brainsmith:
- Look for a repeating pattern called 'encoder' (top-level hierarchy)
- The repeating unit is 'encoder.layer.0' (one complete encoder layer)
- All encoder layers (layer.0, layer.1, layer.2, etc.) will be processed using the same hardware
- The name scopes must exactly match the ONNX node names for proper identification

#### Multiple Hierarchy Groups

For models with multiple independent repeating structures, you can specify multiple hierarchy groups in the `loop_body_hierarchy` configuration:

```yaml
finn_config:
  loop_body_hierarchy: [
    ['encoder', 'encoder.layer.0'],
    ['decoder', 'decoder.layer.0']
  ]
```

This advanced configuration enables MLO for models like:
- **Encoder-Decoder architectures** (e.g., T5, BART) with separate encoder and decoder stacks
- **Multi-tower models** with independent processing paths
- **Hierarchical models** with multiple levels of repeating structures

**Multiple Group Behavior:**
- Each group creates a separate loop rolling region
- Groups can have different numbers of layers (e.g., 12 encoder layers, 6 decoder layers)
- Weight streaming is managed independently for each group
- Hardware resources can be shared or dedicated per group depending on the implementation

**Example: T5 Model Configuration**
```yaml
finn_config:
  loop_body_hierarchy: [
    ['encoder', 'encoder.block.0'],        # T5 encoder blocks
    ['decoder', 'decoder.block.0']         # T5 decoder blocks
  ]
```

This tells Brainsmith to implement both the first encoder block and first decoder block in hardware, then reuse them for all subsequent blocks in their respective stacks.

#### Hierarchy Level Specification

The `loop_body_hierarchy` can specify multiple levels of hierarchy to precisely control what gets included in the loop body:

**Two-level hierarchy (simple case):**
```yaml
loop_body_hierarchy: [['encoder', 'encoder.layer.0']]
```
- Includes all nodes under `encoder.layer.0.*`
- Good for simple transformer architectures

**Three-level hierarchy (precise control):**
```yaml
loop_body_hierarchy: [
  ['bert', 'bert.encoder', 'bert.encoder.layer.0']
]
```
- Specifies the full path: model → encoder stack → specific layer
- Provides more precise control over node selection
- Useful for complex models with nested structures

The FINN loop rolling step will find all ONNX nodes whose names start with the final hierarchy level (e.g., `bert.encoder.layer.0`) and extract them as the loop body.

### Weight Streaming

Instead of storing all weights on-chip, MLO:
1. **Streams weights from HBM/DRAM** for each layer as needed
2. **Prefetches weights** for the next layer while processing the current one
3. **Manages weight buffers** to overlap computation and memory access
4. **Reuses computation hardware** across all layers

### Loop Rolling Process

The loop rolling transformation (`step_loop_rolling` in FINN) performs these key operations:

1. **Parses the `loop_body_hierarchy`** to identify which nodes belong to the repeating structure
2. **Extracts nodes by name scope matching** - finds all ONNX nodes whose names match the specified hierarchy pattern (e.g., nodes starting with 'bert.encoder.layer.0')
3. **Creates a `FINNLoop_0_` namespace** - renames extracted nodes with the `FINNLoop_0_` prefix to indicate they are part of the loop body
4. **Generates loop iteration logic** - creates control structures to iterate through all layers using the same hardware
5. **Sets up weight streaming infrastructure** - configures memory interfaces to stream different weights for each iteration
6. **Updates folding configuration** - modifies parallelization parameters to account for the loop structure

**Technical Details:**
- Node extraction uses the dynamo-generated name scopes to precisely identify which operations belong to each layer
- The `FINNLoop_0_` prefix in folding configs (like `FINNLoop_0_MVAU_rtl_0`) indicates nodes that are part of the rolled loop body
- Loop iteration count is determined by analyzing how many layers match the specified pattern
- Weight addresses are automatically calculated based on layer index and parameter sizes

**Note:** The loop rolling step is implemented in FINN as `finn.builder.build_dataflow_steps.step_loop_rolling` and requires proper name scope metadata to function correctly.

#### Loop Body Extraction Details

The specific extraction logic is implemented in the FINN library (`finn.builder.build_dataflow_steps.step_loop_rolling`). While the exact source code lines are not visible in this repository, the process performs these operations based on observable behavior:

**Node Selection Process:**
```python
# Conceptual extraction logic (actual implementation in FINN)
def extract_loop_body_nodes(model, loop_body_hierarchy):
    """Extract nodes matching the loop body hierarchy pattern."""
    extracted_nodes = []

    # Get the target pattern from hierarchy (e.g., 'bert.encoder.layer.0')
    target_pattern = loop_body_hierarchy[0][-1]  # Final level

    # Find all nodes whose names start with the target pattern
    for node in model.graph.node:
        if node.name.startswith(target_pattern):
            extracted_nodes.append(node)

    return extracted_nodes

def rename_for_loop_rolling(nodes):
    """Rename extracted nodes with FINNLoop_0_ prefix."""
    for node in nodes:
        # Transform: 'bert.encoder.layer.0.attention.self.query'
        # Into:      'FINNLoop_0_attention_self_query'
        original_name = node.name
        suffix = original_name.replace(target_pattern + '.', '')
        node.name = f'FINNLoop_0_{suffix.replace(".", "_")}'
```

**Evidence from Folding Configurations:**
The transformation results are visible in the folding configuration files, where original layer-specific nodes become `FINNLoop_0_` prefixed nodes:
- `bert.encoder.layer.0.attention.output.dense` → `FINNLoop_0_MVAU_rtl_0`
- `bert.encoder.layer.0.intermediate.dense` → `FINNLoop_0_MVAU_rtl_1`
- `bert.encoder.layer.0.output.LayerNorm` → `FINNLoop_0_LayerNorm_hls_0`

To see the exact extraction implementation, you would need to examine the FINN library source code at `finn/src/finn/builder/build_dataflow_steps.py`.

## Configuration

### Basic MLO Setup

To enable MLO in your blueprint, add the `loop_body_hierarchy` configuration:

```yaml
name: "BERT with MLO"
description: "BERT model with Multilayer Offload"

finn_config:
  loop_body_hierarchy: [['encoder', 'encoder.layer.0']]
  split_large_fifos: true
  fifosim_n_inferences: 2  # Speed up FIFO simulation

design_space:
  steps:
    - "qonnx_to_finn"
    - "bert_streamlining"
    - "infer_kernels"
    - "create_dataflow_partition"
    - "specialize_layers"
    - "loop_rolling"        # This step implements MLO
    - "target_fps_parallelization"
    - "apply_folding_config"
    # ... rest of pipeline
```

### BERT MLO Example

For BERT models, a typical MLO configuration looks like:

```yaml
# bert_mlo_demo.yaml
name: "BERT Demo"
description: "Hugging face BERT model with MLO"

extends: "../../brainsmith/blueprints/bert.yaml"

finn_config:
  loop_body_hierarchy: [['encoder', 'encoder.layer.0']]
  split_large_fifos: true
  fifosim_n_inferences: 2
  verify_steps: ['folded_hls_cppsim', 'stitched_ip_rtlsim']

design_space:
  steps:
    - at_start:
        insert:
          - "bert_cleanup"
          - "remove_head"
          - "remove_tail"
          - "generate_reference_io"
    - at_end:
        insert: "shell_metadata_handover"
```

### Encoder-Decoder MLO Example

For models with both encoder and decoder stacks (like T5, BART), you can use multiple hierarchy groups:

```yaml
# t5_mlo_demo.yaml
name: "T5 with MLO"
description: "T5 model with encoder and decoder offload"

finn_config:
  loop_body_hierarchy: [
    ['encoder', 'encoder.block.0'],      # T5 encoder blocks
    ['decoder', 'decoder.block.0']       # T5 decoder blocks
  ]
  split_large_fifos: true

design_space:
  steps:
    - "qonnx_to_finn"
    - "bert_streamlining"  # Can reuse BERT streamlining for transformers
    - "infer_kernels"
    - "create_dataflow_partition"
    - "specialize_layers"
    - "loop_rolling"       # Handles both encoder and decoder groups
    - "target_fps_parallelization"
    - "apply_folding_config"
    # ... rest of pipeline
```

This configuration creates two separate loop rolling regions - one for the encoder stack and one for the decoder stack, each reusing their respective hardware implementations.

## Performance Characteristics

### Memory Bandwidth Requirements

MLO places high demands on memory bandwidth since weights must be streamed continuously:

- **Weight streaming bandwidth**: Model size × layers × clock frequency / execution cycles
- **Activation memory**: Only need to store activations for current layer
- **Memory efficiency**: Much lower on-chip memory usage

### Throughput vs. Latency Trade-offs

**Advantages:**
- **Much larger models** can be implemented
- **Lower on-chip memory usage** (BRAM/UltraRAM)
- **Better memory utilization** across layers

**Trade-offs:**
- **Reduced throughput** due to sequential layer processing
- **Higher memory bandwidth requirements**
- **Increased latency** for single inference
- **More complex control logic**

### When to Use MLO

**Use MLO when:**
- Model is too large to fit on-chip (>24 layers typical threshold)
- High-bandwidth memory is available (HBM preferred)
- Batch processing can amortize sequential layer costs
- Model has clear repeating structure (transformers, CNNs with residual blocks)

**Avoid MLO when:**
- Model easily fits on-chip with traditional approach
- Ultra-low latency is critical
- Limited memory bandwidth available
- Model lacks clear repeating structure

## Implementation Details

### Folding Configuration

MLO requires special consideration for folding (parallelization) parameters:

```python
# Generate folding config for MLO
python gen_folding_config.py \
    --simd 4 \
    --pe 4 \
    --num_layers 2 \  # Number of layers to implement
    -t 1 \
    -o ./configs/bert_mlo_demo.json
```

The folding configuration affects both:
- **Compute parallelism** within each layer
- **Memory bandwidth requirements** for weight streaming

### Weight Management

MLO generates additional logic for:
- **Weight buffer management**: Double/triple buffering for overlap
- **DMA controllers**: Efficient weight streaming from external memory
- **Address generation**: Calculating weight addresses for each layer
- **Synchronization**: Coordinating weight loads with computation

### Loop Control

The generated accelerator includes:
- **Layer counters**: Track current layer being processed
- **State machines**: Control weight loading and computation phases
- **Flow control**: Manage data flow between layers
- **Completion detection**: Signal when all layers are processed

## Roofline Analysis

Brainsmith includes built-in roofline analysis for MLO configurations:

```python
# MLO models in roofline analysis
bert_large_mlo = {
    'offload': True,           # Enable MLO mode
    'arch': 'bert',
    'num_layers': 24,          # Total layers (only 1 implemented)
    'seq_len': 512,
    'num_heads': 16,
    'head_size': 64,
    'intermediate': 4*16*64,
}
```

The `'offload': True` flag tells the roofline model to:
- Calculate sequential execution cycles (`num_layers` iterations)
- Account for weight streaming bandwidth requirements
- Model memory access patterns for large models

## Example: BERT MLO Demo

The `examples/bert/bert_mlo_demo.sh` demonstrates a complete MLO workflow:

```bash
#!/bin/bash
# BERT MLO Demo

# Generate folding configuration
python gen_folding_config.py \
    --simd 4 \
    --pe 4 \
    --num_layers 2 \
    -t 1 \
    -o ./configs/bert_mlo_demo.json

# Run BERT demo with MLO
python bert_demo.py \
    -o bert_mlo_demo \
    -n 4 \                    # 4 attention heads
    -l 2 \                    # 2 layers total
    -z 64 \                   # Hidden size 64
    -i 256 \                  # Intermediate size 256
    -b 8 \                    # 8-bit quantization
    -q 32 \                   # Sequence length 32
    --blueprint ./bert_mlo_demo.yaml
```

This creates a BERT model with 2 encoder layers where only the first layer is implemented in hardware, and the second layer reuses the same hardware with different weights.

## Best Practices

### Loop Body Identification

1. **Analyze your ONNX model structure** - Use tools like Netron to visualize the graph hierarchy
2. **Find repeating name scopes** - Look for patterns like `encoder.layer.0`, `encoder.layer.1`, etc.
3. **Match PyTorch module names** - The hierarchy should correspond to your model's module structure
4. **Verify name scope consistency** - Ensure all repeated layers follow the same naming convention
5. **Test with small models** - Start with 2-3 layers to verify correct loop body identification

**For Multiple Hierarchy Groups:**
6. **Identify independent repeating structures** - Look for separate stacks like encoder/decoder
7. **Ensure group isolation** - Verify groups don't have cross-dependencies that complicate rolling
8. **Balance resource allocation** - Consider if groups should share hardware or have dedicated resources
9. **Test groups independently** - Validate each hierarchy group works before combining them

**CRITICAL: ONNX Export Requirements**
```python
# When exporting your model to ONNX, you MUST use dynamo=True
# This generates the metadata (name scopes) that MLO requires for loop body discovery
import brevitas.onnx as bo

bo.export_qonnx(
    model,
    inputs,
    output_path,
    dynamo=True,              # Generates name scope metadata for MLO
    input_names=['input_ids'],
    opset_version=18,
    do_constant_folding=True
)
```

**Alternative: Custom Loop Rolling for Non-Dynamo Export**

If you cannot use `dynamo=True` (due to compatibility issues, model complexity, or other constraints), you'll need to implement a custom loop rolling step. This involves writing your own loop body extraction logic that replicates what FINN's `LoopExtraction` transformation does with PyTorch metadata.

**Understanding the Standard Implementation:**
FINN's standard loop rolling (in `deps/finn/src/finn/transformation/fpgadataflow/loop_rolling.py`) works by:
1. Using `PytorchHierarchyNode` to parse `pkg.torch.onnx.name_scopes` metadata
2. Building a hierarchy tree from the metadata
3. Using prefix matching to find nodes under specified hierarchy paths
4. Extracting matching nodes and creating loop templates with `FINNLoop_0_` prefixes

**Custom Implementation:**
```python
from brainsmith.core.plugins import step
from qonnx.transformation.base import Transformation
from qonnx.core.modelwrapper import ModelWrapper
import copy

@step(
    name="custom_loop_rolling",
    category="topology_opt",
    description="Custom loop rolling with manual node collection"
)
def custom_loop_rolling_step(model, cfg):
    """Custom loop rolling step for models without dynamo metadata."""

    class CustomLoopRolling(Transformation):
        def __init__(self, loop_body_hierarchy):
            super().__init__()
            self.loop_body_hierarchy = loop_body_hierarchy

        def apply(self, model):
            # Replicate FINN's LoopExtraction + LoopRolling logic
            # without relying on PyTorch name_scopes metadata

            # Step 1: Manual node pattern matching (replaces hierarchy parser)
            loop_body_nodes = self.collect_loop_body_nodes(model)

            # Step 2: Create loop template (replaces FINN's template creation)
            loop_template = self.create_loop_template(model, loop_body_nodes)

            # Step 3: Remove original nodes and add loop structure
            rolled_model = self.apply_loop_rolling(model, loop_body_nodes, loop_template)

            return rolled_model

        def collect_loop_body_nodes(self, model):
            """
            Manual node collection that replaces PytorchHierarchyNode.get_nodes().

            This must identify the same nodes that would be found by:
            P = PytorchHierarchyNode()
            for node in model.graph.node: P.add_node(node)
            nodes = P.get_nodes(self.loop_body_hierarchy[0])
            """
            loop_nodes = []

            # Strategy 1: Pattern matching on node names
            # Look for nodes belonging to first layer/block
            hierarchy_path = self.loop_body_hierarchy[0]  # e.g., ['encoder', 'encoder.layer.0']
            target_prefix = hierarchy_path[-1]  # e.g., 'encoder.layer.0'

            for node in model.graph.node:
                # Match nodes that would have the target hierarchy prefix
                if node.name.startswith(target_prefix):
                    loop_nodes.append(node)
                # Alternative: look for pattern in node name
                elif f".{target_prefix.split('.')[-1]}." in node.name:  # e.g., ".layer.0."
                    loop_nodes.append(node)

            # Strategy 2: Graph structure analysis for complex cases
            if not loop_nodes:
                loop_nodes = self.analyze_graph_structure(model, target_prefix)

            return loop_nodes

        def analyze_graph_structure(self, model, target_prefix):
            """
            Fallback method using graph connectivity analysis.
            Identifies repeating subgraph patterns when name matching fails.
            """
            # Find repeated subgraph structures
            # This is more complex but handles cases where naming isn't consistent
            nodes = []

            # Example: Find nodes between specific input/output patterns
            # Look for activation functions that mark layer boundaries
            # Analyze weight tensor sizes that repeat across layers
            # Use topological sorting to identify layer boundaries

            return nodes

        def create_loop_template(self, model, loop_body_nodes):
            """
            Create loop template that replicates FINN's template creation.
            The template represents one iteration that will be reused.
            """
            # Create a subgraph containing just the loop body nodes
            loop_subgraph = self.extract_subgraph(model, loop_body_nodes)

            # Add FINNLoop_0_ prefix to node names (matches FINN convention)
            for node in loop_subgraph.graph.node:
                node.name = f"FINNLoop_0_{node.name}"

            return loop_subgraph

        def extract_subgraph(self, model, nodes):
            """Extract nodes and their dependencies into a new model."""
            # Implementation would extract the subgraph containing loop_body_nodes
            # Include necessary inputs, outputs, and intermediate values
            subgraph_model = ModelWrapper(copy.deepcopy(model.model))
            # ... subgraph extraction logic ...
            return subgraph_model

        def apply_loop_rolling(self, model, original_nodes, loop_template):
            """
            Apply the loop rolling by removing original nodes and adding loop structure.
            This replicates what FINN's LoopRolling transformation does.
            """
            # Remove the original loop body nodes from the main graph
            for node in original_nodes:
                model.graph.node.remove(node)

            # Add the loop template to the graph
            for template_node in loop_template.graph.node:
                model.graph.node.append(template_node)

            # Add loop control logic, iteration parameters, etc.
            # This is where the actual looping mechanism gets implemented

            return model

    # Apply the custom transformation with your hierarchy configuration
    hierarchy = cfg.loop_body_hierarchy if hasattr(cfg, 'loop_body_hierarchy') else [['encoder', 'encoder.layer.0']]
    model = model.transform(CustomLoopRolling(hierarchy))
    return model
```

**Key Implementation Points:**

1. **Node Pattern Matching**: Replace PyTorch metadata parsing with manual pattern matching on node names
2. **Hierarchy Simulation**: Manually identify what `PytorchHierarchyNode.get_nodes()` would find
3. **Template Creation**: Create loop templates with `FINNLoop_0_` prefixes matching FINN conventions
4. **Graph Modification**: Remove original nodes and add loop structure like `LoopRolling` transformation

**Advanced Strategies:**
```python
# Strategy for complex models without clear naming patterns
def identify_layer_boundaries(self, model):
    """Use graph analysis to find layer boundaries."""

    # Method 1: Look for parameter/weight nodes that repeat
    weight_patterns = {}
    for node in model.graph.node:
        if node.op_type in ['MatMul', 'Conv']:
            # Analyze weight tensor shapes and names
            pass

    # Method 2: Use activation function positions
    layer_boundaries = []
    for i, node in enumerate(model.graph.node):
        if node.op_type in ['Relu', 'Gelu', 'LayerNormalization']:
            # These often mark layer boundaries
            layer_boundaries.append(i)

    # Method 3: Analyze data flow patterns
    # Look for nodes that have similar input/output patterns
    return layer_boundaries
```

**When to use this approach:**
- Your model export framework doesn't support `dynamo=True` (TensorFlow, JAX, older PyTorch versions)
- Complex model architectures that don't export cleanly with dynamo
- Models with custom operators or non-standard layer structures
- Need fine-grained control over which nodes are included in the loop body
- Legacy models or frameworks without proper metadata support
- Production deployments where you need guaranteed behavior without metadata dependencies

**Implementation considerations:**
- **Complexity**: Requires deep understanding of your model's ONNX graph structure and FINN's loop rolling internals
- **Maintenance**: Must manually identify repeating patterns that `PytorchHierarchyNode` would detect automatically
- **Edge Cases**: Need to handle skip connections, residual blocks, and cross-layer dependencies manually
- **Testing**: More extensive validation needed since you're bypassing FINN's tested metadata-based approach
- **Debugging**: Harder to debug since you lose the hierarchical structure information from PyTorch
- **Performance**: May be slower during compilation since pattern matching is less efficient than metadata lookup

**Technical Requirements:**
Your custom implementation must replicate these key FINN behaviors:
1. **Node Collection**: Find the same nodes that `PytorchHierarchyNode.get_nodes(hierarchy)` would return
2. **Template Creation**: Generate loop templates with `FINNLoop_0_` naming convention
3. **Graph Modification**: Remove original nodes and add loop control structure
4. **Dependency Handling**: Maintain proper input/output connections and intermediate values
5. **Metadata Preservation**: Keep any essential node attributes and properties intact

**Example: Finding BERT encoder hierarchy**
```python
# In your PyTorch model, if you have:
# self.encoder.layer[0], self.encoder.layer[1], ...
# The ONNX export (with dynamo=True) will create name scopes like:
# encoder.layer.0.*, encoder.layer.1.*, ...
# So your loop_body_hierarchy should be: [['encoder', 'encoder.layer.0']]
```

### Memory System Design

1. **Use HBM when available** - Higher bandwidth than DDR for weight streaming
2. **Optimize memory access patterns** - Sequential access is more efficient
3. **Size buffers appropriately** - Balance memory usage vs. bandwidth utilization

### Model Architecture

1. **Ensure clear layer boundaries** in your model structure
2. **Consistent layer shapes** across the repeated structure
3. **Minimize cross-layer dependencies** that complicate weight streaming

### Performance Tuning

1. **Profile memory bandwidth utilization** - Should be >80% for efficiency
2. **Balance compute and memory** - Don't over-parallelize if memory-bound
3. **Consider mixed precision** - Lower precision reduces bandwidth requirements
4. **Optimize FIFO depths** - Critical for maintaining pipeline efficiency

### Verification

1. **Use smaller models first** - Debug with 2-3 layers before scaling up
2. **Compare against non-MLO** - Verify functional correctness
3. **Test weight loading** - Ensure correct weights loaded for each layer
4. **Monitor memory bandwidth** - Verify streaming performance

## Debugging MLO Issues

### Common Problems

**Missing or incorrect metadata (most common):**
- Ensure ONNX export used `dynamo=True` to generate name scope metadata
- Verify the ONNX model contains proper hierarchical node names
- If unable to use dynamo export, implement custom loop rolling step (see Loop Body Identification section)

**Incorrect loop body identification:**
- Check `loop_body_hierarchy` matches your model structure
- Verify layer naming conventions in ONNX graph

**Memory bandwidth bottlenecks:**
- Profile actual vs. theoretical bandwidth usage
- Consider reducing parallelism or increasing memory frequency

**Weight loading errors:**
- Check weight buffer sizes and addressing logic
- Verify DMA controller configuration

**Pipeline stalls:**
- Analyze FIFO depths and utilization
- Look for producer/consumer mismatches

### Debug Tools

1. **Save intermediate models** - Use `save_intermediate_models: true`
2. **Enable verification** - Use RTL simulation to check correctness
3. **Memory tracing** - Monitor weight loading patterns
4. **Performance counters** - Track cycles, bandwidth utilization

## Future Enhancements

### Planned Features

- **Multi-level loop rolling** - Support for nested repeating structures
- **Dynamic weight caching** - Intelligent caching of frequently accessed weights
- **Mixed-precision streaming** - Different precision for different layers
- **Async weight prefetching** - More sophisticated memory scheduling

### Research Directions

- **Sparse weight streaming** - Skip zero weights to reduce bandwidth
- **Compressed weight formats** - On-the-fly decompression
- **Multi-model support** - Switch between different models dynamically
- **Cross-layer optimization** - Optimize across layer boundaries

## See Also

- [Design Space Exploration](design_space_exploration.md) - Understanding execution trees
- [Blueprint Schema](blueprint_schema.md) - Configuration syntax
- [Hardware Kernels](hardware_kernels.md) - Building custom accelerators
- [BERT Examples](../examples/bert/) - Complete MLO implementations
