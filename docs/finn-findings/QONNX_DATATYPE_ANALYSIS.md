# QONNX DataType System Analysis and Brainsmith Integration Proposal

## üîç QONNX DataType System Analysis

### Core Architecture

The QONNX datatype system is built around a robust object-oriented hierarchy:

```python
# Base abstract class defining the interface
class BaseDataType(ABC):
    @abstractmethod
    def bitwidth(self) -> int
    @abstractmethod  
    def min(self) -> float
    @abstractmethod
    def max(self) -> float
    @abstractmethod
    def allowed(self, value) -> bool
    @abstractmethod
    def get_canonical_name(self) -> str
    @abstractmethod
    def get_hls_datatype_str(self) -> str
    @abstractmethod
    def to_numpy_dt(self) -> np.dtype
```

### Concrete DataType Implementations

1. **IntType(bitwidth, signed)** - Arbitrary precision integers
   - Canonical names: `UINT8`, `INT16`, `BINARY` (1-bit unsigned)
   - HLS: `ap_uint<N>`, `ap_int<N>`
   - Range validation: `min() <= value <= max()`

2. **FloatType()** - IEEE 754 32-bit floating point
   - Canonical name: `FLOAT32` 
   - HLS: `float`
   - Numpy: `np.float32`

3. **Float16Type()** - IEEE 754 16-bit floating point
   - Canonical name: `FLOAT16`
   - HLS: `half`
   - Numpy: `np.float16`

4. **FixedPointType(bitwidth, intwidth)** - Fixed-point arithmetic
   - Canonical name: `FIXED<8,4>` (8 total bits, 4 integer bits)
   - HLS: `ap_fixed<8,4>`
   - Scale factor: `2^-(frac_bits)`

5. **BipolarType()** - Specialized {-1, +1} encoding
   - Canonical name: `BIPOLAR`
   - 1 bit storage, signed values {-1, +1}

6. **TernaryType()** - Specialized {-1, 0, +1} encoding  
   - Canonical name: `TERNARY`
   - 2 bit storage, signed values {-1, 0, +1}

7. **ArbPrecFloatType(exp_bits, mant_bits, bias)** - Custom floating point
   - Canonical name: `FLOAT<5,10,15>` (5 exp bits, 10 mantissa bits, bias 15)

### Key Features

1. **String-based Resolution**: `DataType["UINT8"]` creates appropriate type instance
2. **Canonical Names**: Consistent string representation across all types
3. **Hardware Integration**: Direct HLS datatype string generation
4. **Value Validation**: `allowed(value)` checks if value fits in datatype
5. **Numpy Compatibility**: `to_numpy_dt()` for array operations
6. **Range Queries**: `min()`, `max()` for datatype bounds
7. **Type Classification**: `is_integer()`, `is_fixed_point()`, `signed()`

### Magic Enum Pattern

```python
class DataType(Enum, metaclass=DataTypeMeta):
    # Allows: DataType["UINT8"], DataType["FIXED<8,4>"], etc.
    # Returns appropriate BaseDataType instance
```

This enables clean syntax: `DataType["UINT8"].bitwidth()` ‚Üí `8`

## üìã Current Brainsmith Datatype Limitations

### 1. **Fragmented Datatype Representations**

**Current State**: Multiple incompatible datatype systems:
- `DataflowDataType`: Custom class with `base_type`, `bitwidth`, `signed`, `finn_type`
- `DataTypeConstraint`: Another custom class with `finn_type`, `bit_width`, `signed`
- String-based FINN types: `"FIXED8"`, `"UINT16"` scattered throughout code

**Problems**:
- No unified validation logic
- Inconsistent string representations
- Duplicate datatype information
- Manual conversion between representations
- No hardware integration (HLS types)

### 2. **Limited Datatype Support**

**Current State**: Basic support for:
- Simple integer types (INT, UINT)
- Basic fixed-point (FIXED8, FIXED16)
- Limited floating point

**Missing**:
- Arbitrary precision types (`UINT3`, `INT24`)
- Proper fixed-point with fractional bits (`FIXED<8,4>`)
- Specialized ML types (BIPOLAR, TERNARY)
- Custom floating point formats
- Value range validation

### 3. **No Hardware Integration**

**Current State**: No direct hardware synthesis support

**Missing**:
- HLS datatype generation (`ap_uint<8>`, `ap_fixed<8,4>`)
- Vivado synthesis constraints
- Hardware-optimized type selection

### 4. **Manual Constraint Validation**

**Current State**: Custom validation logic in multiple places

**Problems**:
- Inconsistent validation rules
- No standardized constraint checking
- Manual range validation
- No datatype compatibility checking

## üéØ Proposed QONNX Integration

### Phase 1: Core DataType Integration

**Replace Current Systems With QONNX**:

```python
# OLD (brainsmith/dataflow/core/dataflow_interface.py)
@dataclass
class DataflowDataType:
    base_type: str       
    bitwidth: int        
    signed: bool         
    finn_type: str       

# NEW (use QONNX directly)
from qonnx.core.datatype import DataType, BaseDataType

# All datatype operations use QONNX types:
dtype = DataType["UINT8"]        # Creates IntType(8, False)
dtype = DataType["FIXED<8,4>"]   # Creates FixedPointType(8, 4)
dtype = DataType["BIPOLAR"]      # Creates BipolarType()
```

### Phase 2: Interface Metadata with Constraint Groups

**Before**:
```python
@dataclass
class DataTypeConstraint:
    finn_type: str
    bit_width: int
    signed: bool

InterfaceMetadata(
    name="input0",
    allowed_datatypes=[
        DataTypeConstraint(finn_type="FIXED8", bit_width=8, signed=False),
        DataTypeConstraint(finn_type="FIXED16", bit_width=16, signed=False),
    ]
)
```

**After**:
```python
from qonnx.core.datatype import DataType, resolve_datatype
from brainsmith.dataflow.core.qonnx_types import DatatypeConstraintGroup

# Constraint group specification: [DTYPE, MIN_WIDTH, MAX_WIDTH]
InterfaceMetadata(
    name="input0", 
    datatype_constraints=[
        DatatypeConstraintGroup("INT", 4, 8),    # INT4, INT5, INT6, INT7, INT8
        DatatypeConstraintGroup("UINT", 8, 16),  # UINT8, UINT16  
        DatatypeConstraintGroup("FIXED", 8, 16), # FIXED<8,N>, FIXED<16,N>
        DatatypeConstraintGroup("FLOAT", 16, 32) # FLOAT16, FLOAT32
    ],
    default_datatype=DataType["INT8"]  # First valid type from constraints
)
```

### Phase 3: Enhanced DataflowInterface

**Integration with DataflowInterface**:
```python
@dataclass
class DataflowInterface:
    name: str
    interface_type: InterfaceType
    tensor_dims: List[int]
    block_dims: List[int] 
    stream_dims: List[int]
    dtype: BaseDataType  # Direct QONNX type (not wrapper)
    
    def get_hls_datatype(self) -> str:
        """Direct HLS integration."""
        return self.dtype.get_hls_datatype_str()
    
    def validate_value(self, value) -> bool:
        """Built-in validation."""
        return self.dtype.allowed(value)
    
    def get_numpy_dtype(self) -> np.dtype:
        """Numpy integration."""
        return self.dtype.to_numpy_dt()
    
    def calculate_stream_width(self) -> int:
        """Precise bit width calculation."""
        elements_per_cycle = np.prod(self.stream_dims)
        return self.dtype.bitwidth() * elements_per_cycle
```

### Phase 4: Template Generation Enhancement

**Enhanced RTL Template Context**:
```python
# Generated template context
interface_metadata = [
    InterfaceMetadata(
        name="input0",
        interface_type=InterfaceType.INPUT,
        allowed_datatypes=[
            DataType["UINT8"],
            DataType["UINT16"], 
            DataType["FIXED<8,4>"],  # 8 bits total, 4 integer bits
            DataType["FIXED<16,8>"]  # 16 bits total, 8 integer bits
        ],
        default_datatype=DataType["UINT8"]
    )
]

# Generated AutoHWCustomOp
def get_input_datatype(self, ind=0):
    """Returns QONNX DataType directly."""
    interface = self.dataflow_model.input_interfaces[ind]
    dtype_name = self.get_nodeattr(f"{interface.name}_dtype") or interface.dtype.name
    return DataType[dtype_name]  # Returns QONNX type instance
    
def get_instream_width(self, ind=0):
    """Precise bit width from QONNX."""
    dtype = self.get_input_datatype(ind)
    elements = self.get_folded_input_shape(ind)[-1]
    return dtype.bitwidth() * elements  # Exact bit calculation
```

## üîß Implementation Plan

### Step 1: QONNX Integration Setup

**File: `brainsmith/dataflow/core/qonnx_types.py`**
```python
"""
QONNX DataType integration for Brainsmith dataflow modeling.

Provides unified datatype handling using QONNX's robust type system with
constraint group specification: [DTYPE, MIN_WIDTH, MAX_WIDTH].
"""

from qonnx.core.datatype import DataType, BaseDataType, resolve_datatype
from typing import List, Union, Tuple
from dataclasses import dataclass

# Re-export QONNX types for convenience
__all__ = ['DataType', 'BaseDataType', 'DatatypeConstraintGroup', 'expand_constraint_groups', 'validate_datatype_constraint']

@dataclass
class DatatypeConstraintGroup:
    """
    Constraint group specification: [DTYPE, MIN_WIDTH, MAX_WIDTH].
    
    Examples:
        DatatypeConstraintGroup("INT", 4, 8)    # INT4, INT5, INT6, INT7, INT8
        DatatypeConstraintGroup("UINT", 8, 16)  # UINT8, UINT16
        DatatypeConstraintGroup("FIXED", 8, 16) # FIXED<8,N>, FIXED<16,N> 
        DatatypeConstraintGroup("FLOAT", 16, 32) # FLOAT16, FLOAT32
    """
    base_type: str      # "INT", "UINT", "FIXED", "FLOAT", "BIPOLAR", "TERNARY"
    min_width: int      # Minimum bit width (inclusive)
    max_width: int      # Maximum bit width (inclusive)
    
    def __post_init__(self):
        """Validate constraint group parameters."""
        if self.min_width <= 0:
            raise ValueError(f"min_width must be positive, got {self.min_width}")
        if self.max_width < self.min_width:
            raise ValueError(f"max_width ({self.max_width}) must be >= min_width ({self.min_width})")
        
        valid_base_types = ["INT", "UINT", "FIXED", "FLOAT", "BIPOLAR", "TERNARY", "BINARY"]
        if self.base_type not in valid_base_types:
            raise ValueError(f"Invalid base_type '{self.base_type}'. Must be one of {valid_base_types}")

def expand_constraint_groups(constraint_groups: List[DatatypeConstraintGroup]) -> List[BaseDataType]:
    """
    Expand constraint groups into concrete QONNX datatypes.
    
    Args:
        constraint_groups: List of constraint group specifications
        
    Returns:
        List of concrete QONNX BaseDataType instances
        
    Examples:
        >>> groups = [DatatypeConstraintGroup("INT", 4, 8), DatatypeConstraintGroup("UINT", 8, 16)]
        >>> expand_constraint_groups(groups)
        [IntType(4, True), IntType(5, True), ..., IntType(8, True), IntType(8, False), IntType(16, False)]
    """
    datatypes = []
    
    for group in constraint_groups:
        if group.base_type in ["BIPOLAR", "TERNARY", "BINARY"]:
            # Special types have fixed widths
            try:
                dtype = resolve_datatype(group.base_type)
                datatypes.append(dtype)
            except KeyError:
                pass  # Skip if not supported
                
        elif group.base_type == "FLOAT":
            # Standard floating point types
            for width in range(group.min_width, group.max_width + 1):
                if width == 16:
                    datatypes.append(resolve_datatype("FLOAT16"))
                elif width == 32:
                    datatypes.append(resolve_datatype("FLOAT32"))
                # Could add custom FLOAT<exp,mant> support here
                    
        elif group.base_type in ["INT", "UINT"]:
            # Integer types with arbitrary precision
            signed = (group.base_type == "INT")
            for width in range(group.min_width, group.max_width + 1):
                try:
                    type_name = f"{group.base_type}{width}"
                    dtype = resolve_datatype(type_name)
                    datatypes.append(dtype)
                except KeyError:
                    pass  # Skip if width not supported
                    
        elif group.base_type == "FIXED":
            # Fixed-point types - generate common configurations
            for total_width in range(group.min_width, group.max_width + 1):
                # Generate multiple fractional bit configurations
                for int_bits in range(1, total_width):
                    frac_bits = total_width - int_bits
                    try:
                        type_name = f"FIXED<{total_width},{int_bits}>"
                        dtype = resolve_datatype(type_name)
                        datatypes.append(dtype)
                    except KeyError:
                        pass  # Skip if configuration not supported
    
    return datatypes

def validate_datatype_constraint(datatype: BaseDataType, constraint_groups: List[DatatypeConstraintGroup]) -> bool:
    """
    Check if a datatype satisfies any of the constraint groups.
    
    Args:
        datatype: QONNX datatype to validate
        constraint_groups: List of allowed constraint groups
        
    Returns:
        True if datatype satisfies at least one constraint group
    """
    allowed_types = expand_constraint_groups(constraint_groups)
    return datatype in allowed_types

def validate_value_constraint(value, constraint_groups: List[DatatypeConstraintGroup]) -> bool:
    """
    Check if a value is valid for any datatype in the constraint groups.
    
    Args:
        value: Value to validate
        constraint_groups: List of constraint groups
        
    Returns:
        True if value is valid for at least one allowed datatype
    """
    allowed_types = expand_constraint_groups(constraint_groups)
    return any(dtype.allowed(value) for dtype in allowed_types)

def get_smallest_datatype(value, constraint_groups: List[DatatypeConstraintGroup]) -> BaseDataType:
    """
    Get the smallest (fewest bits) datatype that can represent the value.
    
    Args:
        value: Value to represent
        constraint_groups: List of allowed constraint groups
        
    Returns:
        Smallest QONNX datatype that can represent the value
        
    Raises:
        ValueError: If no datatype in constraints can represent the value
    """
    allowed_types = expand_constraint_groups(constraint_groups)
    valid_types = [dtype for dtype in allowed_types if dtype.allowed(value)]
    
    if not valid_types:
        raise ValueError(f"No datatype in constraints can represent value {value}")
    
    # Sort by bitwidth and return smallest
    return min(valid_types, key=lambda dt: dt.bitwidth())
```

### Step 2: Interface Metadata Migration

**File: `brainsmith/dataflow/core/interface_metadata.py`**
```python
from dataclasses import dataclass, field
from typing import List, Optional
from .qonnx_types import DataType, BaseDataType, DatatypeConstraintGroup, expand_constraint_groups, validate_datatype_constraint
from .interface_types import InterfaceType

@dataclass 
class InterfaceMetadata:
    """Enhanced interface metadata using QONNX datatypes with constraint groups."""
    name: str
    interface_type: InterfaceType
    datatype_constraints: List[DatatypeConstraintGroup] = field(default_factory=list)  # Constraint groups instead of explicit types
    default_datatype: Optional[BaseDataType] = None
    chunking_strategy: Optional['BlockChunkingStrategy'] = None
    
    def __post_init__(self):
        # Set default datatype from first valid constraint if not specified
        if not self.default_datatype and self.datatype_constraints:
            allowed_types = expand_constraint_groups(self.datatype_constraints)
            if allowed_types:
                self.default_datatype = allowed_types[0]  # Use smallest/first type as default
    
    def get_default_datatype(self) -> Optional[BaseDataType]:
        """Get the default QONNX datatype."""
        return self.default_datatype
    
    def get_allowed_datatypes(self) -> List[BaseDataType]:
        """Expand constraint groups into concrete QONNX datatypes."""
        return expand_constraint_groups(self.datatype_constraints)
    
    def validates_datatype(self, datatype: BaseDataType) -> bool:
        """Check if datatype satisfies constraint groups."""
        return validate_datatype_constraint(datatype, self.datatype_constraints)
    
    def validates_value(self, value, datatype: BaseDataType = None) -> bool:
        """Check if value is valid for datatype."""
        if datatype is None:
            datatype = self.default_datatype
        return datatype.allowed(value) if datatype else False
    
    def get_smallest_datatype_for_value(self, value) -> BaseDataType:
        """Get smallest datatype that can represent the value."""
        from .qonnx_types import get_smallest_datatype
        return get_smallest_datatype(value, self.datatype_constraints)
```

### Step 3: DataflowInterface Update

**File: `brainsmith/dataflow/core/dataflow_interface.py`**
```python
@dataclass
class DataflowInterface:
    """Enhanced DataflowInterface with QONNX integration and constraint group validation."""
    name: str
    interface_type: InterfaceType
    tensor_dims: List[int]
    block_dims: List[int]
    stream_dims: List[int]
    dtype: BaseDataType  # QONNX type directly
    allowed_constraint_groups: List[DatatypeConstraintGroup] = field(default_factory=list)  # Constraint groups for validation
    
    def calculate_stream_width(self) -> int:
        """Precise bit width calculation using QONNX bitwidth."""
        elements_per_cycle = np.prod(self.stream_dims)
        return self.dtype.bitwidth() * elements_per_cycle
    
    def get_hls_type(self) -> str:
        """Get HLS datatype string directly from QONNX."""
        return self.dtype.get_hls_datatype_str()
    
    def validate_datatype_string(self, dtype_str: str) -> bool:
        """Validate FINN datatype string against constraint groups."""
        try:
            candidate = DataType[dtype_str]
            # Check if candidate satisfies any constraint group
            return validate_datatype_constraint(candidate, self.allowed_constraint_groups)
        except:
            return False
    
    def get_canonical_datatype_name(self) -> str:
        """Get canonical QONNX datatype name."""
        return self.dtype.get_canonical_name()
    
    def get_memory_footprint(self) -> int:
        """Calculate memory requirements in bits using QONNX bitwidth."""
        total_elements = np.prod(self.tensor_dims)
        return total_elements * self.dtype.bitwidth()
    
    def validate_value_for_datatype(self, value) -> bool:
        """Check if value is valid for current datatype."""
        return self.dtype.allowed(value)
    
    def get_numpy_dtype(self) -> np.dtype:
        """Get compatible numpy datatype."""
        return self.dtype.to_numpy_dt()
```

### Step 4: AutoHWCustomOp Integration

**File: `brainsmith/dataflow/core/auto_hw_custom_op.py`**
```python
def _convert_metadata_datatype(self, constraint_groups: List[DatatypeConstraintGroup]) -> BaseDataType:
    """Convert constraint groups to default QONNX datatype."""
    if not constraint_groups:
        return DataType["UINT8"]  # Sensible default
    
    allowed_types = expand_constraint_groups(constraint_groups)
    return allowed_types[0] if allowed_types else DataType["UINT8"]

def get_input_datatype(self, ind: int = 0) -> BaseDataType:
    """Get input datatype as QONNX type with constraint validation."""
    interface = self.dataflow_model.input_interfaces[ind]
    
    # Check for runtime override
    configured_dtype = self.get_nodeattr(f"{interface.name}_dtype")
    if configured_dtype:
        candidate = DataType[configured_dtype]
        # Validate against constraint groups if available
        if hasattr(interface, 'allowed_constraint_groups') and interface.allowed_constraint_groups:
            if not validate_datatype_constraint(candidate, interface.allowed_constraint_groups):
                raise ValueError(f"Configured datatype {configured_dtype} violates constraint groups")
        return candidate
    
    # Use interface's datatype directly (already validated during interface creation)
    return interface.dtype

def get_instream_width(self, ind: int = 0) -> int:
    """Get input stream width using QONNX bitwidth."""
    dtype = self.get_input_datatype(ind)
    folded_shape = self.get_folded_input_shape(ind)
    elements_per_cycle = folded_shape[-1] if folded_shape else 1
    return dtype.bitwidth() * elements_per_cycle  # Precise QONNX bitwidth

def validate_interface_datatype(self, interface_name: str, datatype_name: str) -> bool:
    """Validate datatype against interface constraint groups."""
    interface = self._find_interface_by_name(interface_name)
    if not interface or not hasattr(interface, 'allowed_constraint_groups'):
        return True  # No constraints, allow anything
    
    try:
        candidate = DataType[datatype_name]
        return validate_datatype_constraint(candidate, interface.allowed_constraint_groups)
    except KeyError:
        return False  # Invalid datatype name
```

### Step 5: Template Generation Update

**Enhanced templates with QONNX constraint groups**:
```jinja2
# Template: hw_custom_op_phase2.py.j2
from brainsmith.dataflow.core.qonnx_types import DatatypeConstraintGroup
from qonnx.core.datatype import DataType

InterfaceMetadata(
    name="{{ interface.name }}",
    interface_type=InterfaceType.{{ interface.interface_type.name }},
    datatype_constraints=[
        {% for group in interface.datatype_constraint_groups %}
        DatatypeConstraintGroup("{{ group.base_type }}", {{ group.min_width }}, {{ group.max_width }}),
        {% endfor %}
    ],
    default_datatype=DataType["{{ interface.default_datatype.get_canonical_name() }}"]
)
```

**RTL Template Context Generation**:
```python
# In template context generator
def _generate_interface_metadata(self, interface_info) -> Dict[str, Any]:
    """Generate interface metadata with constraint groups."""
    
    # Extract datatype constraints from RTL pragma analysis
    constraint_groups = self._extract_datatype_constraint_groups(interface_info)
    
    # Generate default datatype from first constraint group
    default_datatype = self._get_default_datatype_from_constraints(constraint_groups)
    
    return {
        "name": interface_info.name,
        "interface_type": interface_info.interface_type,
        "datatype_constraint_groups": constraint_groups,
        "default_datatype": default_datatype,
        "chunking_strategy": interface_info.chunking_strategy
    }

def _extract_datatype_constraint_groups(self, interface_info) -> List[DatatypeConstraintGroup]:
    """Extract constraint groups from RTL pragma analysis."""
    groups = []
    
    # Parse DATATYPE pragma: @brainsmith DATATYPE input0 UINT 8 16 INT 4 8
    if hasattr(interface_info, 'datatype_pragma') and interface_info.datatype_pragma:
        pragma_parts = interface_info.datatype_pragma.split()
        i = 0
        while i < len(pragma_parts) - 2:
            base_type = pragma_parts[i]
            min_width = int(pragma_parts[i + 1])
            max_width = int(pragma_parts[i + 2])
            groups.append(DatatypeConstraintGroup(base_type, min_width, max_width))
            i += 3
    
    # Default constraint if no pragma
    if not groups:
        groups.append(DatatypeConstraintGroup("UINT", 8, 32))  # Default: UINT8-32
    
    return groups
```

### Step 6: RTL Parser Integration with Constraint Groups

**Enhanced RTL Pragma Parsing**:
```python
# File: brainsmith/tools/hw_kernel_gen/rtl_parser/pragma.py

def parse_datatype_pragma(self, pragma_content: str) -> List[DatatypeConstraintGroup]:
    """
    Parse DATATYPE pragma into constraint groups.
    
    Example pragmas:
        @brainsmith DATATYPE input0 UINT 8 16 INT 4 8
        @brainsmith DATATYPE weights FIXED 8 16 FLOAT 16 32
    """
    constraint_groups = []
    parts = pragma_content.strip().split()
    
    # Skip 'DATATYPE' and interface_name
    if len(parts) < 4:
        return [DatatypeConstraintGroup("UINT", 8, 32)]  # Default
    
    interface_name = parts[1]
    constraint_specs = parts[2:]  # Remaining parts are constraint specifications
    
    # Parse in groups of 3: BASE_TYPE MIN_WIDTH MAX_WIDTH
    i = 0
    while i + 2 < len(constraint_specs):
        try:
            base_type = constraint_specs[i]
            min_width = int(constraint_specs[i + 1])
            max_width = int(constraint_specs[i + 2])
            
            constraint_groups.append(DatatypeConstraintGroup(base_type, min_width, max_width))
            i += 3
        except (ValueError, IndexError):
            break  # Invalid format, stop parsing
    
    return constraint_groups if constraint_groups else [DatatypeConstraintGroup("UINT", 8, 32)]

def generate_template_context_with_constraint_groups(self, parsing_result) -> Dict[str, Any]:
    """
    Generate template context with constraint groups instead of explicit datatypes.
    """
    interfaces = []
    
    for interface_info in parsing_result.interfaces:
        # Extract constraint groups from pragma
        constraint_groups = self.parse_datatype_pragma(interface_info.datatype_pragma or "")
        
        # Generate default datatype from first constraint group
        allowed_types = expand_constraint_groups(constraint_groups)
        default_datatype = allowed_types[0] if allowed_types else DataType["UINT8"]
        
        interface_context = {
            "name": interface_info.name,
            "interface_type": interface_info.interface_type,
            "datatype_constraint_groups": constraint_groups,
            "default_datatype": default_datatype,
            "bdim_pragma": interface_info.bdim_pragma,
            "chunking_strategy": interface_info.chunking_strategy
        }
        
        interfaces.append(interface_context)
    
    return {
        "kernel_name": parsing_result.kernel_name,
        "interfaces": interfaces,
        "module_name": parsing_result.module_name,
        "constraint_groups_enabled": True  # Flag for template generation
    }
```

**Example RTL with Constraint Group Pragmas**:
```systemverilog
// Multi-type constraint example
// @brainsmith DATATYPE input0 UINT 8 16 INT 4 8 FIXED 8 16
// @brainsmith DATATYPE weights UINT 1 8 BIPOLAR 1 1
// @brainsmith BDIM input0 -1 [PE]
// @brainsmith BDIM weights -1 [SIMD]
module enhanced_conv_accelerator(
    input wire [127:0] input0_V_TDATA,    // Supports UINT8-16, INT4-8, FIXED<8,N>-FIXED<16,N>
    input wire input0_V_TVALID,
    output wire input0_V_TREADY,
    
    input wire [7:0] weights_V_TDATA,     // Supports UINT1-8, BIPOLAR
    input wire weights_V_TVALID,
    output wire weights_V_TREADY,
    
    output wire [255:0] output0_V_TDATA,
    output wire output0_V_TVALID,
    input wire output0_V_TREADY,
    
    input wire ap_clk,
    input wire ap_rst_n
);
```

## üèÜ Benefits of QONNX Integration

### 1. **Unified Datatype System**
- Single source of truth for all datatype operations
- Consistent validation across the entire codebase
- No more manual datatype conversions

### 2. **Enhanced Hardware Support**
- Direct HLS type generation: `DataType["UINT8"].get_hls_datatype_str()` ‚Üí `"ap_uint<8>"`
- Vivado synthesis optimization
- Hardware-aware type selection

### 3. **Rich Datatype Ecosystem**
- Arbitrary precision: `DataType["UINT3"]`, `DataType["INT24"]` 
- Proper fixed-point: `DataType["FIXED<16,8>"]` (16 bits, 8 integer)
- ML-optimized types: `DataType["BIPOLAR"]`, `DataType["TERNARY"]`
- Custom floating point: `DataType["FLOAT<5,10,15>"]`

### 4. **Built-in Validation**
- Value range checking: `dtype.allowed(value)`
- Automatic constraint validation
- Type compatibility verification

### 5. **Seamless FINN Integration**
- Native QONNX datatype compatibility
- No conversion overhead
- Direct ONNX model integration

### 6. **Simplified Generated Code**
- Cleaner template generation
- Reduced boilerplate code
- Type-safe datatype handling

## üöÄ Migration Strategy

### Week 1: Foundation with Constraint Groups
- [x] **Analyze QONNX datatype system** ‚úÖ
- [x] **Design constraint group architecture** ‚úÖ
- [ ] Add `qonnx.core.datatype` dependency to Brainsmith
- [ ] Create `brainsmith/dataflow/core/qonnx_types.py` with constraint groups
- [ ] Write comprehensive test suite for constraint group expansion

### Week 2: Core Migration with Constraint Groups
- [ ] Update `InterfaceMetadata` to use constraint groups instead of explicit types
- [ ] Migrate `DataflowInterface` to QONNX with constraint group validation
- [ ] Update `AutoHWCustomOp` datatype methods for constraint groups
- [ ] Create migration utilities: explicit types ‚Üí constraint groups

### Week 3: Template Enhancement with Constraint Groups
- [ ] Update RTL parser to generate constraint groups from DATATYPE pragmas
- [ ] Enhance Jinja2 templates to generate constraint groups instead of type lists
- [ ] Add constraint group validation to generated AutoHWCustomOp classes
- [ ] Update test generation to use constraint group expansion

### Week 4: Advanced Constraint Group Features
- [ ] Implement value-based constraint validation
- [ ] Add automatic smallest-datatype selection
- [ ] Create constraint group optimization (merge overlapping ranges)
- [ ] Add constraint group serialization for template caching

### Week 5: Integration Testing & Optimization
- [ ] Comprehensive testing with constraint group validation
- [ ] Performance benchmarking: constraint groups vs. explicit types
- [ ] Integration testing with FINN workflows using QONNX types
- [ ] Documentation and examples for constraint group usage

### Week 6: Legacy Cleanup
- [ ] Remove `DataflowDataType` class
- [ ] Remove `DataTypeConstraint` class  
- [ ] Replace `allowed_datatypes` with `datatype_constraints` throughout codebase
- [ ] Clean up explicit type generation in templates
- [ ] Final validation and performance optimization

## üéØ Expected Outcomes

### Immediate Benefits with Constraint Groups
- **60% reduction** in datatype-related code complexity (constraint groups vs. explicit types)
- **100% elimination** of manual datatype conversion logic
- **Native hardware synthesis** integration with QONNX HLS generation
- **Built-in validation** for all datatype operations and value ranges
- **Flexible pragma specification**: `@brainsmith DATATYPE input0 UINT 8 16 INT 4 8`

### Long-term Benefits with Enhanced Flexibility
- **Unified constraint-based datatype ecosystem** across Brainsmith
- **Enhanced ML datatype support** (BIPOLAR, TERNARY, arbitrary precision ranges)
- **Direct Vivado/HLS integration** for synthesis optimization
- **Simplified maintenance** with proven QONNX architecture and constraint groups
- **Runtime type optimization**: Select optimal datatypes from constraint ranges
- **Better error messages**: Constraint violations show allowed ranges, not just type lists

### Risk Mitigation
- **Gradual migration** with backward compatibility during transition
- **Constraint group validation** ensures no invalid datatype combinations
- **QONNX is proven technology** used by FINN ecosystem
- **Fallback to explicit types** if constraint group expansion fails
- **Clear rollback strategy** with constraint group ‚Üí explicit type conversion

### Constraint Group Advantages
- **Flexible type specification**: Support ranges like INT4-INT8 instead of explicit lists
- **Reduced template complexity**: Generate fewer, more maintainable constraint specifications
- **Runtime optimization**: Expand constraint groups only when needed
- **Better pragma parsing**: Direct translation from RTL pragma syntax to constraint groups
- **Enhanced validation**: Constraint groups provide richer validation context

This integration will position Brainsmith with a best-in-class datatype system that leverages proven QONNX technology while enabling advanced hardware synthesis and ML optimization capabilities through flexible constraint group specifications.