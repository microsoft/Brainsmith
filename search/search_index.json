{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Brainsmith","text":"<p>From PyTorch to RTL - FPGA Accelerator Compiler for AI</p> <p>Brainsmith automates design space exploration (DSE) and implementation of neural networks on FPGA, from PyTorch to RTL. It builds on FINN, QONNX, and Brevitas to create dataflow accelerators with tunable parameters.</p>"},{"location":"#pre-release","title":"Pre-Release","text":"<p>Pre-Release Status</p> <p>This repository is in a pre-release state and under active co-development by Microsoft and AMD.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Plugin System - Extensible architecture for registering custom kernels, transforms, and build steps</li> <li>Blueprint Interface - YAML-based declarative configuration with inheritance support</li> <li>Segment-based Execution - Efficient DSE through intelligent computation reuse</li> <li>BERT Demo - Example end-to-end acceleration (PyTorch to stitched-IP RTL)</li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li> <p> Quick Start</p> <p>Get up and running in minutes</p> </li> <li> <p> Installation</p> <p>Set up your development environment</p> </li> <li> <p> Architecture</p> <p>Understand how Brainsmith works</p> </li> <li> <p> API Reference</p> <p>Explore the codebase</p> </li> </ul>"},{"location":"#architecture-overview","title":"Architecture Overview","text":"<pre><code>graph LR\n    A[PyTorch Model] --&gt; B[Brevitas Quantization]\n    B --&gt; C[ONNX Export]\n    C --&gt; D[QONNX]\n    D --&gt; E[Brainsmith DSE]\n    E --&gt; F[FINN Dataflow]\n    F --&gt; G[RTL Generation]\n    G --&gt; H[Vivado Synthesis]\n    H --&gt; I[FPGA Bitstream]</code></pre>"},{"location":"#example-design-space-exploration","title":"Example: Design Space Exploration","text":"<pre><code># Run DSE with ONNX model and blueprint\nsmith model.onnx blueprint.yaml --output-dir ./results\n</code></pre>"},{"location":"#built-with","title":"Built With","text":"<p>Brainsmith builds upon:</p> <ul> <li>FINN - Dataflow compiler for quantized neural networks</li> <li>QONNX - Quantized ONNX representation</li> <li>Brevitas - PyTorch quantization library</li> </ul>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"#acknowledgments","title":"Acknowledgments","text":"<p>Brainsmith is developed through a collaboration between Microsoft and AMD.</p>"},{"location":"blueprint_schema/","title":"Blueprint Schema Reference","text":"<p>Blueprints are YAML files that define the design space for FPGA accelerator generation, including hardware kernels, build steps, and exploration parameters.</p>"},{"location":"blueprint_schema/#schema-structure","title":"Schema Structure","text":"<pre><code># Required: Blueprint metadata\nname: \"string\"                          # Blueprint name\ndescription: \"string\"                   # Optional: Blueprint description\n\n# Optional: Inherit from parent blueprint\nextends: \"relative/path/to/parent.yaml\" # Path relative to child blueprint\n\n# Required: Core configuration\nclock_ns: 5.0                          # Target clock period in nanoseconds (required)\noutput: \"estimates\"                    # Output type: estimates | rtl | bitfile\n                                      # Default: \"estimates\"\nboard: \"Pynq-Z1\"                      # Target FPGA board (required for rtl/bitfile)\n\n# Optional: Additional configuration\nverify: false                         # Enable verification (default: false)\nverify_data: \"path/to/verify_data/\"   # Directory with input.npy and expected_output.npy\nsave_intermediate_models: false       # Save intermediate models (default: false)\nparallel_builds: 4                    # Concurrent FINN builds during DSE (default: 4)\n\n# Optional: Step range control (for testing/debugging)\nstart_step: \"streamline\"              # Start execution from this step (inclusive)\nstop_step: \"specialize_layers\"        # Stop execution at this step (inclusive)\n\n# Optional: Direct FINN parameter overrides\nfinn_config:                          # Maps internally to finn_overrides\n  minimize_bit_width: false\n  rtlsim_batch_size: 100\n  shell_flow_type: \"vivado_zynq\"\n  # Any other FINN DataflowBuildConfig parameter...\n\n# Required: Design space definition\ndesign_space:\n  # Kernel definitions (for hardware mapping)\n  kernels:\n    - KernelName                      # Use all available backends\n    - KernelName: BackendName         # Specific backend only\n    - KernelName: [Backend1, Backend2] # Multiple specific backends\n\n  # Build pipeline steps\n  steps:\n    - \"step_name\"                     # Single step\n    - [\"optionA\", \"optionB\"]          # Branch: mutually exclusive options\n    - [\"step\", ~]                     # Optional step (~ or null = skip)\n\n    # Step operations (for inheritance or organization)\n    - after: \"target_step\"\n      insert: \"new_step\"              # Insert single step\n    - before: \"target_step\"  \n      insert: \n        - \"step1\"                     # Insert multiple steps \n        - \"step2\"\n        - [\"step3a\", \"step3b\"]        # Insert a branching point\n    - replace: \"old_step\"\n      with: [[\"new1\", \"new2\"]]        # Replace with a branching point\n    - remove: \"unwanted_step\"         # Remove a step\n    - at_start:\n        insert: \"first_step\"\n    - at_end:\n        insert: [\"last1\", \"last2\"]   \n</code></pre>"},{"location":"blueprint_schema/#field-definitions","title":"Field Definitions","text":""},{"location":"blueprint_schema/#core-configuration","title":"Core Configuration","text":""},{"location":"blueprint_schema/#clock_ns-required","title":"clock_ns (required)","text":"<p>The target clock period in nanoseconds. This is the only required configuration field. <pre><code>clock_ns: 5.0    # 5ns = 200MHz clock frequency\n</code></pre></p>"},{"location":"blueprint_schema/#output","title":"output","text":"<p>Determines how far to proceed in the build pipeline: - <code>\"estimates\"</code> (default) - Generate resource estimates only - <code>\"rtl\"</code> - Generate RTL code and IP blocks - <code>\"bitfile\"</code> - Full synthesis to FPGA bitstream</p>"},{"location":"blueprint_schema/#board","title":"board","text":"<p>Target FPGA board. Required when <code>output</code> is <code>\"rtl\"</code> or <code>\"bitfile\"</code>. Common boards include: - <code>\"Pynq-Z1\"</code>, <code>\"Pynq-Z2\"</code> - Xilinx PYNQ boards - <code>\"Ultra96\"</code> - Avnet Ultra96 - <code>\"ZCU104\"</code>, <code>\"ZCU102\"</code> - Xilinx ZCU boards - <code>\"VCK190\"</code> - Xilinx Versal board - <code>\"V80\"</code> - AMD Versal V80</p>"},{"location":"blueprint_schema/#optional-configuration","title":"Optional Configuration","text":""},{"location":"blueprint_schema/#verify-verify_data","title":"verify &amp; verify_data","text":"<p>Enable verification with test data: <pre><code>verify: true\nverify_data: \"path/to/verify_data/\"  # Directory containing input.npy and expected_output.npy\n</code></pre></p>"},{"location":"blueprint_schema/#debug","title":"debug","text":"<p>Enable debug logging and additional diagnostics (not currently implemented).</p>"},{"location":"blueprint_schema/#save_intermediate_models","title":"save_intermediate_models","text":"<p>Save model state after each transformation step (useful for debugging).</p>"},{"location":"blueprint_schema/#parallel_builds","title":"parallel_builds","text":"<p>Number of concurrent FINN builds to run during design space exploration. Higher values can speed up exploration but require more memory. <pre><code>parallel_builds: 4  # Default: 4\n</code></pre></p>"},{"location":"blueprint_schema/#start_step-stop_step","title":"start_step &amp; stop_step","text":"<p>Control the execution range of steps for testing and debugging. Both parameters are optional and specify step boundaries (inclusive).</p> <pre><code>start_step: \"streamline\"        # Start from this step (inclusive)\nstop_step: \"specialize_layers\"  # Stop at this step (inclusive)\n</code></pre> <p>Use Cases: - Testing individual steps: Set both to the same value to run only that step - Creating checkpoints: Use <code>stop_step</code> to build up to a certain point - Resuming from intermediate: Use <code>start_step</code> with a previously saved intermediate model - Debugging failures: Isolate problematic steps for investigation</p> <p>CLI Overrides: CLI flags <code>--start-step</code> and <code>--stop-step</code> override blueprint values: <pre><code># Override blueprint to test single step\nsmith dse model.onnx blueprint.yaml --start-step streamline --stop-step streamline\n\n# Run from beginning up to a checkpoint\nsmith dse model.onnx blueprint.yaml --stop-step specialize_layers\n</code></pre></p> <p>Notes: - Steps are identified by name and must match step names in the <code>steps</code> list - For branch points (list of steps), specify any step name within the branch - Slicing preserves branch structure within the specified range - Use with <code>save_intermediate_models: true</code> to enable checkpointing</p>"},{"location":"blueprint_schema/#finn-configuration-overrides","title":"FINN Configuration Overrides","text":"<p>The <code>finn_config</code> section allows direct access to FINN's DataflowBuildConfig parameters: <pre><code>finn_config:\n  minimize_bit_width: false      # Skip bit-width optimization\n  rtlsim_batch_size: 100        # Batch size for RTL simulation\n  shell_flow_type: \"vivado_zynq\" # Shell type for Zynq devices\n  generate_outputs: [\"estimate_only\", \"rtlsim_performance\"]\n</code></pre></p>"},{"location":"blueprint_schema/#design-space-definition","title":"Design Space Definition","text":""},{"location":"blueprint_schema/#kernels","title":"Kernels","text":"<p>Kernels define the hardware implementations available for neural network layers. When the <code>infer_kernels</code> step is executed, Brainsmith automatically maps layers to these kernels.</p> <p>Pre-Release Note: Backend specifications are validated and stored in the design space, but backend selection is not yet implemented. Currently, the build will use the default backend based on the <code>preferred_impl_style</code> nodeattr in the HWCustomOp. Full backend selection support is planned for a future release.</p> <pre><code>kernels:\n  # Use all available backends for a kernel\n  - Thresholding                      # Thresholding layers\n\n  # Specify particular backends\n  - LayerNorm: LayerNorm_hls          # Use HLS backend only\n  - MVAU: [MVAU_hls, MVAU_rtl]        # Use both HLS and RTL backends\n</code></pre> <p>Backend names must match the full registered name from the backend implementation. For example, if a backend is registered as: <pre><code>@backend(name=\"LayerNorm_hls\", kernel=\"LayerNorm\", language=\"hls\")\nclass LayerNorm_hls(LayerNorm, HLSBackend):\n    ...\n</code></pre> Then use <code>LayerNorm_hls</code> in the blueprint, not just <code>hls</code>.</p> <p>Common FINN kernels: - <code>MVAU</code> - Matrix-Vector-Activation Unit (dense/linear layers) - <code>Thresholding</code> - Quantized activation functions - <code>ElementwiseBinaryOperation</code> - Element-wise operations - <code>Pool</code> - Pooling layers - <code>LayerNorm</code> - Layer normalization - <code>HWSoftmax</code> - Hardware softmax - <code>DuplicateStreams</code> - Stream duplication - <code>Shuffle</code> - Tensor shuffling/reshaping</p>"},{"location":"blueprint_schema/#steps","title":"Steps","text":"<p>Steps define the transformation pipeline. They can be: 1. Linear - Applied unconditionally 2. Branching - Create alternative paths 3. Optional - Can be skipped</p> <pre><code>steps:\n  # Linear steps - always executed\n  - \"qonnx_to_finn\"                   # Convert QONNX to FINN format\n  - \"tidy_up\"                         # Clean up the model\n\n  # Branching - creates multiple execution paths\n  - [\"streamline\", \"streamline_aggressive\"]  # Try both approaches\n\n  # Optional steps - creates paths with and without\n  - [\"minimize_bit_width\", ~]         # ~ means skip this step\n\n  # Special step for kernel inference\n  - \"infer_kernels\"                   # Maps layers to hardware kernels\n\n  # Common FINN pipeline steps\n  - \"create_dataflow_partition\"       # Partition into dataflow regions\n  - \"specialize_layers\"               # Specialize to hardware\n  - \"apply_folding_config\"            # Apply parallelization\n  - \"generate_estimate_reports\"       # Generate resource estimates\n</code></pre> <p>Step Validation: Invalid step names will raise a <code>ValueError</code> with helpful suggestions. For example, if you misspell \"streamline\" as \"steamline\", you'll get an error suggesting the correct name.</p> <p>Branch Point Restrictions:  - Branch points (lists) can only contain strings or skip indicators - Nested lists are not allowed within branch points - To insert a branch point via step operations, use double brackets: <code>with: [[\"option1\", \"option2\"]]</code></p> <p>Skip Values: The following values all indicate a step should be skipped: - <code>~</code> (recommended) - <code>null</code> or <code>None</code> - Empty string <code>\"\"</code></p> <p>All skip values are normalized to <code>~</code> internally.</p>"},{"location":"blueprint_schema/#step-operations","title":"Step Operations","text":"<p>Step operations allow you to modify the step list when inheriting from parent blueprints or organizing complex pipelines.</p>"},{"location":"blueprint_schema/#operation-types","title":"Operation Types","text":"<p>after - Insert steps after a target step: <pre><code>- after: \"tidy_up\"\n  insert: \"custom_cleanup\"          # Insert single step\n\n- after: \"streamline\"\n  insert: [\"verify\", \"log_stats\"]   # Insert multiple steps\n</code></pre></p> <p>before - Insert steps before a target step: <pre><code>- before: \"generate_reports\"\n  insert: [\"save_checkpoint\", \"validate_results\"]\n</code></pre></p> <p>replace - Replace a step with alternatives: <pre><code>- replace: \"minimize_bit_width\"\n  with: [\"quantize_weights\", \"quantize_activations\"]\n</code></pre></p> <p>remove - Remove unwanted steps: <pre><code>- remove: \"debug_step\"              # Remove single step\n</code></pre></p> <p>at_start/at_end - Insert at list boundaries: <pre><code>- at_start:\n    insert: \"initialize_environment\"\n- at_end:\n    insert: [\"cleanup\", \"generate_summary\"]\n</code></pre></p>"},{"location":"blueprint_schema/#inheritance","title":"Inheritance","text":"<p>Blueprint inheritance allows reusing and extending existing configurations.</p> <pre><code># parent.yaml\nname: \"Base FINN Pipeline\"\nclock_ns: 5.0\noutput: \"estimates\"\n\ndesign_space:\n  steps:\n    - \"qonnx_to_finn\"\n    - \"tidy_up\"\n    - \"streamline\"\n\n  kernels:\n    - MVAU\n\n# child.yaml - extends parent\nextends: \"parent.yaml\"\nname: \"Extended Pipeline\"\noutput: \"rtl\"                      # Override parent\nboard: \"Pynq-Z1\"                   # Add new field\n\ndesign_space:\n  steps:\n    # Parent steps are inherited, then these operations applied:\n    - after: \"streamline\"\n      insert: \"custom_optimization\"\n    - at_end:\n      insert: \"package_ip\"\n\n  # Kernels are replaced entirely (no merge)\n  kernels:\n    - MVAU\n    - Thresholding                 # Add new kernel\n</code></pre>"},{"location":"blueprint_schema/#inheritance-rules","title":"Inheritance Rules","text":"<ol> <li>Simple fields (name, clock_ns, etc.) - Child overrides parent</li> <li>finn_config - Deep merged (child fields override parent fields)</li> <li>steps - Parent steps are inherited. Use step operations to modify them. If child specifies direct steps without operations, parent steps are replaced entirely.</li> <li>kernels - Child replaces parent entirely (no merge). Note: If child blueprint has no <code>kernels</code> section at all, parent kernels are inherited. An empty <code>kernels: []</code> list explicitly clears parent kernels.</li> </ol>"},{"location":"blueprint_schema/#execution-semantics","title":"Execution Semantics","text":""},{"location":"blueprint_schema/#execution-tree-structure","title":"Execution Tree Structure","text":"<p>Brainsmith builds an execution tree from the blueprint where: - Nodes represent execution segments (groups of sequential steps) - Branches occur at variation points (lists in steps) - Leaves represent complete execution paths</p>"},{"location":"blueprint_schema/#branch-expansion","title":"Branch Expansion","text":"<p>Given these steps: <pre><code>steps:\n  - \"tidy_up\"                      # Linear step\n  - [\"streamline\", \"streamline_aggressive\"]  # Branch point (2 options)\n  - \"convert_to_hw\"                # Linear step\n  - [\"fold_constants\", ~]          # Branch point (2 options)\n</code></pre></p> <p>This creates 4 execution paths: 1. <code>tidy_up \u2192 streamline \u2192 convert_to_hw \u2192 fold_constants</code> 2. <code>tidy_up \u2192 streamline \u2192 convert_to_hw \u2192 (skip)</code> 3. <code>tidy_up \u2192 streamline_aggressive \u2192 convert_to_hw \u2192 fold_constants</code> 4. <code>tidy_up \u2192 streamline_aggressive \u2192 convert_to_hw \u2192 (skip)</code></p>"},{"location":"blueprint_schema/#segment-based-execution","title":"Segment-Based Execution","text":"<p>To optimize performance, Brainsmith groups sequential steps into segments: - Steps between branch points form a single segment - Each segment is executed as one FINN build - Artifacts are shared at branch points to avoid redundant computation</p> <p>Pre-Release Note: Segment-based execution is functional but requires further testing and refinement for large design spaces.</p>"},{"location":"blueprint_schema/#environment-variables","title":"Environment Variables","text":""},{"location":"blueprint_schema/#brainsmith_max_combinations","title":"BRAINSMITH_MAX_COMBINATIONS","text":"<p>Controls the maximum number of design space combinations (execution paths) allowed:</p> <pre><code>export BRAINSMITH_MAX_COMBINATIONS=100000  # Default: 100000\n</code></pre> <p>This limit prevents accidentally creating design spaces that are too large to explore. If your blueprint generates more paths than this limit, you'll receive an error with the actual count. You can then either: - Reduce the design space by removing some branch points - Increase the limit if you have sufficient computational resources</p>"},{"location":"cli_api_reference/","title":"Brainsmith CLI API Reference","text":"<p>This document provides a complete reference for the Brainsmith dual-command CLI structure.</p>"},{"location":"cli_api_reference/#overview","title":"Overview","text":"<p>Brainsmith uses a dual-command structure: - <code>brainsmith</code> - Application-level configuration and setup - <code>smith</code> - Operational commands for design space exploration and kernel generation</p>"},{"location":"cli_api_reference/#command-structure","title":"Command Structure","text":""},{"location":"cli_api_reference/#brainsmith-application-configuration","title":"<code>brainsmith</code> - Application Configuration","text":"<pre><code>brainsmith [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Global Options: - <code>--config, -c PATH</code> - Use specific configuration file - <code>--build-dir PATH</code> - Override build directory - <code>--debug</code> - Enable debug mode with detailed logging - <code>--version</code> - Show Brainsmith version - <code>--help</code> - Show help message</p>"},{"location":"cli_api_reference/#smith-operational-commands","title":"<code>smith</code> - Operational Commands","text":"<pre><code>smith [COMMAND] [OPTIONS]\n</code></pre> <p>Global Options: - <code>--version</code> - Show version - <code>--help</code> - Show help message</p>"},{"location":"cli_api_reference/#configuration-management","title":"Configuration Management","text":""},{"location":"cli_api_reference/#brainsmith-config-manage-configuration","title":"<code>brainsmith config</code> - Manage Configuration","text":""},{"location":"cli_api_reference/#brainsmith-config-show","title":"<code>brainsmith config show</code>","text":"<p>Display current configuration with all active settings.</p> <pre><code>brainsmith config show [OPTIONS]\n</code></pre> <p>Options: - <code>--format, -f [table|yaml|json|env]</code> - Output format (default: table) - <code>--verbose, -v</code> - Include source information and path validation - <code>--external-only</code> - For env format, show only external tool variables</p> <p>Examples: <pre><code># Show configuration as table\nbrainsmith config show\n\n# Show as YAML with source information\nbrainsmith config show --format yaml --verbose\n\n# Show environment variables\nbrainsmith config show --format env\n</code></pre></p>"},{"location":"cli_api_reference/#brainsmith-config-init","title":"<code>brainsmith config init</code>","text":"<p>Initialize a new configuration file.</p> <pre><code>brainsmith config init [OPTIONS]\n</code></pre> <p>Options: - <code>--user</code> - Create user-level config (~/.brainsmith/config.yaml) - <code>--project</code> - Create project-level config (./brainsmith_settings.yaml) - <code>--force, -f</code> - Overwrite existing file - <code>--full</code> - Include all possible configuration fields</p> <p>Examples: <pre><code># Create project configuration\nbrainsmith config init --project\n\n# Create user defaults\nbrainsmith config init --user --force\n\n# Create full configuration with all fields\nbrainsmith config init --full\n</code></pre></p>"},{"location":"cli_api_reference/#brainsmith-config-export","title":"<code>brainsmith config export</code>","text":"<p>Export configuration as shell environment script.</p> <pre><code>brainsmith config export [OPTIONS]\n</code></pre> <p>Options: - <code>--shell [bash|zsh|fish|powershell]</code> - Shell format (default: bash)</p> <p>Examples: <pre><code># Export for current shell\neval $(brainsmith config export)\n\n# Export for fish shell\neval (brainsmith config export --shell fish)\n</code></pre></p>"},{"location":"cli_api_reference/#setup-and-dependencies","title":"Setup and Dependencies","text":""},{"location":"cli_api_reference/#brainsmith-setup-system-setup","title":"<code>brainsmith setup</code> - System Setup","text":""},{"location":"cli_api_reference/#brainsmith-setup-all","title":"<code>brainsmith setup all</code>","text":"<p>Install all dependencies (C++ simulation, Xilinx simulation, board files).</p> <pre><code>brainsmith setup all [OPTIONS]\n</code></pre> <p>Options: - <code>--force, -f</code> - Force reinstallation even if already present</p>"},{"location":"cli_api_reference/#brainsmith-setup-cppsim","title":"<code>brainsmith setup cppsim</code>","text":"<p>Setup C++ simulation dependencies (cnpy, finn-hlslib).</p> <pre><code>brainsmith setup cppsim [OPTIONS]\n</code></pre> <p>Options: - <code>--force, -f</code> - Force reinstallation</p>"},{"location":"cli_api_reference/#brainsmith-setup-xsim","title":"<code>brainsmith setup xsim</code>","text":"<p>Setup Xilinx simulation (build finn-xsim with Vivado).</p> <pre><code>brainsmith setup xsim [OPTIONS]\n</code></pre> <p>Options: - <code>--force, -f</code> - Force rebuild</p>"},{"location":"cli_api_reference/#brainsmith-setup-boards","title":"<code>brainsmith setup boards</code>","text":"<p>Download FPGA board definition files.</p> <pre><code>brainsmith setup boards [OPTIONS]\n</code></pre> <p>Options: - <code>--force, -f</code> - Force redownload - <code>--repo, -r [xilinx|avnet|realdigital]</code> - Specific repository to download - <code>--verbose, -v</code> - Show detailed list of all board files</p>"},{"location":"cli_api_reference/#brainsmith-setup-check","title":"<code>brainsmith setup check</code>","text":"<p>Check the status of all setup components.</p> <pre><code>brainsmith setup check [OPTIONS]\n</code></pre> <p>Options: - <code>--verbose, -v</code> - Show detailed information</p> <p>Example output: <pre><code>Setup Status:\n\u2713 cnpy (C++ NPY support)\n\u2713 finn-hlslib headers\n\u2713 finn-xsim\n\u2713 Vivado 2024.2\n\u2713 Vitis HLS 2024.2\n\u2713 Board files (47 boards)\n</code></pre></p>"},{"location":"cli_api_reference/#design-space-exploration","title":"Design Space Exploration","text":""},{"location":"cli_api_reference/#smith-default-command","title":"<code>smith</code> (default command)","text":"<p>Run design space exploration for neural network acceleration.</p> <pre><code>smith MODEL BLUEPRINT [OPTIONS]\n</code></pre> <p>Arguments: - <code>MODEL</code> - Path to ONNX model file - <code>BLUEPRINT</code> - Path to Blueprint YAML file defining exploration strategy</p> <p>Options: - <code>--output-dir, -o PATH</code> - Output directory (defaults to build dir with timestamp)</p> <p>Examples: <pre><code># Basic DSE\nsmith model.onnx blueprint.yaml\n\n# With custom output directory\nsmith model.onnx blueprint.yaml --output-dir ./results\n\n# Using brainsmith context with debug mode\nbrainsmith --debug smith model.onnx blueprint.yaml\n</code></pre></p>"},{"location":"cli_api_reference/#hardware-kernel-generation","title":"Hardware Kernel Generation","text":""},{"location":"cli_api_reference/#smith-kernel","title":"<code>smith kernel</code>","text":"<p>Generate hardware kernel from RTL for FINN integration.</p> <pre><code>smith kernel RTL_FILE [OPTIONS]\n</code></pre> <p>Arguments: - <code>RTL_FILE</code> - Path to SystemVerilog RTL source file with embedded pragmas</p> <p>Options: - <code>--output-dir, -o PATH</code> - Directory for generated files (default: RTL file location) - <code>--validate</code> - Validate RTL only without generating files - <code>--info</code> - Display parsed kernel metadata and exit - <code>--artifacts [autohwcustomop|rtlbackend|wrapper]</code> - Generate specific files only - <code>--no-strict</code> - Disable strict validation - <code>--include-rtl PATH</code> - Additional RTL files to include (can specify multiple) - <code>--rtl-path PATHS</code> - Colon-separated paths to search for RTL files - <code>--verbose, -v</code> - Enable verbose output</p> <p>Examples: <pre><code># Generate kernel files\nsmith kernel my_accelerator.sv\n\n# Validate RTL only\nsmith kernel my_accelerator.sv --validate\n\n# Generate specific artifacts\nsmith kernel my_accelerator.sv --artifacts autohwcustomop --artifacts wrapper\n\n# With additional RTL files\nsmith kernel top.sv --include-rtl helper.sv --include-rtl memory.sv\n</code></pre></p>"},{"location":"cli_api_reference/#configuration-settings","title":"Configuration Settings","text":""},{"location":"cli_api_reference/#settings-hierarchy","title":"Settings Hierarchy","text":"<p>Configuration is resolved in the following priority order (highest to lowest):</p> <ol> <li>Command-line arguments - Direct CLI options</li> <li>Environment variables - <code>BSMITH_*</code> prefixed variables</li> <li>Project configuration - <code>./brainsmith_settings.yaml</code></li> <li>User configuration - <code>~/.brainsmith/config.yaml</code></li> <li>Built-in defaults - Hardcoded in the application</li> </ol>"},{"location":"cli_api_reference/#available-settings","title":"Available Settings","text":""},{"location":"cli_api_reference/#core-settings","title":"Core Settings","text":"<ul> <li><code>debug</code> (bool) - Enable DEBUG-level logging and detailed error traces</li> <li><code>build_dir</code> (path) - Build directory for artifacts</li> <li><code>deps_dir</code> (path) - Dependencies directory</li> </ul>"},{"location":"cli_api_reference/#xilinx-tools","title":"Xilinx Tools","text":"<ul> <li><code>xilinx_path</code> (path) - Xilinx root installation path</li> <li><code>xilinx_version</code> (string) - Xilinx tool version (e.g., \"2024.2\")</li> <li><code>vivado_path</code> (path) - Path to Vivado (auto-detected)</li> <li><code>vitis_path</code> (path) - Path to Vitis (auto-detected)</li> <li><code>vitis_hls_path</code> (path) - Path to Vitis HLS (auto-detected)</li> </ul>"},{"location":"cli_api_reference/#tool-settings","title":"Tool Settings","text":"<ul> <li><code>platform_repo_paths</code> (string) - Platform repository paths</li> <li><code>plugins_strict</code> (bool) - Strict plugin loading</li> <li><code>vivado_ip_cache</code> (path) - Vivado IP cache directory</li> <li><code>netron_port</code> (int) - Port for Netron visualization</li> </ul>"},{"location":"cli_api_reference/#finn-configuration","title":"FINN Configuration","text":"<ul> <li><code>finn.finn_root</code> (path) - FINN root directory</li> <li><code>finn.finn_build_dir</code> (path) - FINN build directory</li> <li><code>finn.finn_deps_dir</code> (path) - FINN dependencies directory</li> <li><code>finn.num_default_workers</code> (int) - Default number of workers</li> </ul>"},{"location":"cli_api_reference/#environment-variables","title":"Environment Variables","text":"<p>All settings can be overridden using environment variables with the <code>BSMITH_</code> prefix:</p> <pre><code>export BSMITH_DEBUG=true\nexport BSMITH_BUILD_DIR=/tmp/builds\nexport BSMITH_XILINX_VERSION=2024.2\nexport BSMITH_FINN__NUM_DEFAULT_WORKERS=8\n</code></pre> <p>Note: Nested settings use double underscore (<code>__</code>) as delimiter.</p>"},{"location":"cli_api_reference/#usage-patterns","title":"Usage Patterns","text":""},{"location":"cli_api_reference/#initial-setup","title":"Initial Setup","text":"<pre><code># Install all dependencies\nbrainsmith setup all\n\n# Initialize user configuration\nbrainsmith config init --user\n\n# Edit ~/.brainsmith/config.yaml to configure Xilinx tools as needed\n\n# Verify setup\nbrainsmith setup check\n</code></pre>"},{"location":"cli_api_reference/#daily-workflow","title":"Daily Workflow","text":"<pre><code># Run DSE with user defaults\nsmith model.onnx blueprint.yaml\n\n# Generate hardware kernel\nsmith kernel accelerator.sv\n\n# Override settings temporarily\nbrainsmith --build-dir /tmp/test smith model.onnx blueprint.yaml\n</code></pre>"},{"location":"cli_api_reference/#team-collaboration","title":"Team Collaboration","text":"<pre><code># Create project configuration\nbrainsmith config init --project\n\n# Team members use project settings\nsmith model.onnx blueprint.yaml\n\n# Check active configuration\nbrainsmith config show --verbose\n</code></pre>"},{"location":"cli_api_reference/#cicd-integration","title":"CI/CD Integration","text":"<pre><code># Export configuration for CI scripts\neval $(brainsmith env activate)\n\n# All tools now have correct paths\nvivado -version\nvitis_hls -version\n\n# Run automated builds\nsmith model.onnx blueprint.yaml --output-dir $CI_ARTIFACTS_DIR\n</code></pre>"},{"location":"cli_api_reference/#troubleshooting","title":"Troubleshooting","text":""},{"location":"cli_api_reference/#check-configuration","title":"Check Configuration","text":"<pre><code># See all active settings\nbrainsmith config show --verbose\n\n# Check environment variables\nbrainsmith env show\n\n# Verify tool detection\nbrainsmith setup check --verbose\n</code></pre>"},{"location":"cli_api_reference/#debug-mode","title":"Debug Mode","text":"<pre><code># Enable DEBUG-level logging and detailed error information\nbrainsmith --debug smith model.onnx blueprint.yaml\n\n# Or edit config to set debug: true as default\n</code></pre>"},{"location":"cli_api_reference/#reset-configuration","title":"Reset Configuration","text":"<pre><code># Remove user configuration\nrm ~/.brainsmith/config.yaml\n\n# Remove project configuration\nrm brainsmith_settings.yaml\n\n# Reinitialize\nbrainsmith config init --user\n</code></pre>"},{"location":"cli_api_reference/#migration-from-legacy-cli","title":"Migration from Legacy CLI","text":"<p>The previous single <code>smith</code> command structure is still supported:</p> Old Command New Command <code>smith config show</code> <code>brainsmith config show</code> <code>smith config export</code> <code>brainsmith config export</code> <code>smith setup all</code> <code>brainsmith setup all</code> <code>smith model.onnx bp.yaml</code> <code>smith model.onnx bp.yaml</code> (unchanged) <code>smith kernel rtl.sv</code> <code>smith kernel rtl.sv</code> (unchanged)"},{"location":"cli_api_reference/#future-commands","title":"Future Commands","text":"<p>These commands are planned for future releases:</p>"},{"location":"cli_api_reference/#smith-run","title":"<code>smith run</code>","text":"<p>Run compiled model on hardware. <pre><code>smith run COMPILED_MODEL [--device DEVICE]\n</code></pre></p>"},{"location":"cli_api_reference/#smith-build","title":"<code>smith build</code>","text":"<p>Build complete acceleration package. <pre><code>smith build MODEL STRATEGY [--target alveo|zynq|versal]\n</code></pre></p>"},{"location":"contributing/","title":"Contributing to Brainsmith","text":"<p>We welcome contributions! This guide will help you get started.</p>"},{"location":"contributing/#development-setup","title":"Development Setup","text":"<ol> <li>Fork the repository</li> <li>Clone your fork:    <pre><code>git clone https://github.com/YOUR_USERNAME/brainsmith.git\ncd brainsmith\n</code></pre></li> <li>Run setup:    <pre><code>./setup-venv.sh\nsource .venv/bin/activate\n</code></pre></li> <li>Configure Vivado paths:    <pre><code>brainsmith config init\n# Edit ~/.brainsmith/config.yaml\n</code></pre></li> </ol>"},{"location":"contributing/#running-tests","title":"Running Tests","text":"<pre><code># All tests\npytest tests/\n\n# Specific test file\npytest tests/integration/test_plugin_system.py\n\n# With coverage\npytest tests/ --cov=brainsmith.core\n</code></pre>"},{"location":"contributing/#code-style","title":"Code Style","text":"<p>We use minimal linting rules during the alpha phase:</p> <pre><code># Check code\nruff check brainsmith/ tests/\n\n# Auto-fix issues\nruff check --fix brainsmith/ tests/\n\n# Format code\nruff format brainsmith/ tests/\n</code></pre>"},{"location":"contributing/#style-guidelines","title":"Style Guidelines","text":"<ul> <li>Line length: 100 characters</li> <li>Python 3.10+ required</li> <li>Docstring style: Google format preferred</li> </ul>"},{"location":"contributing/#making-changes","title":"Making Changes","text":"<ol> <li> <p>Create a feature branch:    <pre><code>git checkout -b feature/my-feature\n</code></pre></p> </li> <li> <p>Make your changes</p> </li> <li> <p>Add tests for new functionality</p> </li> <li> <p>Run tests and linting:    <pre><code>pytest tests/\nruff check brainsmith/ tests/\n</code></pre></p> </li> <li> <p>Commit with descriptive messages:    <pre><code>git commit -m \"Add feature X that does Y\"\n</code></pre></p> </li> <li> <p>Push to your fork:    <pre><code>git push origin feature/my-feature\n</code></pre></p> </li> <li> <p>Open a Pull Request on GitHub</p> </li> </ol>"},{"location":"contributing/#pr-guidelines","title":"PR Guidelines","text":"<ul> <li>Describe what your PR does and why</li> <li>Reference any related issues</li> <li>Ensure all tests pass</li> <li>Add documentation for new features</li> <li>Keep PRs focused and reasonably sized</li> </ul>"},{"location":"contributing/#documentation","title":"Documentation","text":"<p>When adding features, update documentation:</p> <ul> <li>Add/update docstrings</li> <li>Update relevant markdown docs in <code>docs/</code></li> <li>Add examples if applicable</li> </ul> <p>Build docs locally:</p> <pre><code>mkdocs serve\n</code></pre> <p>Then open http://127.0.0.1:8000</p>"},{"location":"contributing/#questions","title":"Questions?","text":"<ul> <li>Open an issue</li> <li>Start a discussion</li> </ul>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>Please be respectful and follow our Code of Conduct.</p>"},{"location":"design_space_exploration/","title":"Execution Tree and Design Space Exploration","text":"<p>The execution tree organizes how Brainsmith explores the design space for neural network accelerators. Each path through the tree represents different design choices (kernel implementations, optimization strategies, parallelization parameters).</p>"},{"location":"design_space_exploration/#tiered-design-space-exploration","title":"Tiered Design Space Exploration","text":"<ul> <li>Global Design Space - All potential dataflow architectures to implement a neural network on FPGA</li> <li>Build Search Space - All potential implementations of a single dataflow architecture.</li> </ul> Local Search Space\u00a0(FINN) Global Design Space\u00a0(Brainsmith) Network optimizations Platform\u202f(board, <code>fpga_part</code>) FIFO sizing Kernel implementations Kernel parallelism\u202f DSE model transforms\u202f(streamlining) Kernel variations\u202f(RTL\u202fvs\u202fHLS,\u202fLUT\u202fvs\u202fDSP) DSE HW transforms\u202f(auto\u2011folding) HW targets\u202f(e.g., <code>target clk</code>)"},{"location":"design_space_exploration/#execution-tree-architecture","title":"Execution Tree Architecture","text":"<p>Design space exploration often involves paths with significant overlap, differing only in specific optimizations or kernel choices. The execution tree exploits this by merging shared pipeline segments and splitting at branch points, enabling artifact reuse and reducing redundant computation.</p> <pre><code>                                          \u250c\u2192 step_minimize_bit_width \u2192 step_hw_codegen\ncleanup \u2192 qonnx_to_finn \u2192 infer_kernels \u2192\u2524\n                                          \u2514\u2192 step_apply_folding_config \u2192 step_hw_codegen\n</code></pre> <p>Steps are collected into segments of contiguous, non-branching steps that are run as a single FINN build.</p>"},{"location":"design_space_exploration/#segments","title":"Segments","text":"<p>Segments group contiguous transformations that execute together in a single FINN build, reducing overhead and enabling efficient caching.</p> <pre><code>@dataclass\nclass DSESegment:\n    transforms: List[Dict[str, Any]]     # Steps to execute\n    branch_choice: Optional[str]         # Which branch was taken\n    parent: Optional['DSESegment']\n    children: Dict[str, 'DSESegment']    # branch_id \u2192 child\n\n    @property\n    def segment_id(self) -&gt; str:        # Path-based ID like \"root\" or \"streamline/fold\"\n    @property\n    def is_branch_point(self) -&gt; bool:  # Has multiple children\n</code></pre>"},{"location":"design_space_exploration/#tree-structure","title":"Tree Structure","text":"<ul> <li>Sequential steps \u2192 Single segment</li> <li>Alternatives \u2192 Branch points with child segments</li> <li>Kernel inference \u2192 Expands to kernel/backend combinations</li> <li>Skip option \u2192 Use <code>~</code> to create optional paths</li> </ul> <pre><code>steps:\n  - \"qonnx_to_finn\"      # These become\n  - \"cleanup\"            # one segment\n  - [\"step_minimize_bit_width\", \"step_apply_folding_config\"]  # Branch point\n  - [\"~\", \"step_set_fifo_depths\"]  # Optional step (skip or execute)\n\nkernels:\n  - LayerNorm: [LayerNorm_hls, LayerNorm_rtl]  # Creates paths\n</code></pre> <p>The skip indicator <code>~</code> allows creating paths that bypass certain optimizations, useful for comparing performance with and without specific transformations.</p>"},{"location":"design_space_exploration/#execution-flow","title":"Execution Flow","text":""},{"location":"design_space_exploration/#1-tree-building","title":"1. Tree Building","text":"<p><code>DSETreeBuilder</code> in <code>brainsmith/core/design/builder.py</code> converts blueprint \u2192 execution tree: - Groups sequential steps into segments - Creates branches for alternatives - Expands kernel inference into transforms</p>"},{"location":"design_space_exploration/#2-traversal","title":"2. Traversal","text":"<p><code>runner.py</code> executes segments using a stack-based approach: <pre><code>stack = [(tree.root, initial_model, 0)]\nwhile stack:\n    segment, input_model, depth = stack.pop()\n\n    # Execute segment (or use cached result)\n    result = self._execute_segment(segment, input_model, output_dir)\n\n    # Queue children for execution\n    if result.success:\n        for child in segment.children.values():\n            stack.append((child, result.output_model, depth + 1))\n</code></pre></p>"},{"location":"design_space_exploration/#3-segment-execution","title":"3. Segment Execution","text":"<p>Each segment = one FINN build: 1. Create output directory: <code>output_dir/segment_id/</code> 2. Check cache (skip if valid output exists) 3. Prepare FINN config 4. Run transformations 5. Discover output in <code>intermediate_models/</code></p>"},{"location":"design_space_exploration/#4-artifact-sharing","title":"4. Artifact Sharing","text":"<p>At branch points, the parent's output is shared with all children to avoid redundant computation: <pre><code>def share_artifacts_at_branch(\n    parent_result: SegmentResult,\n    child_segments: List[DSESegment],\n    base_output_dir: Path\n) -&gt; None:\n    \"\"\"Copy build artifacts to child segments.\"\"\"\n    if not parent_result.success:\n        return\n\n    for child in child_segments:\n        child_dir = base_output_dir / child.segment_id\n        # Full directory copy for FINN compatibility\n        if child_dir.exists():\n            shutil.rmtree(child_dir)\n        shutil.copytree(parent_result.output_dir, child_dir)\n</code></pre></p>"},{"location":"docs-implementation-plan/","title":"Documentation Website Implementation Plan","text":""},{"location":"docs-implementation-plan/#phase-1-initial-configuration-proof-of-concept","title":"Phase 1: Initial Configuration &amp; Proof-of-Concept","text":"<p>Goal: Set up Material for MkDocs with a working proof-of-concept deployable to GitHub Pages</p> <p>Estimated Time: 3-4 hours</p>"},{"location":"docs-implementation-plan/#step-1-install-dependencies-15-mins","title":"Step 1: Install Dependencies (15 mins)","text":""},{"location":"docs-implementation-plan/#11-add-to-pyprojecttoml","title":"1.1 Add to pyproject.toml","text":"<p>Add documentation dependencies to a new <code>[tool.poetry.group.docs]</code> section:</p> <pre><code>[tool.poetry.group.docs]\noptional = true\n\n[tool.poetry.group.docs.dependencies]\nmkdocs = \"~=1.6.0\"\nmkdocs-material = \"~=9.5.0\"\nmkdocstrings = {extras = [\"python\"], version = \"~=0.25.0\"}\nmkdocs-git-revision-date-localized-plugin = \"~=1.2.0\"\nmkdocs-minify-plugin = \"~=0.8.0\"\nmike = \"~=2.1.0\"\npymdown-extensions = \"~=10.8.0\"\n</code></pre>"},{"location":"docs-implementation-plan/#12-install-dependencies","title":"1.2 Install Dependencies","text":"<pre><code>poetry install --with docs\n</code></pre>"},{"location":"docs-implementation-plan/#step-2-create-mkdocs-configuration-30-mins","title":"Step 2: Create MkDocs Configuration (30 mins)","text":""},{"location":"docs-implementation-plan/#21-create-mkdocsyml","title":"2.1 Create mkdocs.yml","text":"<p>Create <code>mkdocs.yml</code> in the project root:</p> <pre><code>site_name: Brainsmith\nsite_description: From PyTorch to RTL - FPGA Accelerator Compiler for AI\nsite_author: Microsoft &amp; AMD\nsite_url: https://microsoft.github.io/brainsmith/  # Update with actual URL\n\nrepo_name: microsoft/brainsmith\nrepo_url: https://github.com/microsoft/brainsmith\nedit_uri: edit/main/docs/\n\ncopyright: Copyright &amp;copy; Microsoft Corporation\n\ntheme:\n  name: material\n  palette:\n    # Light mode\n    - media: \"(prefers-color-scheme: light)\"\n      scheme: default\n      primary: indigo\n      accent: blue\n      toggle:\n        icon: material/brightness-7\n        name: Switch to dark mode\n    # Dark mode\n    - media: \"(prefers-color-scheme: dark)\"\n      scheme: slate\n      primary: indigo\n      accent: blue\n      toggle:\n        icon: material/brightness-4\n        name: Switch to light mode\n\n  features:\n    - navigation.instant      # Fast page loads\n    - navigation.tracking     # URL updates with scroll\n    - navigation.tabs         # Top-level tabs\n    - navigation.sections     # Collapsible sections\n    - navigation.expand       # Expand all sections\n    - navigation.top          # Back to top button\n    - search.suggest          # Search suggestions\n    - search.highlight        # Highlight search terms\n    - content.code.copy       # Copy button for code blocks\n    - content.code.annotate   # Annotations in code blocks\n\n  icon:\n    repo: fontawesome/brands/github\n\nplugins:\n  - search\n  - mkdocstrings:\n      handlers:\n        python:\n          options:\n            docstring_style: google\n            show_source: true\n            show_root_heading: true\n            show_root_full_path: false\n            show_signature_annotations: true\n            merge_init_into_class: true\n            heading_level: 2\n  - git-revision-date-localized:\n      enable_creation_date: true\n      type: timeago\n\nmarkdown_extensions:\n  # Code highlighting\n  - pymdownx.highlight:\n      anchor_linenums: true\n      line_spans: __span\n      pygments_lang_class: true\n  - pymdownx.inlinehilite\n  - pymdownx.superfences:\n      custom_fences:\n        - name: mermaid\n          class: mermaid\n          format: !!python/name:pymdownx.superfences.fence_code_format\n\n  # Content features\n  - pymdownx.tabbed:\n      alternate_style: true\n  - admonition\n  - pymdownx.details\n  - attr_list\n  - md_in_html\n  - def_list\n  - pymdownx.tasklist:\n      custom_checkbox: true\n\n  # Navigation\n  - toc:\n      permalink: true\n      toc_depth: 3\n\n  # Typography\n  - pymdownx.emoji:\n      emoji_index: !!python/name:material.extensions.emoji.twemoji\n      emoji_generator: !!python/name:material.extensions.emoji.to_svg\n\nnav:\n  - Home: index.md\n  - Getting Started:\n    - Installation: getting-started/installation.md\n    - Quick Start: getting-started/quickstart.md\n    - Configuration: getting-started/configuration.md\n  - Architecture:\n    - Overview: architecture/overview.md\n    - Plugin System: architecture/plugin-system.md\n  - API Reference:\n    - Core: api-reference/core.md\n    - Plugins: api-reference/plugins.md\n  - Contributing: contributing.md\n\nextra:\n  social:\n    - icon: fontawesome/brands/github\n      link: https://github.com/microsoft/brainsmith\n    - icon: fontawesome/brands/python\n      link: https://pypi.org/project/brainsmith/\n  version:\n    provider: mike\n</code></pre>"},{"location":"docs-implementation-plan/#step-3-create-initial-documentation-structure-45-mins","title":"Step 3: Create Initial Documentation Structure (45 mins)","text":""},{"location":"docs-implementation-plan/#31-create-docs-directory-structure","title":"3.1 Create docs/ directory structure","text":"<pre><code>mkdir -p docs/getting-started\nmkdir -p docs/architecture\nmkdir -p docs/api-reference\n</code></pre>"},{"location":"docs-implementation-plan/#32-create-indexmd-landing-page","title":"3.2 Create index.md (Landing Page)","text":"<p>Create <code>docs/index.md</code>:</p> <pre><code># Brainsmith\n\n**From PyTorch to RTL - FPGA Accelerator Compiler for AI**\n\nBrainsmith automates design space exploration (DSE) and implementation of neural networks on FPGA, from PyTorch to RTL. It builds on FINN, QONNX, and Brevitas to create dataflow accelerators with tunable parameters.\n\n---\n\n## Pre-Release\n\n!!! warning \"Pre-Release Status\"\n    This repository is in a pre-release state and under active co-development by Microsoft and AMD.\n\n## Key Features\n\n- **Plugin System** - Extensible architecture for registering custom kernels, transforms, and build steps\n- **Blueprint Interface** - YAML-based declarative configuration with inheritance support\n- **Segment-based Execution** - Efficient DSE through intelligent computation reuse\n- **BERT Demo** - Example end-to-end acceleration (PyTorch to stitched-IP RTL)\n\n## Quick Links\n\n&lt;div class=\"grid cards\" markdown&gt;\n\n- :material-clock-fast: **[Quick Start](getting-started/quickstart.md)**\n\n    Get up and running in minutes\n\n- :material-cog: **[Installation](getting-started/installation.md)**\n\n    Set up your development environment\n\n- :material-book-open-variant: **[Architecture](architecture/overview.md)**\n\n    Understand how Brainsmith works\n\n- :material-code-braces: **[API Reference](api-reference/core.md)**\n\n    Explore the codebase\n\n&lt;/div&gt;\n\n## Architecture Overview\n\n```mermaid\ngraph LR\n    A[PyTorch Model] --&gt; B[Brevitas Quantization]\n    B --&gt; C[ONNX Export]\n    C --&gt; D[QONNX]\n    D --&gt; E[Brainsmith DSE]\n    E --&gt; F[FINN Dataflow]\n    F --&gt; G[RTL Generation]\n    G --&gt; H[Vivado Synthesis]\n    H --&gt; I[FPGA Bitstream]\n</code></pre>"},{"location":"docs-implementation-plan/#example-design-space-exploration","title":"Example: Design Space Exploration","text":"<pre><code># Run DSE with ONNX model and blueprint\nsmith model.onnx blueprint.yaml --output-dir ./results\n</code></pre>"},{"location":"docs-implementation-plan/#built-with","title":"Built With","text":"<p>Brainsmith builds upon:</p> <ul> <li>FINN - Dataflow compiler for quantized neural networks</li> <li>QONNX - Quantized ONNX representation</li> <li>Brevitas - PyTorch quantization library</li> </ul>"},{"location":"docs-implementation-plan/#license","title":"License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"docs-implementation-plan/#acknowledgments","title":"Acknowledgments","text":"<p>Brainsmith is developed through a collaboration between Microsoft and AMD. <pre><code>### 3.3 Create getting-started/installation.md\n\n```markdown\n# Installation\n\nThis guide will help you set up Brainsmith for development.\n\n## Prerequisites\n\n- Ubuntu 22.04+\n- Python 3.10+\n- Vivado Design Suite 2024.2\n- [Poetry](https://python-poetry.org/docs/#installation)\n\n## Poetry Environment Setup\n\n### Automated Setup\n\n```bash\n# Run automated setup script\n./setup-venv.sh\n\n# Activate virtual environment\nsource .venv/bin/activate\n</code></pre></p>"},{"location":"docs-implementation-plan/#manual-setup","title":"Manual Setup","text":"<p>If you prefer manual setup:</p> <pre><code># Install Poetry (if not already installed)\ncurl -sSL https://install.python-poetry.org | python3 -\n\n# Install dependencies\npoetry install\n\n# Activate virtual environment\npoetry shell\n</code></pre>"},{"location":"docs-implementation-plan/#configuration","title":"Configuration","text":""},{"location":"docs-implementation-plan/#initialize-configuration","title":"Initialize Configuration","text":"<pre><code># Create config file\nbrainsmith config init\n</code></pre> <p>This creates <code>~/.brainsmith/config.yaml</code>.</p>"},{"location":"docs-implementation-plan/#edit-configuration","title":"Edit Configuration","text":"<p>Edit <code>~/.brainsmith/config.yaml</code> to set your Xilinx paths:</p> <pre><code>xilinx_path: /opt/Xilinx/Vivado/2024.2\nxilinx_version: 2024.2\nbuild_dir: /tmp/finn_dev_${USER}\n</code></pre>"},{"location":"docs-implementation-plan/#verify-configuration","title":"Verify Configuration","text":"<pre><code># View current configuration\nbrainsmith config show\n\n# Export environment variables\neval $(brainsmith config export)\n</code></pre>"},{"location":"docs-implementation-plan/#validate-installation","title":"Validate Installation","text":"<p>Run the quick validation test:</p> <pre><code>./examples/bert/quicktest.sh\n</code></pre> <p>This runs a minimal BERT example (single layer) to verify everything works.</p>"},{"location":"docs-implementation-plan/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start Guide - Run your first DSE</li> <li>Configuration Guide - Learn about configuration options <pre><code>### 3.4 Create getting-started/quickstart.md\n\n```markdown\n# Quick Start\n\nGet started with Brainsmith in 5 minutes.\n\n## Prerequisites\n\nMake sure you've completed the [installation](installation.md).\n\n## Run Your First DSE\n\n### 1. Prepare Your Model\n\nFor this quickstart, we'll use the included BERT example:\n\n```bash\ncd examples/bert\n</code></pre></li> </ul>"},{"location":"docs-implementation-plan/#2-run-the-quick-test","title":"2. Run the Quick Test","text":"<pre><code>./quicktest.sh\n</code></pre> <p>This will:</p> <ol> <li>Generate a folding configuration for minimal resources</li> <li>Build a single-layer BERT accelerator</li> <li>Run RTL simulation to verify correctness</li> </ol> <p>Build Time</p> <p>The quicktest takes approximately 30-60 minutes, depending on your system.</p>"},{"location":"docs-implementation-plan/#3-explore-results","title":"3. Explore Results","text":"<p>Results are saved in <code>examples/bert/quicktest/</code>:</p> <pre><code>quicktest/\n\u251c\u2500\u2500 model.onnx              # Quantized ONNX model\n\u251c\u2500\u2500 final_output/           # Generated RTL and reports\n\u2502   \u251c\u2500\u2500 stitched_ip/       # Synthesizable RTL\n\u2502   \u2514\u2500\u2500 report/            # Performance estimates\n\u2514\u2500\u2500 build_dataflow.log     # Build log\n</code></pre>"},{"location":"docs-implementation-plan/#understanding-the-output","title":"Understanding the Output","text":""},{"location":"docs-implementation-plan/#performance-report","title":"Performance Report","text":"<p>Check <code>final_output/report/estimate_reports.json</code>:</p> <pre><code>{\n  \"throughput\": \"1234.5 fps\",\n  \"latency\": \"0.81 ms\",\n  \"resources\": {\n    \"LUT\": 12345,\n    \"FF\": 23456,\n    \"BRAM\": 34,\n    \"DSP\": 56\n  }\n}\n</code></pre>"},{"location":"docs-implementation-plan/#rtl-output","title":"RTL Output","text":"<p>The generated RTL is in <code>final_output/stitched_ip/</code>:</p> <ul> <li><code>finn_design_wrapper.v</code> - Top-level module</li> <li><code>*.v</code> - Individual kernel implementations</li> </ul>"},{"location":"docs-implementation-plan/#next-steps_1","title":"Next Steps","text":""},{"location":"docs-implementation-plan/#customize-the-design","title":"Customize the Design","text":"<p>Edit the blueprint to explore different configurations:</p> <pre><code># bert_quicktest.yaml\ndesign_space:\n  kernels:\n    - name: MatMul\n      backends:\n        - matmul_rtl  # Try different backends\n  steps:\n    - step_target_fps_parallelization:\n        target_fps: 100  # Adjust target performance\n</code></pre>"},{"location":"docs-implementation-plan/#run-full-dse","title":"Run Full DSE","text":"<pre><code>smith model.onnx blueprint.yaml --output-dir ./results\n</code></pre>"},{"location":"docs-implementation-plan/#learn-more","title":"Learn More","text":"<ul> <li>Architecture Overview - Understand how Brainsmith works</li> <li>Blueprint Reference - Learn blueprint syntax</li> <li>CLI Reference - Explore CLI commands <pre><code>### 3.5 Create architecture/overview.md\n\n```markdown\n# Architecture Overview\n\nBrainsmith's architecture is built around three core concepts: **Blueprints**, **Plugins**, and **Segment-based DSE**.\n\n## High-Level Architecture\n\n```mermaid\ngraph TB\n    A[Blueprint YAML] --&gt; B[Design Space Parser]\n    B --&gt; C[DSE Tree Builder]\n    C --&gt; D[Exploration Tree]\n    D --&gt; E[Segment Runner]\n    E --&gt; F{Plugin Registry}\n    F --&gt; G[Transforms]\n    F --&gt; H[Kernels]\n    F --&gt; I[Steps]\n    E --&gt; J[Build Artifacts]\n    J --&gt; K[RTL Output]\n</code></pre></li> </ul>"},{"location":"docs-implementation-plan/#core-components","title":"Core Components","text":""},{"location":"docs-implementation-plan/#1-blueprint-system","title":"1. Blueprint System","text":"<p>Blueprints define design spaces in YAML:</p> <pre><code>name: \"My Accelerator\"\ndesign_space:\n  kernels:\n    - MatMul\n    - Conv2d\n  steps:\n    - cleanup\n    - qonnx_to_finn\n    - step_create_dataflow_partition\n</code></pre> <p>Key Features:</p> <ul> <li>Inheritance support (<code>extends: base.yaml</code>)</li> <li>Dynamic step operations (insert, replace, remove)</li> <li>Parameter sweeps for exploration</li> </ul> <p>Location: <code>brainsmith/core/design/</code></p>"},{"location":"docs-implementation-plan/#2-plugin-registry","title":"2. Plugin Registry","text":"<p>A singleton registry manages all extensible components:</p> <pre><code>from brainsmith.core.plugins import transform, kernel, step\n\n@transform(name=\"MyTransform\")\nclass MyTransform:\n    def apply(self, model):\n        # Transform logic\n        pass\n\n@kernel(name=\"MyKernel\")\nclass MyKernel:\n    # Kernel implementation\n    pass\n</code></pre> <p>Plugin Types:</p> <ul> <li>Transforms - Graph transformations</li> <li>Kernels - Hardware operator implementations</li> <li>Backends - RTL/HLS implementations per kernel</li> <li>Steps - Build pipeline operations</li> </ul> <p>Location: <code>brainsmith/core/plugins/registry.py</code></p>"},{"location":"docs-implementation-plan/#3-segment-based-dse","title":"3. Segment-Based DSE","text":"<p>The exploration tree is divided into segments for efficient computation reuse:</p> <pre><code>graph TD\n    A[Root: Model Input] --&gt; B[Segment 1: Preprocessing]\n    B --&gt; C[Segment 2: Kernel Selection]\n    C --&gt; D[Branch A: RTL Backend]\n    C --&gt; E[Branch B: HLS Backend]\n    D --&gt; F[Segment 3a: RTL Codegen]\n    E --&gt; G[Segment 3b: HLS Codegen]</code></pre> <p>Benefits:</p> <ul> <li>Only changed segments re-execute</li> <li>Shared artifacts cached across branches</li> <li>Parallelizable execution (planned)</li> </ul> <p>Location: <code>brainsmith/core/dse/</code></p>"},{"location":"docs-implementation-plan/#compilation-pipeline","title":"Compilation Pipeline","text":"<p>The standard dataflow compilation follows this pipeline:</p> <pre><code>sequenceDiagram\n    participant U as User\n    participant B as Blueprint Parser\n    participant D as DSE Engine\n    participant P as Plugin Registry\n    participant F as FINN\n\n    U-&gt;&gt;B: Load blueprint\n    B-&gt;&gt;D: Build exploration tree\n    D-&gt;&gt;P: Get transforms/kernels\n    P-&gt;&gt;D: Return plugin instances\n    D-&gt;&gt;F: Execute transforms\n    F-&gt;&gt;D: Return transformed model\n    D-&gt;&gt;U: Generate RTL + reports</code></pre>"},{"location":"docs-implementation-plan/#pipeline-stages","title":"Pipeline Stages","text":"<ol> <li>ONNX \u2192 QONNX - Add quantization metadata</li> <li>QONNX \u2192 FINN - Create dataflow partition</li> <li>Kernel Inference - Replace ops with hardware kernels</li> <li>Specialization - Configure kernel parameters</li> <li>Folding - Apply parallelization strategy</li> <li>Bit Width Minimization - Optimize data types</li> <li>Hardware Codegen - Generate RTL/HLS</li> <li>IP Generation - Create Vivado IP blocks</li> <li>FIFO Sizing - Determine buffer depths</li> <li>Stitched IP - Connect kernels with AXI stream</li> <li>RTL Simulation - Verify correctness</li> </ol>"},{"location":"docs-implementation-plan/#configuration-system","title":"Configuration System","text":"<p>Pydantic-based configuration with layered overrides:</p> <pre><code>graph LR\n    A[Built-in Defaults] --&gt; B[Project Config]\n    B --&gt; C[User Config]\n    C --&gt; D[CLI Args]\n    D --&gt; E[Effective Config]</code></pre> <p>Priority Order:</p> <ol> <li>CLI arguments / environment vars (highest)</li> <li>Explicit <code>--config</code> file</li> <li><code>~/.brainsmith/config.yaml</code> (user)</li> <li><code>.brainsmith/config.yaml</code> (project)</li> <li>Built-in defaults (lowest)</li> </ol> <p>Location: <code>brainsmith/config/</code></p>"},{"location":"docs-implementation-plan/#two-cli-design","title":"Two-CLI Design","text":""},{"location":"docs-implementation-plan/#brainsmith-cli","title":"<code>brainsmith</code> CLI","text":"<p>Application-level commands:</p> <ul> <li><code>config init/show/export</code> - Configuration management</li> <li><code>setup all</code> - Dependency installation</li> <li><code>smith ...</code> - Proxy to smith CLI</li> </ul>"},{"location":"docs-implementation-plan/#smith-cli","title":"<code>smith</code> CLI","text":"<p>Operational commands:</p> <ul> <li><code>dse model.onnx blueprint.yaml</code> - Run DSE</li> <li><code>kernel file.sv</code> - Generate kernel from RTL</li> </ul> <p>Location: <code>brainsmith/interface/cli.py</code></p>"},{"location":"docs-implementation-plan/#key-patterns","title":"Key Patterns","text":""},{"location":"docs-implementation-plan/#decorator-based-registration","title":"Decorator-Based Registration","text":"<pre><code>@transform(name=\"CustomTransform\", framework=\"brainsmith\")\nclass CustomTransform(Transformation):\n    pass\n</code></pre>"},{"location":"docs-implementation-plan/#framework-integration","title":"Framework Integration","text":"<p>External transforms from FINN/QONNX are wrapped:</p> <pre><code># Automatically wrapped\nget_transform(\"finn:Streamline\")  # FINN transform\nget_transform(\"qonnx:InferShapes\")  # QONNX transform\n</code></pre>"},{"location":"docs-implementation-plan/#lazy-plugin-loading","title":"Lazy Plugin Loading","text":"<p>Plugins are discovered on first access:</p> <pre><code># First call triggers discovery\ntransform_cls = get_transform(\"MyTransform\")\n</code></pre>"},{"location":"docs-implementation-plan/#next-steps_2","title":"Next Steps","text":"<ul> <li>Plugin System - Deep dive into plugins</li> <li>Segment Execution - DSE tree mechanics (coming soon)</li> <li>Dataflow Pipeline - Compilation stages (coming soon) <pre><code>### 3.6 Create api-reference/core.md\n\n```markdown\n# Core API Reference\n\n::: brainsmith.core\n    options:\n      show_root_heading: true\n      show_source: false\n      heading_level: 2\n\n## Main API\n\n::: brainsmith.core.explore_design_space\n    options:\n      show_root_heading: true\n      heading_level: 3\n\n## DSE Components\n\n::: brainsmith.core.DSESegment\n    options:\n      show_root_heading: true\n      heading_level: 3\n\n::: brainsmith.core.DSETree\n    options:\n      show_root_heading: true\n      heading_level: 3\n\n::: brainsmith.core.SegmentRunner\n    options:\n      show_root_heading: true\n      heading_level: 3\n\n## Design Components\n\n::: brainsmith.core.DesignSpace\n    options:\n      show_root_heading: true\n      heading_level: 3\n\n::: brainsmith.core.parse_blueprint\n    options:\n      show_root_heading: true\n      heading_level: 3\n\n## Configuration\n\n::: brainsmith.core.ForgeConfig\n    options:\n      show_root_heading: true\n      heading_level: 3\n</code></pre></li> </ul>"},{"location":"docs-implementation-plan/#37-create-api-referencepluginsmd","title":"3.7 Create api-reference/plugins.md","text":"<pre><code># Plugin System API Reference\n\nThe plugin system is the core of Brainsmith's extensibility.\n\n## Registry\n\n::: brainsmith.core.plugins.registry.Registry\n    options:\n      show_root_heading: true\n      heading_level: 3\n      members:\n        - register\n        - get\n        - find\n        - all\n        - reset\n\n## Registration Functions\n\n::: brainsmith.core.plugins.registry.plugin\n    options:\n      show_root_heading: true\n      heading_level: 3\n\n::: brainsmith.core.plugins.registry.transform\n    options:\n      show_root_heading: true\n      heading_level: 3\n\n::: brainsmith.core.plugins.registry.kernel\n    options:\n      show_root_heading: true\n      heading_level: 3\n\n::: brainsmith.core.plugins.registry.backend\n    options:\n      show_root_heading: true\n      heading_level: 3\n\n::: brainsmith.core.plugins.registry.step\n    options:\n      show_root_heading: true\n      heading_level: 3\n\n## Access Functions\n\n::: brainsmith.core.plugins.registry.get_transform\n    options:\n      show_root_heading: true\n      heading_level: 3\n\n::: brainsmith.core.plugins.registry.get_kernel\n    options:\n      show_root_heading: true\n      heading_level: 3\n\n::: brainsmith.core.plugins.registry.get_backend\n    options:\n      show_root_heading: true\n      heading_level: 3\n\n::: brainsmith.core.plugins.registry.get_step\n    options:\n      show_root_heading: true\n      heading_level: 3\n\n## Query Functions\n\n::: brainsmith.core.plugins.registry.list_transforms\n    options:\n      show_root_heading: true\n      heading_level: 3\n\n::: brainsmith.core.plugins.registry.list_kernels\n    options:\n      show_root_heading: true\n      heading_level: 3\n\n::: brainsmith.core.plugins.registry.has_transform\n    options:\n      show_root_heading: true\n      heading_level: 3\n</code></pre>"},{"location":"docs-implementation-plan/#38-create-contributingmd","title":"3.8 Create contributing.md","text":"<pre><code># Contributing to Brainsmith\n\nWe welcome contributions! This guide will help you get started.\n\n## Development Setup\n\n1. Fork the repository\n2. Clone your fork:\n   ```bash\n   git clone https://github.com/YOUR_USERNAME/brainsmith.git\n   cd brainsmith\n   ```\n3. Run setup:\n   ```bash\n   ./setup-venv.sh\n   source .venv/bin/activate\n   ```\n4. Configure Vivado paths:\n   ```bash\n   brainsmith config init\n   # Edit ~/.brainsmith/config.yaml\n   ```\n\n## Running Tests\n\n```bash\n# All tests\npytest tests/\n\n# Specific test file\npytest tests/integration/test_plugin_system.py\n\n# With coverage\npytest tests/ --cov=brainsmith.core\n</code></pre>"},{"location":"docs-implementation-plan/#code-style","title":"Code Style","text":"<p>We use minimal linting rules during the alpha phase:</p> <pre><code># Check code\nruff check brainsmith/ tests/\n\n# Auto-fix issues\nruff check --fix brainsmith/ tests/\n\n# Format code\nruff format brainsmith/ tests/\n</code></pre>"},{"location":"docs-implementation-plan/#style-guidelines","title":"Style Guidelines","text":"<ul> <li>Line length: 100 characters</li> <li>Python 3.10+ required</li> <li>Docstring style: Google format preferred</li> </ul>"},{"location":"docs-implementation-plan/#making-changes","title":"Making Changes","text":"<ol> <li> <p>Create a feature branch:    <pre><code>git checkout -b feature/my-feature\n</code></pre></p> </li> <li> <p>Make your changes</p> </li> <li> <p>Add tests for new functionality</p> </li> <li> <p>Run tests and linting:    <pre><code>pytest tests/\nruff check brainsmith/ tests/\n</code></pre></p> </li> <li> <p>Commit with descriptive messages:    <pre><code>git commit -m \"Add feature X that does Y\"\n</code></pre></p> </li> <li> <p>Push to your fork:    <pre><code>git push origin feature/my-feature\n</code></pre></p> </li> <li> <p>Open a Pull Request on GitHub</p> </li> </ol>"},{"location":"docs-implementation-plan/#pr-guidelines","title":"PR Guidelines","text":"<ul> <li>Describe what your PR does and why</li> <li>Reference any related issues</li> <li>Ensure all tests pass</li> <li>Add documentation for new features</li> <li>Keep PRs focused and reasonably sized</li> </ul>"},{"location":"docs-implementation-plan/#documentation","title":"Documentation","text":"<p>When adding features, update documentation:</p> <ul> <li>Add/update docstrings</li> <li>Update relevant markdown docs in <code>docs/</code></li> <li>Add examples if applicable</li> </ul> <p>Build docs locally:</p> <pre><code>mkdocs serve\n</code></pre> <p>Then open http://127.0.0.1:8000</p>"},{"location":"docs-implementation-plan/#questions","title":"Questions?","text":"<ul> <li>Open an issue</li> <li>Start a discussion</li> </ul>"},{"location":"docs-implementation-plan/#code-of-conduct","title":"Code of Conduct","text":"<p>Please be respectful and follow our Code of Conduct. <pre><code>---\n\n## Step 4: Set Up GitHub Actions for Auto-Deploy (30 mins)\n\n### 4.1 Create .github/workflows/docs.yml\n\n```yaml\nname: Deploy Documentation\n\non:\n  push:\n    branches:\n      - main\n  workflow_dispatch:\n\npermissions:\n  contents: write\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          fetch-depth: 0  # Needed for git-revision-date-localized\n\n      - name: Setup Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: '3.10'\n\n      - name: Install Poetry\n        uses: snok/install-poetry@v1\n        with:\n          version: 1.8.0\n          virtualenvs-create: true\n          virtualenvs-in-project: true\n\n      - name: Install dependencies\n        run: |\n          poetry install --with docs\n\n      - name: Build and deploy docs\n        run: |\n          poetry run mkdocs gh-deploy --force\n</code></pre></p>"},{"location":"docs-implementation-plan/#step-5-enable-github-pages-5-mins","title":"Step 5: Enable GitHub Pages (5 mins)","text":"<ol> <li>Go to repository Settings \u2192 Pages</li> <li>Source: Deploy from a branch</li> <li>Branch: <code>gh-pages</code> / <code>/ (root)</code></li> <li>Save</li> </ol>"},{"location":"docs-implementation-plan/#step-6-test-locally-15-mins","title":"Step 6: Test Locally (15 mins)","text":""},{"location":"docs-implementation-plan/#61-build-and-serve-locally","title":"6.1 Build and serve locally","text":"<pre><code># Activate environment\nsource .venv/bin/activate\n\n# Serve with live reload\nmkdocs serve\n</code></pre> <p>Open http://127.0.0.1:8000</p>"},{"location":"docs-implementation-plan/#62-test-all-features","title":"6.2 Test all features","text":"<ul> <li> Navigation works</li> <li> Search works</li> <li> Code blocks render correctly</li> <li> Dark/light mode toggle</li> <li> Mermaid diagrams render</li> <li> API reference pages load</li> <li> Mobile responsive</li> </ul>"},{"location":"docs-implementation-plan/#step-7-deploy-to-github-pages-10-mins","title":"Step 7: Deploy to GitHub Pages (10 mins)","text":""},{"location":"docs-implementation-plan/#71-manual-deployment-first-time","title":"7.1 Manual deployment (first time)","text":"<pre><code># Build and deploy\nmkdocs gh-deploy --force\n</code></pre>"},{"location":"docs-implementation-plan/#72-verify-deployment","title":"7.2 Verify deployment","text":"<p>Visit: <code>https://microsoft.github.io/brainsmith/</code></p>"},{"location":"docs-implementation-plan/#step-8-add-documentation-badge-to-readme-5-mins","title":"Step 8: Add Documentation Badge to README (5 mins)","text":"<p>Add to README.md:</p> <pre><code>[![Documentation](https://img.shields.io/badge/docs-mkdocs-blue)](https://microsoft.github.io/brainsmith/)\n</code></pre>"},{"location":"docs-implementation-plan/#deliverables-checklist","title":"Deliverables Checklist","text":"<ul> <li> <code>mkdocs.yml</code> configured with Material theme</li> <li> Documentation structure created in <code>docs/</code></li> <li> Landing page (<code>index.md</code>) with overview</li> <li> Getting started guides (installation, quickstart)</li> <li> Architecture overview with Mermaid diagrams</li> <li> API reference pages using mkdocstrings</li> <li> Contributing guide</li> <li> GitHub Actions workflow for auto-deploy</li> <li> GitHub Pages enabled</li> <li> Local build tested</li> <li> Initial deployment completed</li> <li> README badge added</li> </ul>"},{"location":"docs-implementation-plan/#success-criteria","title":"Success Criteria","text":"<p>\u2705 Documentation site is live at GitHub Pages URL \u2705 All pages render correctly with proper navigation \u2705 Search functionality works \u2705 API reference auto-generates from docstrings \u2705 Mermaid diagrams display correctly \u2705 Dark/light mode toggle functions \u2705 Mobile responsive design \u2705 Auto-deployment on push to main branch</p>"},{"location":"docs-implementation-plan/#next-steps-phase-2","title":"Next Steps (Phase 2)","text":"<p>After proof-of-concept is working:</p> <ol> <li>Content Migration - Move existing docs to new structure</li> <li>Tutorials - Add hands-on guides with code examples</li> <li>API Coverage - Document all major modules</li> <li>Versioning - Set up mike for version management</li> <li>Advanced Features - Add social cards, more diagrams</li> <li>Search Optimization - Configure search weights and boosts</li> </ol>"},{"location":"docs-implementation-plan/#troubleshooting","title":"Troubleshooting","text":""},{"location":"docs-implementation-plan/#poetry-install-fails","title":"Poetry install fails","text":"<pre><code># Clear cache and retry\npoetry cache clear pypi --all\npoetry install --with docs\n</code></pre>"},{"location":"docs-implementation-plan/#mkdocs-serve-fails","title":"mkdocs serve fails","text":"<pre><code># Check Python version\npython --version  # Should be 3.10+\n\n# Reinstall dependencies\npoetry install --with docs --sync\n</code></pre>"},{"location":"docs-implementation-plan/#github-pages-not-updating","title":"GitHub Pages not updating","text":"<pre><code># Force rebuild\nmkdocs gh-deploy --force\n\n# Check GitHub Actions logs in repo\n</code></pre>"},{"location":"docs-implementation-plan/#mermaid-diagrams-not-rendering","title":"Mermaid diagrams not rendering","text":"<ul> <li>Ensure <code>pymdownx.superfences</code> is configured correctly in mkdocs.yml</li> <li>Check JavaScript console for errors</li> <li>Verify Mermaid syntax is valid</li> </ul>"},{"location":"docs-implementation-plan/#time-breakdown","title":"Time Breakdown","text":"Task Estimated Time Install dependencies 15 mins Configure mkdocs.yml 30 mins Create doc structure 45 mins GitHub Actions setup 30 mins Enable GitHub Pages 5 mins Local testing 15 mins Deploy to GitHub Pages 10 mins Add README badge 5 mins Total ~2.5 hours <p>Add buffer time for troubleshooting: 3-4 hours total</p>"},{"location":"documentation-website-plan/","title":"Brainsmith Documentation Website Plan","text":""},{"location":"documentation-website-plan/#executive-summary","title":"Executive Summary","text":"<p>Recommendation: Material for MkDocs + mkdocstrings</p> <p>This is the modern industry standard for Python projects, used by FastAPI, Pydantic, Typer, Microsoft, Google, NVIDIA, and many top open-source projects.</p>"},{"location":"documentation-website-plan/#core-technology-stack","title":"Core Technology Stack","text":""},{"location":"documentation-website-plan/#1-material-for-mkdocs-base-framework","title":"1. Material for MkDocs (Base framework)","text":"<ul> <li>Beautiful, modern UI that looks professional out-of-the-box</li> <li>Built-in search with offline support</li> <li>Mobile-responsive design</li> <li>Native dark/light mode</li> <li>Live preview during development (<code>mkdocs serve</code>)</li> <li>One-command GitHub Pages deployment (<code>mkdocs gh-deploy</code>)</li> </ul>"},{"location":"documentation-website-plan/#2-mkdocstrings-python-api-documentation","title":"2. mkdocstrings-python (API Documentation)","text":"<ul> <li>Auto-generates API reference from Python docstrings</li> <li>Supports Google, NumPy, and Sphinx docstring styles</li> <li>Automatic cross-references between API objects</li> <li>Type annotation display</li> <li>Inheritance diagrams</li> <li>Perfect for your plugin system, CLI, core modules</li> </ul>"},{"location":"documentation-website-plan/#3-mike-version-management","title":"3. mike (Version Management)","text":"<ul> <li>Multiple documentation versions (v0.1.0-alpha.1, latest, stable)</li> <li>Each version preserved independently via git branches</li> <li>Version selector dropdown in UI</li> <li>Essential for tracking changes across releases</li> </ul>"},{"location":"documentation-website-plan/#key-features-for-brainsmith","title":"Key Features for Brainsmith","text":""},{"location":"documentation-website-plan/#diagrams-visualizations","title":"Diagrams &amp; Visualizations","text":"<ul> <li>Native Mermaid.js integration for:</li> <li>Architecture diagrams (DSE tree, segment execution)</li> <li>Flowcharts (compilation pipeline)</li> <li>Sequence diagrams (plugin registry flow)</li> <li>State diagrams (build stages)</li> <li>No external tools needed - diagrams in markdown code blocks</li> </ul>"},{"location":"documentation-website-plan/#code-highlighting","title":"Code Highlighting","text":"<ul> <li>Multi-language syntax highlighting:</li> <li>Python (core codebase)</li> <li>SystemVerilog/Verilog (RTL kernels)</li> <li>YAML (blueprints)</li> <li>Bash (setup scripts)</li> <li>JSON (configs)</li> </ul>"},{"location":"documentation-website-plan/#advanced-features","title":"Advanced Features","text":"<ul> <li>Content tabs - Show Docker vs Poetry setup side-by-side</li> <li>Admonitions - Notes, warnings, tips for complex concepts</li> <li>Code annotations - Inline explanations in code examples</li> <li>Social cards - Auto-generated preview images for social sharing</li> <li>Navigation sections - Collapsible sidebar for large doc sets</li> <li>Search highlighting - Find content instantly</li> </ul>"},{"location":"documentation-website-plan/#why-this-beats-alternatives","title":"Why This Beats Alternatives","text":""},{"location":"documentation-website-plan/#vs-sphinx","title":"vs Sphinx:","text":"<ul> <li>\u2705 Markdown instead of reStructuredText (easier to write/maintain)</li> <li>\u2705 Instant live reload (no <code>make html</code> rebuilds)</li> <li>\u2705 Modern, beautiful UI out-of-box (Sphinx requires heavy theming)</li> <li>\u2705 Faster setup and simpler configuration</li> <li>\u26a0\ufe0f mkdocstrings bridges the API doc gap that Sphinx traditionally had</li> </ul>"},{"location":"documentation-website-plan/#vs-docusaurus","title":"vs Docusaurus:","text":"<ul> <li>\u2705 Python-native tooling (fits your stack)</li> <li>\u2705 Simpler, no React/Node.js needed</li> <li>\u2705 Better Python API documentation integration</li> </ul>"},{"location":"documentation-website-plan/#vs-read-the-docs","title":"vs Read the Docs:","text":"<ul> <li>\u2705 GitHub Pages hosting (free, integrated with repo)</li> <li>\u2705 More modern design aesthetics</li> <li>\u2705 Better control over customization</li> </ul>"},{"location":"documentation-website-plan/#recommended-plugin-stack","title":"Recommended Plugin Stack","text":"<pre><code># mkdocs.yml\nplugins:\n  - search              # Built-in search\n  - mkdocstrings:       # API docs from docstrings\n      handlers:\n        python:\n          options:\n            docstring_style: google\n            show_source: true\n            show_root_heading: true\n  - mike:               # Version management\n      version_selector: true\n  - git-revision-date-localized  # Last updated dates\n  - minify              # Optimize HTML/CSS/JS\n\nmarkdown_extensions:\n  - pymdownx.superfences:    # Code blocks + Mermaid\n      custom_fences:\n        - name: mermaid\n          class: mermaid\n          format: !!python/name:pymdownx.superfences.fence_code_format\n  - pymdownx.tabbed:         # Content tabs\n      alternate_style: true\n  - pymdownx.highlight       # Code highlighting\n  - pymdownx.inlinehilite    # Inline code\n  - admonition              # Callout boxes\n  - pymdownx.details        # Collapsible admonitions\n  - attr_list               # Button styling\n  - md_in_html              # Markdown in HTML\n  - toc:                    # Table of contents\n      permalink: true\n</code></pre>"},{"location":"documentation-website-plan/#suggested-documentation-structure","title":"Suggested Documentation Structure","text":"<pre><code>docs/\n\u251c\u2500\u2500 index.md                      # Landing page\n\u251c\u2500\u2500 getting-started/\n\u2502   \u251c\u2500\u2500 installation.md          # Setup with Poetry\n\u2502   \u251c\u2500\u2500 quickstart.md            # BERT quicktest walkthrough\n\u2502   \u2514\u2500\u2500 configuration.md         # Config system guide\n\u251c\u2500\u2500 user-guide/\n\u2502   \u251c\u2500\u2500 blueprints.md            # Blueprint system\n\u2502   \u251c\u2500\u2500 design-space.md          # DSE concepts\n\u2502   \u251c\u2500\u2500 cli-reference.md         # smith/brainsmith CLIs\n\u2502   \u2514\u2500\u2500 examples.md              # Common workflows\n\u251c\u2500\u2500 architecture/\n\u2502   \u251c\u2500\u2500 overview.md              # High-level architecture\n\u2502   \u251c\u2500\u2500 plugin-system.md         # Registry &amp; decorators\n\u2502   \u251c\u2500\u2500 segment-execution.md     # DSE tree mechanics\n\u2502   \u2514\u2500\u2500 dataflow-pipeline.md     # ONNX\u2192RTL flow\n\u251c\u2500\u2500 kernel-development/\n\u2502   \u251c\u2500\u2500 kernel-integrator.md     # RTL kernel creation\n\u2502   \u251c\u2500\u2500 pragma-reference.md      # SystemVerilog pragmas\n\u2502   \u2514\u2500\u2500 backend-development.md   # Adding backends\n\u251c\u2500\u2500 api-reference/\n\u2502   \u251c\u2500\u2500 core.md                  # ::: brainsmith.core\n\u2502   \u251c\u2500\u2500 plugins.md               # ::: brainsmith.core.plugins\n\u2502   \u251c\u2500\u2500 kernels.md               # ::: brainsmith.kernels\n\u2502   \u2514\u2500\u2500 transforms.md            # ::: brainsmith.transforms\n\u251c\u2500\u2500 tutorials/\n\u2502   \u251c\u2500\u2500 first-accelerator.md     # End-to-end example\n\u2502   \u251c\u2500\u2500 custom-kernel.md         # Adding new kernel\n\u2502   \u2514\u2500\u2500 blueprint-authoring.md   # Creating blueprints\n\u2514\u2500\u2500 contributing/\n    \u251c\u2500\u2500 development-setup.md\n    \u251c\u2500\u2500 testing.md\n    \u2514\u2500\u2500 code-style.md\n</code></pre>"},{"location":"documentation-website-plan/#implementation-estimate","title":"Implementation Estimate","text":""},{"location":"documentation-website-plan/#initial-setup-2-4-hours","title":"Initial Setup: ~2-4 hours","text":"<ul> <li>Install deps, configure mkdocs.yml</li> <li>Set up GitHub Actions for auto-deploy</li> <li>Create basic structure and landing page</li> </ul>"},{"location":"documentation-website-plan/#content-migration-8-16-hours","title":"Content Migration: ~8-16 hours","text":"<ul> <li>Move existing docs/ content to new structure</li> <li>Convert READMEs to proper pages</li> <li>Add architecture diagrams with Mermaid</li> </ul>"},{"location":"documentation-website-plan/#api-documentation-4-8-hours","title":"API Documentation: ~4-8 hours","text":"<ul> <li>Set up mkdocstrings for all modules</li> <li>Write module-level docstrings</li> <li>Create API reference pages</li> </ul>"},{"location":"documentation-website-plan/#polish-examples-8-16-hours","title":"Polish &amp; Examples: ~8-16 hours","text":"<ul> <li>Add tutorials with code examples</li> <li>Create Mermaid diagrams for key concepts</li> <li>Write quickstart guides</li> </ul> <p>Total: 22-44 hours for comprehensive documentation</p>"},{"location":"documentation-website-plan/#resources-references","title":"Resources &amp; References","text":"<ul> <li>Material for MkDocs Official</li> <li>mkdocstrings Documentation</li> <li>mike Version Management</li> <li>Real Python MkDocs Tutorial</li> <li>MkDocs Catalog of Projects</li> </ul>"},{"location":"documentation-website-plan/#example-projects-using-this-stack","title":"Example Projects Using This Stack","text":"<ul> <li>FastAPI: https://fastapi.tiangolo.com/</li> <li>Pydantic: https://docs.pydantic.dev/</li> <li>Typer: https://typer.tiangolo.com/</li> <li>Textual: https://textual.textualize.io/</li> <li>Prefect: https://docs.prefect.io/</li> </ul>"},{"location":"hardware_kernels/","title":"Brainsmith Hardware Kernels","text":""},{"location":"hardware_kernels/#definition","title":"Definition","text":"<p>Hardware Kernels are synthesizable hardware modules that implement neural network operators. They serve as the foundational components from which Brainsmith constructs accelerators.</p>"},{"location":"hardware_kernels/#interfaces","title":"Interfaces","text":"<p>To ensure complete modularity, kernel interfaces are limited to the following:</p> <ul> <li>Control - Clock and reset (optional second clock for double-pumped designs)</li> <li>Input/Output - AXI-Stream for activations and weights. A minimum of one input and one output.</li> <li>Config - (Optional) Maximum of a single AXI-Lite interface for runtime configuration/debugging</li> </ul> <p>See protocol_validator.py for complete interface signal definitions, and the name suffix rules for RTL interfaces.</p>"},{"location":"hardware_kernels/#why-kernels","title":"Why Kernels?","text":"<p>Brainsmith chooses kernels as its fundamental abstraction for several reasons: 1. Preserve Design Space - AI models are designed and expressed in terms of Layers/nodes, and preserving this design granularity allows for natural extension from AI frameworks like PyTorch/ONNX. 2. Prevent Exponential Explosion - Although there is theoretical value in fully decomposing models to individual operations, this exponentially explodes the design space. 3. Hand Optimization - Allows hardware engineers to design kernels hand-optimized on the kernel scale without requiring deep knowledge of the AI model. This allows for hand-optimized performance that fully generated designs lack, while maintaining flexibility by composing the final hardware graph through Brainsmith.</p>"},{"location":"hardware_kernels/#implementation-examples","title":"Implementation &amp; Examples","text":"<p>Each kernel requires five components for full functionality:</p>"},{"location":"hardware_kernels/#1-rtlhls-source-code","title":"1. RTL/HLS Source Code","text":"<p>RTL/HLS hardware implementations of neural network operations with top-level interfaces matching the required specification. RTL (SystemVerilog) implementations generally provide maximum control while Vitis HLS enables faster development.</p> <ul> <li>RTL example: FINN's <code>thresholding.sv</code> + <code>thresholding_axi.sv</code></li> <li>HLS example: <code>brainsmith/kernels/layernorm/layernorm.hpp</code></li> </ul>"},{"location":"hardware_kernels/#2-codegen-template","title":"2. Codegen Template","text":"<p>RTL/HLS wrapper template for runtime code generation, bridging high-level Kernel Operators with the actual implementation. For RTL templates this usually takes the form of a standard verilog (not SystemVerilog) with key variables replaced with string substitution. For HLS kernels there is a generalized structured kernel template, with different template functions to fill in the Backend Operator. These templates enable FINN to programmatically configure and instantiate hardware blocks with the exact parameters needed for each layer in a neural network.</p> <ul> <li>RTL example: FINN's <code>thresholding_template_wrapper.v</code></li> <li>HLS example: FINN's generic <code>templates.py</code> + <code>brainsmith/kernels/layernorm/layernorm_hls.py</code></li> </ul> <p>PRE-RELEASE NOTE: Templating is undergoing review to standardize &amp; simplify between RTL &amp; HLS Kernels.</p>"},{"location":"hardware_kernels/#3-kernel-operator-hwcustomop","title":"3. Kernel Operator (HWCustomOp)","text":"<p>An ONNX node defining the interface and infrastructure of the kernel, implementing the abstract HWCustomOp. The Kernel Operator defines the contract for streaming dataflow operations with AXI Stream interfaces, managing execution modes (rtlsim/cppsim) and hardware attributes like FIFO depths and partitioning.</p> <ul> <li>Parent class: <code>HWCustomOp</code></li> <li>RTL example: FINN's <code>thresholding.py</code></li> <li>HLS example: <code>brainsmith/kernels/layernorm/layernorm.py</code></li> </ul> <p>PRE-RELEASE NOTE: HWCustomOp is in the process of automation and consolidation to simplify new Kernel implementation.</p>"},{"location":"hardware_kernels/#4-kernel-inference-transform","title":"4. Kernel Inference Transform","text":"<p>Detection and conversion logic to identify compatible ONNX operations in a graph and replace them with the target kernel's HWCustomOp. Each transform performs three essential tasks:  1. Pattern matching - Find all instances of the ONNX op/subgraph your kernel can implement 2. Validation - Ensure operations meet hardware requirements (like integer datatypes and compatible shapes) 3. Conversion - Instantiate configured HWCustomOp instances with extracted parameters</p> <p>Some kernels may require additional pre-processing transforms to prepare the graph for conversion. For example, <code>ExpandNorms</code> breaks the standard ONNX operator <code>LayerNormalization</code> into <code>FuncLayerNorm</code>, <code>Add</code>, and <code>Mul</code> ops so that each component can be lowered to a separate kernel.</p> <ul> <li>Example: <code>brainsmith/kernels/layernorm/infer_layernorm.py</code></li> </ul>"},{"location":"hardware_kernels/#5-backend-operator-rtlbackendhlsbackend","title":"5. Backend Operator (RTLBackend/HLSBackend)","text":"<p>HLSBackend generates C++ code with pragmas that gets synthesized to RTL via Xilinx Vitis HLS, supporting both C++ and RTL simulation modes. RTLBackend works directly with pre-written Verilog modules from finn-rtllib, offering more hardware control but only RTL simulation. Both backends inherit from HWCustomOp and ultimately produce Verilog IP blocks, with HLS trading some control for easier development and RTL providing maximum optimization potential.</p> <ul> <li>Parent class (RTL): <code>RTLBackend</code></li> <li>Parent class (HLS): <code>HLSBackend</code></li> <li>RTL example: FINN's <code>thresholding_rtl.py</code></li> <li>HLS example: <code>brainsmith/kernels/layernorm/layernorm_hls.py</code></li> </ul> <p>PRE-RELEASE NOTE: Backend Operators are undergoing review to standardize &amp; simplify between RTL &amp; HLS Kernels.</p>"},{"location":"kernel-integrator-pragma-reference/","title":"Kernel Integrator Pragma Reference","text":""},{"location":"kernel-integrator-pragma-reference/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Overview</li> <li>Pragma Types</li> <li>TOP_MODULE</li> <li>INCLUDE_RTL</li> <li>BDIM (Block Dimension)</li> <li>SDIM (Stream Dimension)</li> <li>RELATIONSHIP</li> <li>DATATYPE</li> <li>DATATYPE_CONSTRAINT</li> <li>DERIVED_PARAMETER</li> <li>WEIGHT</li> <li>ALIAS</li> <li>AXILITE_PARAM</li> </ul>"},{"location":"kernel-integrator-pragma-reference/#overview","title":"Overview","text":"<p>Pragmas are special single-line comments that provide metadata to the Kernel Integrator. They must follow the format:</p> <pre><code>// @brainsmith &lt;PRAGMA_TYPE&gt; &lt;arguments...&gt;\n</code></pre> <p>All pragmas must: - Start with <code>// @brainsmith</code> (single-line comment) - Be followed by a valid pragma type - Include required arguments in the correct order - Appear before or within the module definition</p>"},{"location":"kernel-integrator-pragma-reference/#pragma-types","title":"Pragma Types","text":""},{"location":"kernel-integrator-pragma-reference/#top_module","title":"TOP_MODULE","text":"<p>Specifies which module to process when multiple modules exist in a file.</p> <p>Syntax: <pre><code>// @brainsmith TOP_MODULE &lt;module_name&gt;\n</code></pre></p> <p>Arguments: - <code>module_name</code> - Name of the module to process</p> <p>Example: <pre><code>// @brainsmith TOP_MODULE thresholding_axi\n</code></pre></p> <p>Notes: - Required only when RTL file contains multiple modules - Must match exact module name in the RTL</p>"},{"location":"kernel-integrator-pragma-reference/#include_rtl","title":"INCLUDE_RTL","text":"<p>Specifies additional RTL files to include in the generated wrapper.</p> <p>Syntax: <pre><code>// @brainsmith INCLUDE_RTL &lt;file_path&gt;\n</code></pre></p> <p>Arguments: - <code>file_path</code> - Path to RTL file (absolute or relative)</p> <p>Examples: <pre><code>// @brainsmith INCLUDE_RTL helper_functions.sv\n// @brainsmith INCLUDE_RTL ../common/axi_infrastructure.v\n// @brainsmith INCLUDE_RTL /opt/rtl_lib/protocols/axi_lite.sv\n</code></pre></p> <p>Notes: - Files are included in order specified - Paths are resolved relative to main RTL file - Use for dependencies and helper modules</p>"},{"location":"kernel-integrator-pragma-reference/#bdim-block-dimension","title":"BDIM (Block Dimension)","text":"<p>Defines block-level tiling dimensions for an interface.</p> <p>Syntax: <pre><code>// @brainsmith BDIM &lt;interface&gt; &lt;attribute_name&gt; SHAPE=&lt;shape_expr&gt;\n</code></pre></p> <p>Arguments: - <code>interface</code> - Interface name - <code>attribute_name</code> - Name for the dimension attribute - <code>shape_expr</code> - Python list expression for shape</p> <p>Examples: <pre><code>// @brainsmith BDIM input input_bdim SHAPE=[CHANNELS]\n// @brainsmith BDIM output output_bdim SHAPE=[NUM_OUTPUTS]\n// @brainsmith BDIM weights weight_bdim SHAPE=[KERNEL_H, KERNEL_W]\n</code></pre></p> <p>Notes: - Shape expressions can reference RTL parameters - Used for FINN's dataflow analysis - Affects memory layout and parallelism</p>"},{"location":"kernel-integrator-pragma-reference/#sdim-stream-dimension","title":"SDIM (Stream Dimension)","text":"<p>Defines stream-level tiling dimensions for an interface.</p> <p>Syntax: <pre><code>// @brainsmith SDIM &lt;interface&gt; &lt;attribute_name&gt; SHAPE=&lt;shape_expr&gt;\n</code></pre></p> <p>Arguments: - <code>interface</code> - Interface name - <code>attribute_name</code> - Name for the dimension attribute - <code>shape_expr</code> - Python list expression for shape</p> <p>Examples: <pre><code>// @brainsmith SDIM input input_sdim SHAPE=[PE]\n// @brainsmith SDIM output output_sdim SHAPE=[NPE]\n// @brainsmith SDIM input simd SHAPE=[SIMD_WIDTH]\n</code></pre></p> <p>Notes: - Represents parallelism within the stream - Must be compatible with BDIM settings - Critical for performance optimization</p>"},{"location":"kernel-integrator-pragma-reference/#relationship","title":"RELATIONSHIP","text":"<p>Defines dimensional constraints between interfaces.</p> <p>Syntax: <pre><code>// @brainsmith RELATIONSHIP &lt;source&gt; &lt;target&gt; &lt;type&gt; [args...]\n</code></pre></p> <p>Types: - <code>EQUAL</code> - All dimensions must match - <code>DEPENDENT &lt;src_dim&gt; &lt;tgt_dim&gt; &lt;dep_type&gt; [scale]</code> - Dimension dependency   - <code>dep_type</code>: <code>copy</code>, <code>scaled</code>, <code>min</code> - <code>MULTIPLE &lt;src_dim&gt; &lt;tgt_dim&gt; [factor=N]</code> - Multiple relationship - <code>DIVISIBLE &lt;src_dim&gt; &lt;tgt_dim&gt;</code> - Divisibility constraint</p> <p>Examples: <pre><code>// @brainsmith RELATIONSHIP input output EQUAL\n// @brainsmith RELATIONSHIP input output DEPENDENT 0 0 copy\n// @brainsmith RELATIONSHIP input output DEPENDENT 1 1 scaled SCALE_FACTOR\n// @brainsmith RELATIONSHIP input output MULTIPLE 0 0 factor=4\n// @brainsmith RELATIONSHIP input output DIVISIBLE 1 1\n</code></pre></p>"},{"location":"kernel-integrator-pragma-reference/#datatype","title":"DATATYPE","text":"<p>Maps interface datatype properties to RTL parameters, enabling full QONNX datatype representation. </p> <p>Syntax: <pre><code>// @brainsmith DATATYPE &lt;interface&gt; &lt;property&gt; &lt;parameter_name&gt;\n</code></pre></p> <p>Arguments: - <code>interface</code> - Interface name - <code>property</code> - Datatype property to map (see below) - <code>parameter_name</code> - RTL parameter controlling this property</p> <p>Supported Properties: - <code>width</code> - Bit width of the datatype - <code>signed</code> - Whether the datatype is signed (0 or 1) - <code>format</code> - Format specifier for the datatype - <code>bias</code> - Bias value for the datatype - <code>fractional_width</code> - Number of fractional bits (fixed-point) - <code>exponent_width</code> - Exponent bit width (floating-point) - <code>mantissa_width</code> - Mantissa bit width (floating-point)</p> <p>Examples: <pre><code>// Basic width mapping\n// @brainsmith DATATYPE input width DATA_WIDTH\n// @brainsmith DATATYPE output width OUT_WIDTH\n\n// Signed/unsigned control\n// @brainsmith DATATYPE input signed INPUT_SIGNED\n// @brainsmith DATATYPE output signed OUTPUT_SIGNED\n\n// Fixed-point configuration\n// @brainsmith DATATYPE weights width WEIGHT_WIDTH\n// @brainsmith DATATYPE weights fractional_width WEIGHT_FRAC_BITS\n// @brainsmith DATATYPE weights signed WEIGHT_SIGNED\n\n// Floating-point configuration\n// @brainsmith DATATYPE input width FP_WIDTH\n// @brainsmith DATATYPE input exponent_width FP_EXP_WIDTH\n// @brainsmith DATATYPE input mantissa_width FP_MANT_WIDTH\n\n// Bias configuration\n// @brainsmith DATATYPE output bias OUTPUT_BIAS\n</code></pre></p>"},{"location":"kernel-integrator-pragma-reference/#datatype_constraint","title":"DATATYPE_CONSTRAINT","text":"<p>Defines allowed datatype ranges for interfaces.</p> <p>Syntax: <pre><code>// @brainsmith DATATYPE_CONSTRAINT &lt;interface&gt; &lt;base_type&gt; &lt;min_width&gt; &lt;max_width&gt;\n</code></pre></p> <p>Arguments: - <code>interface</code> - Interface name (e.g., \"input\", \"output\", \"weights\") - <code>base_type</code> - Base datatype or \"*\" for any type - <code>min_width</code> - Minimum bit width - <code>max_width</code> - Maximum bit width</p> <p>Examples: <pre><code>// @brainsmith DATATYPE_CONSTRAINT input * 1 32      // Any type, 1-32 bits\n// @brainsmith DATATYPE_CONSTRAINT output INT 8 8    // Integer, exactly 8 bits\n// @brainsmith DATATYPE_CONSTRAINT weights * 1 16    // Any type, 1-16 bits\n</code></pre></p> <p>Valid Base Types: - <code>*</code> - Any datatype - <code>INT</code> - Integer types (signed/unsigned) - <code>FLOAT</code> - Floating-point types - <code>FIXED</code> - Fixed-point types</p>"},{"location":"kernel-integrator-pragma-reference/#derived_parameter","title":"DERIVED_PARAMETER","text":"<p>Links module parameters to Python expressions.</p> <p>Syntax: <pre><code>// @brainsmith DERIVED_PARAMETER &lt;parameter_name&gt; = &lt;python_expression&gt;\n</code></pre></p> <p>Arguments: - <code>parameter_name</code> - RTL parameter name - <code>python_expression</code> - Valid Python expression</p> <p>Examples: <pre><code>// @brainsmith DERIVED_PARAMETER TOTAL_BITS = DATA_WIDTH * NUM_CHANNELS\n// @brainsmith DERIVED_PARAMETER ADDR_BITS = math.ceil(math.log2(DEPTH))\n// @brainsmith DERIVED_PARAMETER OUTPUT_SIZE = INPUT_SIZE // STRIDE + 1\n</code></pre></p> <p>Available in Expressions: - Other RTL parameters - Python math module functions - Basic arithmetic operators</p>"},{"location":"kernel-integrator-pragma-reference/#weight","title":"WEIGHT","text":"<p>Marks an interface as containing weight data.</p> <p>Syntax: <pre><code>// @brainsmith WEIGHT &lt;interface_name&gt;\n</code></pre></p> <p>Arguments: - <code>interface_name</code> - Name of the weight interface</p> <p>Example: <pre><code>// @brainsmith WEIGHT threshold\n// @brainsmith WEIGHT kernel_weights\n</code></pre></p> <p>Effects: - Interface is marked with <code>is_weight=True</code> in generated code - Affects dataflow graph construction - May change interface handling in FINN</p>"},{"location":"kernel-integrator-pragma-reference/#alias","title":"ALIAS","text":"<p>Exposes RTL parameters with different names in the HWCustomOp.</p> <p>Syntax: <pre><code>// @brainsmith ALIAS &lt;rtl_parameter&gt; &lt;exposed_name&gt;\n</code></pre></p> <p>Arguments: - <code>rtl_parameter</code> - Original parameter name in RTL - <code>exposed_name</code> - Name to expose in Python interface</p> <p>Examples: <pre><code>// @brainsmith ALIAS T_WIDTH threshold_width\n// @brainsmith ALIAS USE_DSP enable_dsp_mode\n// @brainsmith ALIAS FIFO_DEPTH input_buffer_depth\n</code></pre></p> <p>Use Cases: - Rename parameters for better Python API - Expose internal parameters - Create user-friendly names</p>"},{"location":"kernel-integrator-pragma-reference/#axilite_param","title":"AXILITE_PARAM","text":"<p>Links a parameter to control a specific property of an AXI-Lite interface.</p> <p>Syntax: <pre><code>// @brainsmith AXILITE_PARAM &lt;param_name&gt; &lt;interface_name&gt; &lt;property&gt;\n</code></pre></p> <p>Arguments: - <code>param_name</code> - Parameter to link (must exist in module parameters) - <code>interface_name</code> - Target AXI-Lite interface name - <code>property</code> - Interface property to control: <code>enable</code>, <code>data_width</code>, or <code>addr_width</code></p> <p>Example: <pre><code>// @brainsmith AXILITE_PARAM USE_AXILITE threshold enable\n</code></pre></p> <p>Effects: - Moves parameter from general parameters to interface-specific control - Parameter controls the specified interface property - For <code>enable</code> property: Controls whether the interface is instantiated - For <code>data_width</code>/<code>addr_width</code>: Sets the interface bus widths</p>"},{"location":"kernel-integrator-pragma-reference/#validation-tips","title":"Validation Tips","text":"<ul> <li>Use <code>--validate</code> flag to check pragmas without generation</li> <li>Enable <code>--verbose</code> for detailed parsing information</li> <li>Start with minimal pragmas and add incrementally</li> <li>Check generated metadata with <code>--info</code> flag</li> </ul>"},{"location":"kernel-integrator-user-guide/","title":"Kernel Integrator User Guide","text":""},{"location":"kernel-integrator-user-guide/#pre-release-note","title":"PRE-RELEASE NOTE","text":"<p>The Kernel Integrator is an experimental feature that offers signficiant automation potential and will work for most simple kernels, it has some rough edges and limitations for complex corner cases (particularly including AXI-Lite config signals).</p>"},{"location":"kernel-integrator-user-guide/#overview","title":"Overview","text":"<p>The Kernel Integrator is an automated tool that bridges the gap between SystemVerilog RTL hardware designs and the FINN compiler framework. It generates Python integration code that allows custom RTL kernels to be seamlessly used within neural network accelerator designs.</p>"},{"location":"kernel-integrator-user-guide/#what-it-does","title":"What It Does","text":"<p>The Kernel Integrator takes a SystemVerilog RTL file annotated with special pragmas and automatically generates:</p> <ol> <li>HWCustomOp Class - A FINN-compatible hardware operator that encapsulates your RTL kernel</li> <li>RTL Backend - Python code that handles the RTL implementation details</li> <li>Verilog Wrapper - SystemVerilog wrapper that adapts your kernel to FINN's interface requirements</li> <li>Python Package - Complete Python package structure with proper imports</li> </ol>"},{"location":"kernel-integrator-user-guide/#key-benefits","title":"Key Benefits","text":"<ul> <li>Automated Integration: No manual Python code writing required</li> <li>Protocol Validation: Automatic detection and validation of AXI-Stream and AXI-Lite interfaces</li> <li>Type Safety: Enforced datatype constraints between RTL and Python</li> <li>FINN Compatibility: Generated code follows FINN best practices</li> <li>Incremental Development: Validate RTL before generating code</li> </ul>"},{"location":"kernel-integrator-user-guide/#how-it-works","title":"How It Works","text":"<p>The Kernel Integrator follows a sophisticated pipeline:</p> <pre><code>RTL File + Pragmas \u2192 Parser \u2192 Metadata \u2192 Generator \u2192 Python/Verilog Files\n</code></pre>"},{"location":"kernel-integrator-user-guide/#1-input-processing","title":"1. Input Processing","text":"<p>The tool reads your SystemVerilog RTL file and extracts: - Module definitions, ports, and parameters - Special <code>@brainsmith</code> pragma annotations - Interface protocols (AXI-Stream, AXI-Lite)</p>"},{"location":"kernel-integrator-user-guide/#2-metadata-construction","title":"2. Metadata Construction","text":"<p>Parsed information is organized into a structured metadata model: - KernelMetadata: Top-level kernel information - InterfaceMetadata: Input/output interfaces with protocols - ParameterMetadata: RTL parameters and their relationships</p>"},{"location":"kernel-integrator-user-guide/#3-code-generation","title":"3. Code Generation","text":"<p>Templates transform metadata into production code: - Python classes that inherit from FINN base classes - Verilog wrappers that handle interface adaptation - Complete package structure with proper imports</p>"},{"location":"kernel-integrator-user-guide/#basic-usage","title":"Basic Usage","text":""},{"location":"kernel-integrator-user-guide/#command-line-interface","title":"Command Line Interface","text":"<pre><code># Generate all files in the same directory as RTL\npython -m brainsmith.tools.kernel_integrator design.sv\n\n# Generate in a specific output directory\npython -m brainsmith.tools.kernel_integrator design.sv -o output/\n\n# Validate RTL without generating files\npython -m brainsmith.tools.kernel_integrator design.sv --validate\n\n# Display parsed metadata\npython -m brainsmith.tools.kernel_integrator design.sv --info\n\n# Generate specific artifacts only\npython -m brainsmith.tools.kernel_integrator design.sv --artifacts autohwcustomop,wrapper\n</code></pre>"},{"location":"kernel-integrator-user-guide/#with-brainsmith-container","title":"With Brainsmith Container","text":"<pre><code># Using the smithy wrapper script\n./smithy kernel design.sv -o output/\n</code></pre>"},{"location":"kernel-integrator-user-guide/#pragma-system","title":"Pragma System","text":"<p>Pragmas are special comments that provide additional metadata to the Kernel Integrator. They must start with <code>@brainsmith</code> and appear as single-line comments in your RTL.</p> <p>See the Pragma Reference guide.</p>"},{"location":"kernel-integrator-user-guide/#rtl-requirements","title":"RTL Requirements","text":""},{"location":"kernel-integrator-user-guide/#module-structure","title":"Module Structure","text":"<p>Your RTL module should follow these conventions:</p> <ol> <li>Clear Port Definitions: Use standard SystemVerilog ANSI-style port declarations</li> <li>Parameter Declaration: Use <code>parameter</code> or <code>localparam</code> appropriately</li> <li>Protocol Compliance: Follow AXI-Stream or AXI-Lite protocols for interfaces</li> </ol>"},{"location":"kernel-integrator-user-guide/#axi-stream-interfaces","title":"AXI-Stream Interfaces","text":"<p>Input/output interfaces should follow AXI-Stream naming: <pre><code>// Input stream\ninput  [WIDTH-1:0] in_tdata,\ninput              in_tvalid,\noutput             in_tready,\n\n// Output stream  \noutput [WIDTH-1:0] out_tdata,\noutput             out_tvalid,\ninput              out_tready\n</code></pre></p>"},{"location":"kernel-integrator-user-guide/#axi-lite-interfaces","title":"AXI-Lite Interfaces","text":"<p>Configuration interfaces should follow AXI-Lite naming: <pre><code>// AXI-Lite slave interface\ninput  [ADDR_WIDTH-1:0] s_axi_awaddr,\ninput                   s_axi_awvalid,\noutput                  s_axi_awready,\n// ... (other AXI-Lite signals)\n</code></pre></p>"},{"location":"kernel-integrator-user-guide/#troubleshooting","title":"Troubleshooting","text":""},{"location":"kernel-integrator-user-guide/#debug-options","title":"Debug Options","text":"<pre><code># Enable verbose output\npython -m brainsmith.tools.kernel_integrator design.sv --verbose\n\n# Disable strict validation for experimentation\npython -m brainsmith.tools.kernel_integrator design.sv --no-strict\n\n# Check parsed metadata without generating files\npython -m brainsmith.tools.kernel_integrator design.sv --info\n</code></pre>"},{"location":"kernel-integrator-user-guide/#integration-with-finn","title":"Integration with FINN","text":"<p>The generated HWCustomOp can be used in FINN workflows:</p> <pre><code>from generated_module import MyKernelHWCustomOp\n\n# Use in ONNX graph construction\nnode = helper.make_node(\n    op_type=\"MyKernelHWCustomOp\",\n    inputs=[\"input_tensor\"],\n    outputs=[\"output_tensor\"],\n    domain=\"brainsmith.custom_ops\",\n    # Set attributes\n    DATA_WIDTH=8,\n    NUM_CHANNELS=64\n)\n</code></pre>"},{"location":"kernel-integrator-user-guide/#best-practices","title":"Best Practices","text":"<ol> <li>Start Simple: Begin with basic pragmas and add complexity incrementally</li> <li>Validate Early: Use <code>--validate</code> flag during development</li> <li>Use Meaningful Names: Clear interface and parameter names improve generated code</li> <li>Document Pragmas: Add comments explaining pragma choices</li> <li>Test Generated Code: Verify the generated HWCustomOp in your FINN workflow</li> </ol>"},{"location":"kernel-integrator-user-guide/#next-steps","title":"Next Steps","text":"<ul> <li>See the Pragma Reference for detailed pragma documentation</li> <li>Check the Quick Start Guide for a step-by-step tutorial</li> <li>Explore examples in <code>examples/kernel_integrator/</code> directory</li> </ul>"},{"location":"plugin_registry/","title":"Plugin Library Registry","text":""},{"location":"plugin_registry/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Key Architectural Concepts</li> <li>Plugin Types</li> <li>Using Plugins</li> <li>Development and Testing</li> <li>Advanced Usage</li> </ul>"},{"location":"plugin_registry/#key-architectural-concepts","title":"Key Architectural Concepts","text":"<ol> <li>Singleton Pattern: A single global registry instance ensures all code sees the same plugins</li> <li>Namespace Management: Framework prefixes (<code>finn:</code>, <code>qonnx:</code>) prevent name collisions between plugins from different sources</li> <li>Registration Order: Multiple registrations of the same name result in the last one overwriting previous ones</li> </ol>"},{"location":"plugin_registry/#plugin-types","title":"Plugin Types","text":""},{"location":"plugin_registry/#1-transforms","title":"1. Transforms","text":"<p>Purpose: Modify ONNX graphs for optimization, hardware mapping, or preprocessing</p> <p>Transforms modify ONNX graphs by pattern matching, node replacement, optimization, or cleanup. Each transform returns a tuple: (modified_model, boolean_indicating_changes). All transforms are subclasses of the QONNX Transformation pass.</p> <p>Interface: <pre><code>from qonnx.transformation.base import Transformation\nfrom brainsmith.core.plugins import transform\n\n@transform(\n    name=\"MyTransform\",   # Defaults to class name if not specified\n    stage=\"topology_opt\",\n    description=\"What this transform does\",\n    author=\"Your Name\"\n)\nclass MyTransform(Transformation):\n    def apply(self, model):\n        # Modify the model\n        graph_modified = False\n        # ... transformation logic ...\n        return (model, graph_modified)\n</code></pre></p> <p>Example: <code>brainsmith/transforms/kernel_opt/set_pumped_compute.py:16</code> <pre><code>@transform(\n    name=\"SetPumpedCompute\",\n    stage=\"kernel_opt\",\n    description=\"Set pumped compute attribute for MVAUs and DynMatMuls\"\n)\nclass SetPumpedCompute(Transformation):\n    def apply(self, model):\n        for node in model.graph.node:\n            if node.op_type == \"MVAU_rtl\":\n                inst = registry.getCustomOp(node)\n                inst.set_nodeattr(\"pumpedCompute\", 1)\n        return (model, False)\n</code></pre></p>"},{"location":"plugin_registry/#2-build-steps","title":"2. Build Steps","text":"<p>Purpose: Define reusable sequences of operations in the compilation flow</p> <p>Steps coordinate sequences of operations in the compilation pipeline. Unlike transforms which modify graphs, steps orchestrate the overall flow and manage shared state through the context dictionary.</p> <p>Interface: <pre><code>from brainsmith.core.plugins import step\n\n@step(\n    name=\"my_step\",  # Required as keyword argument\n    category=\"optimization\",\n    dependencies=[\"previous_step\"],  # Optional\n    description=\"What this step does\"\n)\ndef my_step(blueprint, context):  # Note: signature is (blueprint, context)\n    # Apply transforms\n    from brainsmith.core.plugins import get_transform\n\n    transform = get_transform(\"SomeTransform\")\n    model = context.get(\"model\")\n    model, _ = transform().apply(model)\n    context[\"model\"] = model\n\n    # Steps don't return values, they modify context\n</code></pre></p> <p>Example: <code>brainsmith/steps/core_steps.py:10</code> <pre><code>@step(\n    name=\"qonnx_to_finn\",\n    category=\"cleanup\",\n    description=\"Convert from QONNX to FINN opset\"\n)\ndef qonnx_to_finn_step(blueprint, context):\n    model = context[\"model\"]\n    # Apply multiple transforms\n    model = apply_cleanup_transforms(model)\n    context[\"model\"] = model\n</code></pre></p>"},{"location":"plugin_registry/#3-kernels","title":"3. Kernels","text":"<p>Purpose: Define custom hardware operators with specific attributes and behavior</p> <p>Kernels implement neural network operations in hardware. They define the hardware interface, parameters, and simulation behavior.</p> <p>Interface: <pre><code>from finn.custom_op.fpgadataflow.hwcustomop import HWCustomOp\nfrom brainsmith.core.plugins import kernel\n\n@kernel(\n    name=\"MyKernel\",  # Required as keyword argument\n    description=\"Hardware implementation of operation\",\n    author=\"Your Name\"\n)\nclass MyKernel(HWCustomOp):\n    def get_nodeattr_types(self):\n        return {\n            # Format: (type, required, default)\n            # Types: \"i\"=int, \"s\"=string, \"f\"=float\n            \"NumChannels\": (\"i\", True, \"\"),  \n            \"SIMD\": (\"i\", True, 1),\n        }\n\n    def execute_node(self, context, graph):\n        # Simulation logic\n        pass\n</code></pre></p> <p>Note: Functional kernels also require: - An inference transform to convert ONNX ops to this kernel - Backend(s) to generate synthesizable code</p> <p>Example: <code>brainsmith/kernels/layernorm/layernorm.py:10</code> <pre><code>@kernel(\n    name=\"LayerNorm\",\n    description=\"Hardware implementation of LayerNorm\"\n)\nclass LayerNorm(HWCustomOp):\n    # Implements layer normalization in hardware\n</code></pre></p>"},{"location":"plugin_registry/#4-backends","title":"4. Backends","text":"<p>Purpose: Generate synthesizable code (C++ for HLS, Verilog for RTL) from kernel specifications</p> <p>Backends generate synthesizable code (HLS C++ or RTL Verilog) from kernel specifications. Different backends can optimize for different targets: low latency, high throughput, or minimal resource usage.</p> <p>Requirements:  - Naming convention: <code>{KernelName}_{language}</code> (e.g., <code>LayerNorm_hls</code>) - Multiple inheritance: from both kernel class and backend base class (HLSBackend or RTLBackend) - First registered backend becomes the default</p> <p>Additional Requirements for Functional Backends: - Associated RTL or HLS source implementation files - Wrapper templates for integration with the generated code - Proper file structure following FINN conventions</p> <p>Interface: <pre><code>from finn.custom_op.fpgadataflow.hlsbackend import HLSBackend\nfrom brainsmith.core.plugins import backend\n\n@backend(\n    name=\"MyKernel_hls\",  # Convention: KernelName_language\n    kernel=\"MyKernel\",    # Which kernel this generates code for\n    language=\"hls\",       # \"hls\" or \"rtl\"\n    description=\"HLS backend for MyKernel\"\n)\nclass MyKernel_hls(MyKernel, HLSBackend):  # Multiple inheritance\n    def global_includes(self):\n        return ['#include \"ap_fixed.h\"']\n\n    def defines(self, var):\n        return [f\"#define NUM_CHANNELS {self.get_nodeattr('NumChannels')}\"]\n\n    def docompute(self):\n        return \"\"\"\n        for (int i = 0; i &lt; NUM_CHANNELS; i++) {\n            output[i] = input[i] * scale[i] + bias[i];\n        }\n        \"\"\"\n</code></pre></p>"},{"location":"plugin_registry/#using-plugins","title":"Using Plugins","text":""},{"location":"plugin_registry/#getting-plugins","title":"Getting Plugins","text":"<p>Retrieve plugins by name, with automatic namespace resolution for unique names.</p> <pre><code>from brainsmith.core.plugins import (\n    get_registry,  # Direct registry access\n    get_transform, get_kernel, get_backend, get_step,\n    list_transforms, list_kernels, list_backends, list_steps,\n    has_transform, has_kernel  # Check existence\n)\n\n# Get a specific plugin\ntransform = get_transform(\"ExpandNorms\")\nkernel = get_kernel(\"LayerNorm\")\nbackend = get_backend(\"LayerNorm_hls\")\nstep = get_step(\"qonnx_to_finn\")\n\n# Check if plugin exists (won't raise exception)\nif has_transform(\"MyTransform\"):\n    transform = get_transform(\"MyTransform\")\n\n# List all plugins of a type\nall_transforms = list_transforms()  # Returns list of names\nall_kernels = list_kernels()\n</code></pre>"},{"location":"plugin_registry/#error-handling","title":"Error Handling","text":"<p>Plugin retrieval raises <code>KeyError</code> with helpful messages when plugins aren't found:</p> <pre><code>try:\n    transform = get_transform(\"NonExistent\")\nexcept KeyError as e:\n    print(e)\n    # KeyError: \"Plugin transform:NonExistent not found. Available (162): \n    # ['qonnx:BatchNormToAffine', 'finn:Streamline', ...]\"\n\n# Use has_* functions to check without exceptions:\nif has_transform(\"MyTransform\"):\n    transform = get_transform(\"MyTransform\")\n</code></pre>"},{"location":"plugin_registry/#namespace-resolution","title":"Namespace Resolution","text":"<p>The system automatically tries common framework prefixes when resolving names. For plugins from external frameworks like FINN or QONNX, you can use either the full namespace name (e.g., \"finn:ConvertBipolarMatMulToXnorPopcount\") or just the simple name if it's unique.</p> <pre><code># These are equivalent if \"Streamline\" is unique:\ntransform1 = get_transform(\"finn:Streamline\")\ntransform2 = get_transform(\"Streamline\")\n\n# For ambiguous names, use explicit namespace:\ntransform = get_transform(\"myframework:CommonName\")\n</code></pre>"},{"location":"plugin_registry/#finding-plugins-by-metadata","title":"Finding Plugins by Metadata","text":"<p>Query plugins by their metadata attributes:</p> <pre><code>from brainsmith.core.plugins import (\n    get_transforms_by_metadata,\n    get_backends_by_metadata\n)\n\n# Find all transforms for a specific stage\ntopology_transforms = get_transforms_by_metadata(stage=\"topology_opt\")\n\n# Find kernel inference transforms\ninference_transforms = get_transforms_by_metadata(kernel_inference=True)\n\n# Find backends by language\nhls_backends = get_backends_by_metadata(language=\"hls\")\n\n# Direct registry access for complex queries\nregistry = get_registry()\ncustom_transforms = registry.find(\"transform\", author=\"MyTeam\", version=\"2.0\")\n</code></pre>"},{"location":"plugin_registry/#framework-plugins","title":"Framework Plugins","text":"<p>Brainsmith automatically integrates plugins from FINN and QONNX frameworks on first access:</p> <ul> <li>FINN: ~98 transforms, ~40 kernels/backends</li> <li>QONNX: ~60 transforms</li> <li>Total: 200+ pre-registered components</li> </ul> <pre><code># List plugins from specific framework\nregistry = get_registry()\nfinn_transforms = registry.find(\"transform\", framework=\"finn\")\nqonnx_transforms = registry.find(\"transform\", framework=\"qonnx\")\n\n# Get framework-specific kernel backends\nfrom brainsmith.core.plugins.registry import list_backends_by_kernel\nmvau_backends = list_backends_by_kernel(\"MVAU\")  # Returns ['MVAU_hls', 'MVAU_rtl']\n</code></pre>"},{"location":"plugin_registry/#development-and-testing","title":"Development and Testing","text":""},{"location":"plugin_registry/#understanding-plugin-registration","title":"Understanding Plugin Registration","text":"<p>Plugins are registered as a side effect of module imports through decorators:</p> <pre><code># This happens automatically when the module is imported:\n@transform(name=\"AutoRegistered\")\nclass AutoRegistered(Transformation):\n    pass\n\n# The decorator is equivalent to:\n# registry.register(\"transform\", \"AutoRegistered\", AutoRegistered)\n</code></pre>"},{"location":"plugin_registry/#testing-with-plugins","title":"Testing with Plugins","text":"<p>Key testing considerations:</p> <ol> <li>Registration is Permanent: Plugins remain registered for the entire Python session</li> <li>Import Side Effects: Decorators execute when modules are imported</li> <li>The reset() Limitation: <code>registry.reset()</code> clears ALL plugins. Test plugins cannot be reloaded because decorators already executed during import</li> </ol> <pre><code># DON'T DO THIS in tests:\nregistry.reset()  # Clears all plugins\n# Test plugins won't come back even with re-import!\n\n# DO THIS instead:\n# 1. Import test plugins early in your test session\n# 2. Use direct registration for test-specific plugins\nregistry.register(\"transform\", \"test_only\", TestTransform)\n</code></pre>"},{"location":"plugin_registry/#debugging-plugin-issues","title":"Debugging Plugin Issues","text":"<pre><code># Check what's registered\nregistry = get_registry()\nprint(f\"Total transforms: {len(registry._plugins['transform'])}\")\n\n# See all registered names\nall_transforms = list_transforms()\nprint(f\"Transform names: {all_transforms}\")\n\n# Check if lazy loading has occurred\nif hasattr(registry, '_discovered'):\n    print(\"Framework plugins have been loaded\")\n\n# Force lazy loading\nregistry._load_plugins()\n</code></pre>"},{"location":"plugin_registry/#advanced-usage","title":"Advanced Usage","text":""},{"location":"plugin_registry/#direct-registry-access","title":"Direct Registry Access","text":"<p>For testing or debugging, access the registry directly:</p> <pre><code>from brainsmith.core.plugins import get_registry\n\nregistry = get_registry()\n\n# Direct registration (testing only)\nregistry.register(\"transform\", \"test_transform\", MyTestTransform, \n                 framework=\"test\", author=\"tester\")\n\n# Inspect registered plugins\ntransforms = registry._plugins[\"transform\"]  # Dict[str, Tuple[Type, Dict]]\n\n# Check if plugins are loaded\nif hasattr(registry, '_discovered'):\n    print(\"Plugins have been loaded\")\n\n# Force plugin loading\nregistry._load_plugins()\n</code></pre>"},{"location":"plugin_registry/#kernel-inference-transforms","title":"Kernel Inference Transforms","text":"<p>Kernel inference transforms are a special category of transform that bridge standard ONNX operations and custom hardware kernels. They analyze the graph to find patterns that can be implemented using specific kernels, then replace those patterns with kernel instances. These transforms typically reside within their kernel's directory rather than the general transforms folder.</p> <pre><code>from brainsmith.core.plugins import kernel_inference\n\n@kernel_inference(\n    kernel=\"MyKernel\",\n    description=\"Infer MyKernel from ONNX patterns\"\n)\nclass InferMyKernel(Transformation):\n    def apply(self, model):\n        # Pattern matching and conversion logic\n        graph_modified = False\n        # Find patterns and replace with kernel\n        return (model, graph_modified)\n</code></pre> <p>Note: The <code>kernel_inference</code> decorator is an alias for the <code>transform</code> decorator that automatically tags the transform with kernel metadata for discovery.</p> <p>For Plugin Registry implementation details, see <code>brainsmith/core/plugins/registry.py</code>. For examples, browse the <code>brainsmith/transforms/</code>, <code>brainsmith/kernels/</code>, and <code>brainsmith/steps/</code> directories.</p>"},{"location":"api-reference/core/","title":"Core API Reference","text":""},{"location":"api-reference/core/#brainsmith.core","title":"<code>core</code>","text":"<p>Brainsmith Core - DSE Architecture</p> <p>This package implements the DSE architecture for FPGA accelerator design.</p>"},{"location":"api-reference/core/#brainsmith.core.DSESegment","title":"<code>DSESegment(transforms: List[Dict[str, Any]], branch_choice: Optional[str] = None, parent: Optional[DSESegment] = None, children: Dict[str, DSESegment] = dict(), status: str = 'pending', output_dir: Optional[Path] = None, error: Optional[str] = None, execution_time: Optional[float] = None, artifacts: ArtifactState = ArtifactState(), finn_config: Dict[str, Any] = dict())</code>  <code>dataclass</code>","text":"<p>A segment in the design space exploration tree.</p> <p>Each segment is executed as a single FINN build, containing all transforms from the last branch point (or root) to the next branch point (or leaf).</p>"},{"location":"api-reference/core/#brainsmith.core.DSESegment.is_branch_point","title":"<code>is_branch_point: bool</code>  <code>property</code>","text":"<p>Check if this segment branches.</p>"},{"location":"api-reference/core/#brainsmith.core.DSESegment.is_leaf","title":"<code>is_leaf: bool</code>  <code>property</code>","text":"<p>Check if this is a complete path endpoint.</p>"},{"location":"api-reference/core/#brainsmith.core.DSESegment.segment_id","title":"<code>segment_id: str</code>  <code>property</code>","text":"<p>Deterministic ID from content.</p>"},{"location":"api-reference/core/#brainsmith.core.DSESegment.add_child","title":"<code>add_child(branch_id: str, transforms: List[Dict[str, Any]]) -&gt; DSESegment</code>","text":"<p>Create a child segment for a branch.</p>"},{"location":"api-reference/core/#brainsmith.core.DSESegment.count_descendants","title":"<code>count_descendants() -&gt; int</code>","text":"<p>Count total number of descendant nodes.</p>"},{"location":"api-reference/core/#brainsmith.core.DSESegment.get_all_transforms","title":"<code>get_all_transforms() -&gt; List[Dict[str, Any]]</code>","text":"<p>Get all transforms from root to end of this segment.</p>"},{"location":"api-reference/core/#brainsmith.core.DSESegment.get_cache_key","title":"<code>get_cache_key() -&gt; str</code>","text":"<p>Simple, deterministic cache key.</p>"},{"location":"api-reference/core/#brainsmith.core.DSESegment.get_path","title":"<code>get_path() -&gt; List[DSESegment]</code>","text":"<p>Get all segments from root to here.</p>"},{"location":"api-reference/core/#brainsmith.core.DSETree","title":"<code>DSETree(root: DSESegment)</code>","text":"<p>Design space exploration tree structure and operations.</p>"},{"location":"api-reference/core/#brainsmith.core.DSETree.count_leaves","title":"<code>count_leaves() -&gt; int</code>","text":"<p>Count leaf nodes in tree.</p>"},{"location":"api-reference/core/#brainsmith.core.DSETree.count_nodes","title":"<code>count_nodes() -&gt; int</code>","text":"<p>Count all nodes in tree.</p>"},{"location":"api-reference/core/#brainsmith.core.DSETree.get_all_segments","title":"<code>get_all_segments() -&gt; List[DSESegment]</code>","text":"<p>Get all segments in the tree.</p>"},{"location":"api-reference/core/#brainsmith.core.DSETree.get_execution_order","title":"<code>get_execution_order() -&gt; List[DSESegment]</code>","text":"<p>Get breadth-first execution order for the tree.</p> <p>This ensures parent nodes are executed before children, enabling proper result sharing.</p> <p>Returns:</p> Type Description <code>List[DSESegment]</code> <p>List of nodes in execution order</p>"},{"location":"api-reference/core/#brainsmith.core.DSETree.get_leaf_segments","title":"<code>get_leaf_segments() -&gt; List[DSESegment]</code>","text":"<p>Get all complete exploration paths (leaf segments).</p>"},{"location":"api-reference/core/#brainsmith.core.DSETree.get_statistics","title":"<code>get_statistics() -&gt; Dict[str, Any]</code>","text":"<p>Get statistics about the DSE tree.</p>"},{"location":"api-reference/core/#brainsmith.core.DSETree.print_tree","title":"<code>print_tree() -&gt; None</code>","text":"<p>Pretty print the DSE tree.</p>"},{"location":"api-reference/core/#brainsmith.core.DSETreeBuilder","title":"<code>DSETreeBuilder</code>","text":"<p>Builds DSE trees from design spaces.</p>"},{"location":"api-reference/core/#brainsmith.core.DSETreeBuilder.build_tree","title":"<code>build_tree(space: DesignSpace, forge_config: ForgeConfig) -&gt; DSETree</code>","text":"<p>Build DSE tree with unified branching.</p> <p>Steps can now be direct strings or lists for variations.</p> <p>Parameters:</p> Name Type Description Default <code>space</code> <code>DesignSpace</code> <p>DesignSpace containing steps and configuration</p> required <code>forge_config</code> <code>ForgeConfig</code> <p>ForgeConfig with FINN parameters</p> required <p>Returns:</p> Type Description <code>DSETree</code> <p>DSETree containing the built tree</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If tree exceeds max_combinations</p>"},{"location":"api-reference/core/#brainsmith.core.DesignSpace","title":"<code>DesignSpace(model_path: str, steps: List[Union[str, List[Optional[str]]]], kernel_backends: List[Tuple[str, List[Type]]], max_combinations: int = 100000)</code>  <code>dataclass</code>","text":"<p>Design space with resolved plugin objects.</p> <p>This is a clean intermediate representation between blueprint YAML and the execution tree. All plugin names have been resolved to actual classes from the registry.</p>"},{"location":"api-reference/core/#brainsmith.core.DesignSpace.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate steps after initialization.</p>"},{"location":"api-reference/core/#brainsmith.core.DesignSpace.__str__","title":"<code>__str__() -&gt; str</code>","text":"<p>Human-readable representation.</p>"},{"location":"api-reference/core/#brainsmith.core.DesignSpace.get_kernel_summary","title":"<code>get_kernel_summary() -&gt; str</code>","text":"<p>Get human-readable summary of kernels and backends.</p>"},{"location":"api-reference/core/#brainsmith.core.DesignSpace.validate_size","title":"<code>validate_size() -&gt; None</code>","text":"<p>Validate that design space doesn't exceed max combinations.</p>"},{"location":"api-reference/core/#brainsmith.core.DesignSpace.validate_steps","title":"<code>validate_steps() -&gt; None</code>","text":"<p>Validate that all referenced steps exist in the registry.</p>"},{"location":"api-reference/core/#brainsmith.core.ForgeConfig","title":"<code>ForgeConfig(clock_ns: float, output: Literal['estimates', 'rtl', 'bitfile'] = 'estimates', board: Optional[str] = None, verify: bool = False, verify_data: Optional[Path] = None, parallel_builds: int = 4, debug: bool = False, save_intermediate_models: bool = False, start_step: Optional[str] = None, stop_step: Optional[str] = None, finn_overrides: Dict[str, Any] = dict())</code>  <code>dataclass</code>","text":"<p>Configuration that actually works.</p>"},{"location":"api-reference/core/#brainsmith.core.SegmentRunner","title":"<code>SegmentRunner(finn_runner: FINNRunner, base_config: Dict[str, Any], kernel_selections: list = None)</code>","text":"<p>Runs DSE segments using FINN.</p> <p>Handles both tree traversal and individual segment execution using FINNRunner for all FINN interactions.</p> <p>Initialize runner.</p> <p>Parameters:</p> Name Type Description Default <code>finn_runner</code> <code>FINNRunner</code> <p>Runner for FINN-specific operations</p> required <code>base_config</code> <code>Dict[str, Any]</code> <p>FINN configuration from blueprint</p> required <code>kernel_selections</code> <code>list</code> <p>Optional list of (kernel, backend) tuples</p> <code>None</code>"},{"location":"api-reference/core/#brainsmith.core.SegmentRunner.run_segment","title":"<code>run_segment(segment: DSESegment, input_model: Path, base_output_dir: Path) -&gt; SegmentResult</code>","text":"<p>Run a single DSE segment.</p> <p>Parameters:</p> Name Type Description Default <code>segment</code> <code>DSESegment</code> <p>Segment to execute</p> required <code>input_model</code> <code>Path</code> <p>Input ONNX model path</p> required <code>base_output_dir</code> <code>Path</code> <p>Base output directory</p> required <p>Returns:</p> Type Description <code>SegmentResult</code> <p>SegmentResult with execution details</p>"},{"location":"api-reference/core/#brainsmith.core.SegmentRunner.run_tree","title":"<code>run_tree(tree: DSETree, initial_model: Path, output_dir: Path) -&gt; TreeExecutionResult</code>","text":"<p>Run all segments in the DSE tree.</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>DSETree</code> <p>DSE tree to execute</p> required <code>initial_model</code> <code>Path</code> <p>Path to initial ONNX model</p> required <code>output_dir</code> <code>Path</code> <p>Base output directory</p> required <p>Returns:</p> Type Description <code>TreeExecutionResult</code> <p>TreeExecutionResult with all segment results</p>"},{"location":"api-reference/core/#brainsmith.core.explore_design_space","title":"<code>explore_design_space(model_path: str, blueprint_path: str, output_dir: str = None, start_step_override: str = None, stop_step_override: str = None)</code>","text":"<p>Explore the design space for an FPGA accelerator.</p> <p>Transforms a neural network model into an FPGA accelerator through blueprint-driven design space exploration and synthesis.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to ONNX model file</p> required <code>blueprint_path</code> <code>str</code> <p>Path to Blueprint YAML file</p> required <code>output_dir</code> <code>str</code> <p>Output directory (defaults to $BSMITH_BUILD_DIR/forge_YYYYMMDD_HHMMSS)</p> <code>None</code> <code>start_step_override</code> <code>str</code> <p>Override blueprint start_step (CLI takes precedence)</p> <code>None</code> <code>stop_step_override</code> <code>str</code> <p>Override blueprint stop_step (CLI takes precedence)</p> <code>None</code> <p>Returns:</p> Type Description <p>TreeExecutionResult containing build artifacts and statistics</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If model or blueprint file doesn't exist</p> <code>ValueError</code> <p>If blueprint is invalid or tree exceeds size limits</p> <code>RuntimeError</code> <p>If no successful builds were produced</p>"},{"location":"api-reference/core/#brainsmith.core.parse_blueprint","title":"<code>parse_blueprint(blueprint_path: str, model_path: str) -&gt; Tuple[DesignSpace, ForgeConfig]</code>","text":"<p>Parse blueprint YAML and return DesignSpace and ForgeConfig.</p> <p>Inheritance is resolved bottom-up: 1. Start from the root parent (no extends) 2. Fully resolve its steps (including operations) 3. Pass resolved steps to child for its operations 4. Repeat until we reach the target blueprint</p>"},{"location":"api-reference/core/#main-api","title":"Main API","text":""},{"location":"api-reference/core/#brainsmith.core.explore_design_space","title":"<code>explore_design_space(model_path: str, blueprint_path: str, output_dir: str = None, start_step_override: str = None, stop_step_override: str = None)</code>","text":"<p>Explore the design space for an FPGA accelerator.</p> <p>Transforms a neural network model into an FPGA accelerator through blueprint-driven design space exploration and synthesis.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to ONNX model file</p> required <code>blueprint_path</code> <code>str</code> <p>Path to Blueprint YAML file</p> required <code>output_dir</code> <code>str</code> <p>Output directory (defaults to $BSMITH_BUILD_DIR/forge_YYYYMMDD_HHMMSS)</p> <code>None</code> <code>start_step_override</code> <code>str</code> <p>Override blueprint start_step (CLI takes precedence)</p> <code>None</code> <code>stop_step_override</code> <code>str</code> <p>Override blueprint stop_step (CLI takes precedence)</p> <code>None</code> <p>Returns:</p> Type Description <p>TreeExecutionResult containing build artifacts and statistics</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If model or blueprint file doesn't exist</p> <code>ValueError</code> <p>If blueprint is invalid or tree exceeds size limits</p> <code>RuntimeError</code> <p>If no successful builds were produced</p> Source code in <code>brainsmith/core/dse_api.py</code> <pre><code>def explore_design_space(\n    model_path: str,\n    blueprint_path: str,\n    output_dir: str = None,\n    start_step_override: str = None,\n    stop_step_override: str = None\n):\n    \"\"\"\n    Explore the design space for an FPGA accelerator.\n\n    Transforms a neural network model into an FPGA accelerator through\n    blueprint-driven design space exploration and synthesis.\n\n    Args:\n        model_path: Path to ONNX model file\n        blueprint_path: Path to Blueprint YAML file\n        output_dir: Output directory (defaults to $BSMITH_BUILD_DIR/forge_YYYYMMDD_HHMMSS)\n        start_step_override: Override blueprint start_step (CLI takes precedence)\n        stop_step_override: Override blueprint stop_step (CLI takes precedence)\n\n    Returns:\n        TreeExecutionResult containing build artifacts and statistics\n\n    Raises:\n        FileNotFoundError: If model or blueprint file doesn't exist\n        ValueError: If blueprint is invalid or tree exceeds size limits\n        RuntimeError: If no successful builds were produced\n    \"\"\"\n    # Verify files exist\n    if not Path(model_path).exists():\n        raise FileNotFoundError(f\"Model file not found: {model_path}\")\n    if not Path(blueprint_path).exists():\n        raise FileNotFoundError(f\"Blueprint file not found: {blueprint_path}\")\n\n    # Determine output directory\n    if output_dir is None:\n        build_dir = get_build_dir()\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        output_dir = str(build_dir / f\"dse_{timestamp}\")\n\n    output_path = Path(output_dir)\n    output_path.mkdir(parents=True, exist_ok=True)\n\n    logger.info(f\"Exploring design space for FPGA accelerator:\")\n    logger.info(f\"  Model: {model_path}\")\n    logger.info(f\"  Blueprint: {blueprint_path}\")\n    logger.info(f\"  Output: {output_dir}\")\n\n    # Parse blueprint\n    design_space, forge_config = parse_blueprint(blueprint_path, str(Path(model_path).absolute()))\n\n    # Apply CLI overrides (CLI &gt; blueprint)\n    start_step = start_step_override or forge_config.start_step\n    stop_step = stop_step_override or forge_config.stop_step\n\n    # Slice steps if specified\n    if start_step or stop_step:\n        logger.info(f\"Applying step range: start={start_step or 'beginning'}, stop={stop_step or 'end'}\")\n        design_space.steps = slice_steps(design_space.steps, start_step, stop_step)\n\n    # Build DSE tree\n    tree_builder = DSETreeBuilder()\n    tree = tree_builder.build_tree(design_space, forge_config)\n\n    logger.info(f\"Design space: {len(design_space.steps)} steps, \"\n                f\"{len(design_space.kernel_backends)} kernels\")\n\n    # Log tree statistics\n    stats = tree.get_statistics()\n    logger.info(f\"DSE tree:\")\n    logger.info(f\"  - Total paths: {stats['total_paths']:,}\")\n    logger.info(f\"  - Total segments: {stats['total_segments']:,}\")\n    logger.info(f\"  - Segment efficiency: {stats['segment_efficiency']}%\")\n\n    # Explore the DSE tree\n    logger.info(\"Starting design space exploration...\")\n\n    # Create runner and execute\n    finn_runner = FINNRunner()\n    runner = SegmentRunner(finn_runner, tree.root.finn_config)\n    results = runner.run_tree(\n        tree=tree,\n        initial_model=Path(model_path),\n        output_dir=Path(output_dir)\n    )\n\n    # Check results\n    result_stats = results.stats\n\n    # Consider both successful and cached builds as valid outcomes\n    valid_builds = result_stats['successful'] + result_stats['cached']\n\n    if valid_builds == 0:\n        raise RuntimeError(f\"DSE failed: No successful builds \"\n                         f\"({result_stats['failed']} failed, {result_stats['skipped']} skipped)\")\n\n    # Warn if only cached results were used\n    if result_stats['successful'] == 0 and result_stats['cached'] &gt; 0:\n        logger.warning(f\"\u26a0\ufe0f  All builds used cached results ({result_stats['cached']} cached). \"\n                      f\"No new builds were executed.\")\n\n    logger.info(f\"\u2705 Design space exploration completed successfully!\")\n    logger.info(f\"   Successful builds: {result_stats['successful']}/{result_stats['total']}\")\n    logger.info(f\"   Total time: {results.total_time:.2f}s\")\n    logger.info(f\"   Output directory: {output_dir}\")\n\n    # Attach design space and tree to results for inspection\n    results.design_space = design_space\n    results.dse_tree = tree\n\n    return results\n</code></pre>"},{"location":"api-reference/core/#dse-components","title":"DSE Components","text":""},{"location":"api-reference/core/#brainsmith.core.DSESegment","title":"<code>DSESegment(transforms: List[Dict[str, Any]], branch_choice: Optional[str] = None, parent: Optional[DSESegment] = None, children: Dict[str, DSESegment] = dict(), status: str = 'pending', output_dir: Optional[Path] = None, error: Optional[str] = None, execution_time: Optional[float] = None, artifacts: ArtifactState = ArtifactState(), finn_config: Dict[str, Any] = dict())</code>  <code>dataclass</code>","text":"<p>A segment in the design space exploration tree.</p> <p>Each segment is executed as a single FINN build, containing all transforms from the last branch point (or root) to the next branch point (or leaf).</p>"},{"location":"api-reference/core/#brainsmith.core.DSESegment.is_branch_point","title":"<code>is_branch_point: bool</code>  <code>property</code>","text":"<p>Check if this segment branches.</p>"},{"location":"api-reference/core/#brainsmith.core.DSESegment.is_leaf","title":"<code>is_leaf: bool</code>  <code>property</code>","text":"<p>Check if this is a complete path endpoint.</p>"},{"location":"api-reference/core/#brainsmith.core.DSESegment.segment_id","title":"<code>segment_id: str</code>  <code>property</code>","text":"<p>Deterministic ID from content.</p>"},{"location":"api-reference/core/#brainsmith.core.DSESegment.add_child","title":"<code>add_child(branch_id: str, transforms: List[Dict[str, Any]]) -&gt; DSESegment</code>","text":"<p>Create a child segment for a branch.</p> Source code in <code>brainsmith/core/dse/segment.py</code> <pre><code>def add_child(self, branch_id: str, transforms: List[Dict[str, Any]]) -&gt; 'DSESegment':\n    \"\"\"Create a child segment for a branch.\"\"\"\n    child = DSESegment(\n        transforms=transforms,\n        branch_choice=branch_id,\n        parent=self,\n        finn_config=self.finn_config.copy()\n    )\n    self.children[branch_id] = child\n    return child\n</code></pre>"},{"location":"api-reference/core/#brainsmith.core.DSESegment.count_descendants","title":"<code>count_descendants() -&gt; int</code>","text":"<p>Count total number of descendant nodes.</p> Source code in <code>brainsmith/core/dse/segment.py</code> <pre><code>def count_descendants(self) -&gt; int:\n    \"\"\"Count total number of descendant nodes.\"\"\"\n    count = len(self.children)\n    for child in self.children.values():\n        count += child.count_descendants()\n    return count\n</code></pre>"},{"location":"api-reference/core/#brainsmith.core.DSESegment.get_all_transforms","title":"<code>get_all_transforms() -&gt; List[Dict[str, Any]]</code>","text":"<p>Get all transforms from root to end of this segment.</p> Source code in <code>brainsmith/core/dse/segment.py</code> <pre><code>def get_all_transforms(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"Get all transforms from root to end of this segment.\"\"\"\n    transforms = []\n    for segment in self.get_path():\n        transforms.extend(segment.transforms)\n    return transforms\n</code></pre>"},{"location":"api-reference/core/#brainsmith.core.DSESegment.get_cache_key","title":"<code>get_cache_key() -&gt; str</code>","text":"<p>Simple, deterministic cache key.</p> Source code in <code>brainsmith/core/dse/segment.py</code> <pre><code>def get_cache_key(self) -&gt; str:\n    \"\"\"Simple, deterministic cache key.\"\"\"\n    return self.segment_id\n</code></pre>"},{"location":"api-reference/core/#brainsmith.core.DSESegment.get_path","title":"<code>get_path() -&gt; List[DSESegment]</code>","text":"<p>Get all segments from root to here.</p> Source code in <code>brainsmith/core/dse/segment.py</code> <pre><code>def get_path(self) -&gt; List['DSESegment']:\n    \"\"\"Get all segments from root to here.\"\"\"\n    path = []\n    node = self\n    while node:\n        path.append(node)\n        node = node.parent\n    path.reverse()\n    return path\n</code></pre>"},{"location":"api-reference/core/#brainsmith.core.DSETree","title":"<code>DSETree(root: DSESegment)</code>","text":"<p>Design space exploration tree structure and operations.</p> Source code in <code>brainsmith/core/dse/tree.py</code> <pre><code>def __init__(self, root: DSESegment):\n    self.root = root\n</code></pre>"},{"location":"api-reference/core/#brainsmith.core.DSETree.count_leaves","title":"<code>count_leaves() -&gt; int</code>","text":"<p>Count leaf nodes in tree.</p> Source code in <code>brainsmith/core/dse/tree.py</code> <pre><code>def count_leaves(self) -&gt; int:\n    \"\"\"Count leaf nodes in tree.\"\"\"\n    return self._count_leaves(self.root)\n</code></pre>"},{"location":"api-reference/core/#brainsmith.core.DSETree.count_nodes","title":"<code>count_nodes() -&gt; int</code>","text":"<p>Count all nodes in tree.</p> Source code in <code>brainsmith/core/dse/tree.py</code> <pre><code>def count_nodes(self) -&gt; int:\n    \"\"\"Count all nodes in tree.\"\"\"\n    return self._count_nodes(self.root)\n</code></pre>"},{"location":"api-reference/core/#brainsmith.core.DSETree.get_all_segments","title":"<code>get_all_segments() -&gt; List[DSESegment]</code>","text":"<p>Get all segments in the tree.</p> Source code in <code>brainsmith/core/dse/tree.py</code> <pre><code>def get_all_segments(self) -&gt; List[DSESegment]:\n    \"\"\"Get all segments in the tree.\"\"\"\n    all_segments = []\n\n    def collect_segments(node: DSESegment):\n        all_segments.append(node)\n        for child in node.children.values():\n            collect_segments(child)\n\n    collect_segments(self.root)\n    return all_segments\n</code></pre>"},{"location":"api-reference/core/#brainsmith.core.DSETree.get_execution_order","title":"<code>get_execution_order() -&gt; List[DSESegment]</code>","text":"<p>Get breadth-first execution order for the tree.</p> <p>This ensures parent nodes are executed before children, enabling proper result sharing.</p> <p>Returns:</p> Type Description <code>List[DSESegment]</code> <p>List of nodes in execution order</p> Source code in <code>brainsmith/core/dse/tree.py</code> <pre><code>def get_execution_order(self) -&gt; List[DSESegment]:\n    \"\"\"\n    Get breadth-first execution order for the tree.\n\n    This ensures parent nodes are executed before children,\n    enabling proper result sharing.\n\n    Returns:\n        List of nodes in execution order\n    \"\"\"\n    if self.root.segment_id == \"root\" and not self.root.transforms:\n        # Skip empty root node in execution\n        queue = list(self.root.children.values())\n    else:\n        queue = [self.root]\n\n    order = []\n    seen = set()\n\n    while queue:\n        node = queue.pop(0)\n        if id(node) in seen:\n            continue\n\n        seen.add(id(node))\n        order.append(node)\n        queue.extend(node.children.values())\n\n    return order\n</code></pre>"},{"location":"api-reference/core/#brainsmith.core.DSETree.get_leaf_segments","title":"<code>get_leaf_segments() -&gt; List[DSESegment]</code>","text":"<p>Get all complete exploration paths (leaf segments).</p> Source code in <code>brainsmith/core/dse/tree.py</code> <pre><code>def get_leaf_segments(self) -&gt; List[DSESegment]:\n    \"\"\"Get all complete exploration paths (leaf segments).\"\"\"\n    leaves = []\n\n    def collect_leaves(node: DSESegment):\n        if node.is_leaf:\n            leaves.append(node)\n        else:\n            for child in node.children.values():\n                collect_leaves(child)\n\n    collect_leaves(self.root)\n    return leaves\n</code></pre>"},{"location":"api-reference/core/#brainsmith.core.DSETree.get_statistics","title":"<code>get_statistics() -&gt; Dict[str, Any]</code>","text":"<p>Get statistics about the DSE tree.</p> Source code in <code>brainsmith/core/dse/tree.py</code> <pre><code>def get_statistics(self) -&gt; Dict[str, Any]:\n    \"\"\"Get statistics about the DSE tree.\"\"\"\n    leaf_count = self.count_leaves()\n    node_count = self.count_nodes()\n\n    # Calculate depth\n    max_depth = 0\n\n    def calculate_depth(node: DSESegment, depth: int = 0):\n        nonlocal max_depth\n        max_depth = max(max_depth, depth)  # Count depth from root\n        for child in node.children.values():\n            calculate_depth(child, depth + 1)\n\n    calculate_depth(self.root)\n\n    # Count total transforms\n    total_transforms = 0\n\n    def count_transforms(node: DSESegment):\n        nonlocal total_transforms\n        total_transforms += len(node.transforms)\n        for child in node.children.values():\n            count_transforms(child)\n\n    count_transforms(self.root)\n\n    # Calculate segment efficiency\n    # Without segments, we'd execute all transforms for each path\n    transforms_without_segments = 0\n    for leaf in self.get_leaf_segments():\n        transforms_without_segments += len(leaf.get_all_transforms())\n\n    segment_efficiency = 1 - (total_transforms / transforms_without_segments) if transforms_without_segments &gt; 0 else 0\n\n    return {\n        'total_paths': leaf_count,\n        'total_segments': node_count,\n        'max_depth': max_depth,\n        'total_transforms': total_transforms,\n        'transforms_without_segments': transforms_without_segments,\n        'segment_efficiency': round(segment_efficiency * 100, 1),  # As percentage\n        'avg_transforms_per_segment': round(total_transforms / node_count, 1) if node_count &gt; 0 else 0\n    }\n</code></pre>"},{"location":"api-reference/core/#brainsmith.core.DSETree.print_tree","title":"<code>print_tree() -&gt; None</code>","text":"<p>Pretty print the DSE tree.</p> Source code in <code>brainsmith/core/dse/tree.py</code> <pre><code>def print_tree(self) -&gt; None:\n    \"\"\"Pretty print the DSE tree.\"\"\"\n    self._print_node(self.root, \"\", True)\n</code></pre>"},{"location":"api-reference/core/#brainsmith.core.SegmentRunner","title":"<code>SegmentRunner(finn_runner: FINNRunner, base_config: Dict[str, Any], kernel_selections: list = None)</code>","text":"<p>Runs DSE segments using FINN.</p> <p>Handles both tree traversal and individual segment execution using FINNRunner for all FINN interactions.</p> <p>Initialize runner.</p> <p>Parameters:</p> Name Type Description Default <code>finn_runner</code> <code>FINNRunner</code> <p>Runner for FINN-specific operations</p> required <code>base_config</code> <code>Dict[str, Any]</code> <p>FINN configuration from blueprint</p> required <code>kernel_selections</code> <code>list</code> <p>Optional list of (kernel, backend) tuples</p> <code>None</code> Source code in <code>brainsmith/core/dse/runner.py</code> <pre><code>def __init__(\n    self,\n    finn_runner: FINNRunner,\n    base_config: Dict[str, Any],\n    kernel_selections: list = None\n) -&gt; None:\n    \"\"\"Initialize runner.\n\n    Args:\n        finn_runner: Runner for FINN-specific operations\n        base_config: FINN configuration from blueprint\n        kernel_selections: Optional list of (kernel, backend) tuples\n    \"\"\"\n    self.finn_runner = finn_runner\n    self.base_config = base_config\n    self.kernel_selections = kernel_selections or []\n\n    # Extract settings from FINN config\n    self.fail_fast = False  # TODO: Add more robust tree exit options\n    output_products = base_config.get(\"output_products\", [\"estimates\"])\n    # Take first output product as primary target\n    self.output_product = output_products[0] if output_products else \"estimates\"\n\n    # Validate required FINN config fields\n    if \"synth_clk_period_ns\" not in base_config:\n        raise ValueError(\"finn_config must specify synth_clk_period_ns\")\n    if \"board\" not in base_config:\n        raise ValueError(\"finn_config must specify board\")\n\n    # Map output products to FINN types\n    self.output_map = {\n        \"estimates\": [\"estimate_reports\"],\n        \"rtl_sim\": [\"estimate_reports\", \"rtlsim_performance\"],\n        \"ip_gen\": [\"estimate_reports\", \"rtlsim_performance\", \"stitched_ip\"],\n        \"bitfile\": [\n            \"estimate_reports\",\n            \"rtlsim_performance\",\n            \"stitched_ip\",\n            \"bitfile\",\n            \"deployment_package\"\n        ]\n    }\n</code></pre>"},{"location":"api-reference/core/#brainsmith.core.SegmentRunner.run_segment","title":"<code>run_segment(segment: DSESegment, input_model: Path, base_output_dir: Path) -&gt; SegmentResult</code>","text":"<p>Run a single DSE segment.</p> <p>Parameters:</p> Name Type Description Default <code>segment</code> <code>DSESegment</code> <p>Segment to execute</p> required <code>input_model</code> <code>Path</code> <p>Input ONNX model path</p> required <code>base_output_dir</code> <code>Path</code> <p>Base output directory</p> required <p>Returns:</p> Type Description <code>SegmentResult</code> <p>SegmentResult with execution details</p> Source code in <code>brainsmith/core/dse/runner.py</code> <pre><code>def run_segment(\n    self,\n    segment: DSESegment,\n    input_model: Path,\n    base_output_dir: Path\n) -&gt; SegmentResult:\n    \"\"\"Run a single DSE segment.\n\n    Args:\n        segment: Segment to execute\n        input_model: Input ONNX model path\n        base_output_dir: Base output directory\n\n    Returns:\n        SegmentResult with execution details\n    \"\"\"\n    segment_dir = base_output_dir / segment.segment_id\n    # Use safe filename (segment_id may contain slashes)\n    safe_name = segment.segment_id.replace(\"/\", \"_\")\n    output_model = segment_dir / f\"{safe_name}_output.onnx\"\n\n    if output_model.exists():\n        # Verify it's a valid ONNX file before using cache\n        try:\n            import onnx\n            onnx.load(str(output_model))\n\n            print(f\"\u2713 Using cached: {segment.segment_id}\")\n            return SegmentResult(\n                success=True,\n                segment_id=segment.segment_id,\n                output_model=output_model,\n                output_dir=segment_dir,\n                cached=True\n            )\n        except Exception:\n            # Invalid cache, remove it and rebuild\n            output_model.unlink()\n\n    print(f\"\\n\u2192 Executing: {segment.segment_id}\")\n\n    # Create FINN config\n    finn_config = self._make_finn_config(segment, segment_dir)\n\n    # Prepare directory and model\n    segment_dir.mkdir(parents=True, exist_ok=True)\n    segment_input = segment_dir / \"input.onnx\"\n    self.finn_runner.prepare_model(input_model, segment_input)\n\n    # Execute build\n    start_time = time.time()\n\n    try:\n        # Use runner for clean FINN interaction\n        final_model = self.finn_runner.build(segment_input, finn_config, segment_dir)\n\n        if final_model:\n            # Copy to expected location\n            self.finn_runner.prepare_model(final_model, output_model)\n            print(f\"\u2713 Completed: {segment.segment_id}\")\n            return SegmentResult(\n                success=True,\n                segment_id=segment.segment_id,\n                output_model=output_model,\n                output_dir=segment_dir,\n                execution_time=time.time() - start_time\n            )\n        else:\n            raise RuntimeError(\"Build succeeded but no output model generated\")\n\n    except ExecutionError:\n        # Re-raise our own errors\n        raise\n    except Exception as e:\n        # Wrap external errors with context but preserve stack trace\n        print(f\"\u2717 Failed: {segment.segment_id}\")\n        raise ExecutionError(\n            f\"Segment '{segment.segment_id}' build failed: {str(e)}\"\n        ) from e\n</code></pre>"},{"location":"api-reference/core/#brainsmith.core.SegmentRunner.run_tree","title":"<code>run_tree(tree: DSETree, initial_model: Path, output_dir: Path) -&gt; TreeExecutionResult</code>","text":"<p>Run all segments in the DSE tree.</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>DSETree</code> <p>DSE tree to execute</p> required <code>initial_model</code> <code>Path</code> <p>Path to initial ONNX model</p> required <code>output_dir</code> <code>Path</code> <p>Base output directory</p> required <p>Returns:</p> Type Description <code>TreeExecutionResult</code> <p>TreeExecutionResult with all segment results</p> Source code in <code>brainsmith/core/dse/runner.py</code> <pre><code>def run_tree(\n    self,\n    tree: DSETree,\n    initial_model: Path,\n    output_dir: Path\n) -&gt; TreeExecutionResult:\n    \"\"\"Run all segments in the DSE tree.\n\n    Args:\n        tree: DSE tree to execute\n        initial_model: Path to initial ONNX model\n        output_dir: Base output directory\n\n    Returns:\n        TreeExecutionResult with all segment results\n    \"\"\"\n    output_dir = Path(output_dir)\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    print(f\"Executing tree with fail_fast={self.fail_fast}\")\n    print(f\"Output: {output_dir}\")\n\n    results = {}\n    skipped = set()\n    start_time = time.time()\n\n    # Use a stack for cleaner iteration\n    stack = [(tree.root, initial_model, 0)]\n\n    while stack:\n        segment, input_model, depth = stack.pop()\n        indent = \"  \" * depth\n\n        # Skip if parent failed\n        if segment.segment_id in skipped:\n            print(f\"{indent}\u2298 Skipped: {segment.segment_id}\")\n            results[segment.segment_id] = SegmentResult(\n                success=False,\n                segment_id=segment.segment_id,\n                error=\"Skipped\"\n            )\n            continue\n\n        # Execute segment\n        print(f\"{indent}\u2192 {segment.segment_id}\")\n\n        # Skip empty segments (e.g., root with immediate branches)\n        if not segment.transforms:\n            print(f\"{indent}  (empty segment, passing through)\")\n            # Create a pass-through result\n            results[segment.segment_id] = SegmentResult(\n                success=True,\n                segment_id=segment.segment_id,\n                output_model=input_model,  # Pass input as output\n                output_dir=output_dir / segment.segment_id,\n                execution_time=0\n            )\n            # Add children to stack\n            for child in reversed(list(segment.children.values())):\n                stack.append((child, input_model, depth + 1))\n            continue\n\n        try:\n            result = self.run_segment(segment, input_model, output_dir)\n            results[segment.segment_id] = result\n        except ExecutionError as e:\n            # Handle execution errors properly\n            print(f\"\u2717 Failed: {segment.segment_id}\")\n            print(f\"  Error: {str(e)}\")\n            if self.fail_fast:\n                raise\n\n            # Create failure result with actual exception\n            results[segment.segment_id] = SegmentResult(\n                success=False,\n                segment_id=segment.segment_id,\n                error=str(e),\n                execution_time=0\n            )\n\n            # Mark descendants for skipping\n            self._mark_descendants_skipped(segment, skipped)\n            # Still need to add children to stack so they get marked as skipped\n            for child in reversed(list(segment.children.values())):\n                stack.append((child, None, depth + 1))\n            continue\n        except Exception as e:\n            # Catch any unexpected errors\n            print(f\"\u2717 Failed: {segment.segment_id}\")\n            print(f\"  Unexpected error: {type(e).__name__}: {str(e)}\")\n            import traceback\n            traceback.print_exc()\n\n            # Create failure result\n            results[segment.segment_id] = SegmentResult(\n                success=False,\n                segment_id=segment.segment_id,\n                error=f\"{type(e).__name__}: {str(e)}\",\n                execution_time=0\n            )\n\n            # Mark descendants for skipping\n            self._mark_descendants_skipped(segment, skipped)\n            for child in reversed(list(segment.children.values())):\n                stack.append((child, None, depth + 1))\n            continue\n\n        # Share artifacts at branch points\n        if segment.is_branch_point:\n            share_artifacts_at_branch(result, list(segment.children.values()), output_dir)\n\n        # Add children to stack (reversed for correct order)\n        for child in reversed(list(segment.children.values())):\n            stack.append((child, result.output_model, depth + 1))\n\n    # Create result and print summary\n    total_time = time.time() - start_time\n    result = TreeExecutionResult(results, total_time)\n    self._print_summary(result)\n\n    return result\n</code></pre>"},{"location":"api-reference/core/#design-components","title":"Design Components","text":""},{"location":"api-reference/core/#brainsmith.core.DesignSpace","title":"<code>DesignSpace(model_path: str, steps: List[Union[str, List[Optional[str]]]], kernel_backends: List[Tuple[str, List[Type]]], max_combinations: int = 100000)</code>  <code>dataclass</code>","text":"<p>Design space with resolved plugin objects.</p> <p>This is a clean intermediate representation between blueprint YAML and the execution tree. All plugin names have been resolved to actual classes from the registry.</p>"},{"location":"api-reference/core/#brainsmith.core.DesignSpace.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate steps after initialization.</p> Source code in <code>brainsmith/core/design/space.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate steps after initialization.\"\"\"\n    self.validate_steps()\n</code></pre>"},{"location":"api-reference/core/#brainsmith.core.DesignSpace.__str__","title":"<code>__str__() -&gt; str</code>","text":"<p>Human-readable representation.</p> Source code in <code>brainsmith/core/design/space.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Human-readable representation.\"\"\"\n    return (\n        f\"DesignSpace(\\n\"\n        f\"  model: {self.model_path}\\n\"\n        f\"  steps: {len(self.steps)}\\n\"\n        f\"  kernels: {len(self.kernel_backends)}\\n\"\n        f\")\"\n    )\n</code></pre>"},{"location":"api-reference/core/#brainsmith.core.DesignSpace.get_kernel_summary","title":"<code>get_kernel_summary() -&gt; str</code>","text":"<p>Get human-readable summary of kernels and backends.</p> Source code in <code>brainsmith/core/design/space.py</code> <pre><code>def get_kernel_summary(self) -&gt; str:\n    \"\"\"Get human-readable summary of kernels and backends.\"\"\"\n    lines = []\n    for kernel_name, backend_classes in self.kernel_backends:\n        backend_names = [cls.__name__ for cls in backend_classes]\n        lines.append(f\"  {kernel_name}: {', '.join(backend_names)}\")\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"api-reference/core/#brainsmith.core.DesignSpace.validate_size","title":"<code>validate_size() -&gt; None</code>","text":"<p>Validate that design space doesn't exceed max combinations.</p> Source code in <code>brainsmith/core/design/space.py</code> <pre><code>def validate_size(self) -&gt; None:\n    \"\"\"Validate that design space doesn't exceed max combinations.\"\"\"\n    estimated_size = self._estimate_combinations()\n    if estimated_size &gt; self.max_combinations:\n        raise ValueError(\n            f\"Design space too large: {estimated_size:,} combinations exceeds \"\n            f\"limit of {self.max_combinations:,}\"\n        )\n</code></pre>"},{"location":"api-reference/core/#brainsmith.core.DesignSpace.validate_steps","title":"<code>validate_steps() -&gt; None</code>","text":"<p>Validate that all referenced steps exist in the registry.</p> Source code in <code>brainsmith/core/design/space.py</code> <pre><code>def validate_steps(self) -&gt; None:\n    \"\"\"Validate that all referenced steps exist in the registry.\"\"\"\n    invalid_steps = []\n\n    def check_step(step: Optional[str]) -&gt; None:\n        \"\"\"Check if a single step is valid.\"\"\"\n        if step and step not in [\"~\", \"\"] and not has_step(step):\n            invalid_steps.append(step)\n\n    # Check all steps, including those in branch points\n    for step_spec in self.steps:\n        if isinstance(step_spec, list):\n            # Branch point - check each option\n            for option in step_spec:\n                check_step(option)\n        else:\n            # Single step\n            check_step(step_spec)\n\n    if invalid_steps:\n        available_steps = list_all_steps()\n        # Find similar steps for suggestions\n        suggestions = []\n        for invalid in invalid_steps[:3]:  # Limit suggestions\n            similar = [s for s in available_steps if invalid.lower() in s.lower() or s.lower() in invalid.lower()]\n            if similar:\n                suggestions.extend(similar[:2])\n\n        error_msg = f\"Invalid steps found: {', '.join(invalid_steps)}\"\n        if suggestions:\n            error_msg += f\"\\n\\nDid you mean one of these? {', '.join(set(suggestions))}\"\n        error_msg += f\"\\n\\nAvailable steps: {', '.join(available_steps)}\"\n        raise ValueError(error_msg)\n</code></pre>"},{"location":"api-reference/core/#brainsmith.core.parse_blueprint","title":"<code>parse_blueprint(blueprint_path: str, model_path: str) -&gt; Tuple[DesignSpace, ForgeConfig]</code>","text":"<p>Parse blueprint YAML and return DesignSpace and ForgeConfig.</p> <p>Inheritance is resolved bottom-up: 1. Start from the root parent (no extends) 2. Fully resolve its steps (including operations) 3. Pass resolved steps to child for its operations 4. Repeat until we reach the target blueprint</p> Source code in <code>brainsmith/core/design/parser.py</code> <pre><code>def parse_blueprint(blueprint_path: str, model_path: str) -&gt; Tuple[DesignSpace, ForgeConfig]:\n    \"\"\"\n    Parse blueprint YAML and return DesignSpace and ForgeConfig.\n\n    Inheritance is resolved bottom-up:\n    1. Start from the root parent (no extends)\n    2. Fully resolve its steps (including operations)\n    3. Pass resolved steps to child for its operations\n    4. Repeat until we reach the target blueprint\n    \"\"\"\n    # Load raw data to check inheritance chain\n    raw_data = load_yaml(\n        blueprint_path,\n        expand_env_vars=True,\n        support_inheritance=False,\n        context_vars={'BLUEPRINT_DIR': str(Path(blueprint_path).parent.absolute())}\n    )\n\n    parent_steps = None\n\n    # If this blueprint extends another, first parse the parent\n    if 'extends' in raw_data:\n        parent_path = raw_data['extends']\n        # Expand env vars in parent path\n        parent_path = expand_env_vars_with_context(\n            parent_path,\n            {'BSMITH_DIR': os.environ.get('BSMITH_DIR', str(Path(__file__).parent.parent.parent.parent.absolute()))}\n        )\n\n        # Resolve parent path relative to current file\n        if not Path(parent_path).is_absolute():\n            parent_path = str(Path(blueprint_path).parent / parent_path)\n\n        # Recursively parse parent to get its fully resolved steps\n        parent_design_space, _ = parse_blueprint(parent_path, model_path)\n        parent_steps = parent_design_space.steps\n\n    # Now load the full merged data for config extraction\n    blueprint_data = load_yaml(\n        blueprint_path,\n        expand_env_vars=True,\n        support_inheritance=True,\n        context_vars={'BLUEPRINT_DIR': str(Path(blueprint_path).parent.absolute())}\n    )\n\n    forge_config = _extract_config_and_mappings(blueprint_data)\n\n    # Parse steps from THIS blueprint only (not inherited steps)\n    # Use raw_data to get only the steps defined in this file\n    steps_data = raw_data.get('design_space', {}).get('steps', [])\n    steps = _parse_steps(steps_data, parent_steps=parent_steps)\n\n    # Parse kernels (use merged data to inherit kernels)\n    kernel_backends = _parse_kernels(blueprint_data.get('design_space', {}).get('kernels', []))\n\n    # Get max_combinations from environment or use default\n    max_combinations = int(os.environ.get(\"BRAINSMITH_MAX_COMBINATIONS\", \"100000\"))\n\n    design_space = DesignSpace(\n        model_path=model_path,\n        steps=steps,\n        kernel_backends=kernel_backends,\n        max_combinations=max_combinations\n    )\n    design_space.validate_size()\n    return design_space, forge_config\n</code></pre>"},{"location":"api-reference/core/#configuration","title":"Configuration","text":""},{"location":"api-reference/core/#brainsmith.core.ForgeConfig","title":"<code>ForgeConfig(clock_ns: float, output: Literal['estimates', 'rtl', 'bitfile'] = 'estimates', board: Optional[str] = None, verify: bool = False, verify_data: Optional[Path] = None, parallel_builds: int = 4, debug: bool = False, save_intermediate_models: bool = False, start_step: Optional[str] = None, stop_step: Optional[str] = None, finn_overrides: Dict[str, Any] = dict())</code>  <code>dataclass</code>","text":"<p>Configuration that actually works.</p>"},{"location":"api-reference/plugins/","title":"Plugin System API Reference","text":"<p>The plugin system is the core of Brainsmith's extensibility.</p>"},{"location":"api-reference/plugins/#registry","title":"Registry","text":""},{"location":"api-reference/plugins/#brainsmith.core.plugins.registry.Registry","title":"<code>Registry()</code>","text":"Source code in <code>brainsmith/core/plugins/registry.py</code> <pre><code>def __init__(self):\n    self._plugins: Dict[str, Dict[str, Tuple[Type, Dict[str, Any]]]] = {\n        'transform': {}, 'kernel': {}, 'backend': {}, 'step': {}\n    }\n</code></pre>"},{"location":"api-reference/plugins/#brainsmith.core.plugins.registry.Registry.register","title":"<code>register(plugin_type: str, name: str, cls: Type, framework: str = 'brainsmith', **metadata) -&gt; None</code>","text":"<p>Register a plugin with optional framework namespace.</p> Source code in <code>brainsmith/core/plugins/registry.py</code> <pre><code>def register(self, plugin_type: str, name: str, cls: Type, \n             framework: str = 'brainsmith', **metadata) -&gt; None:\n    \"\"\"Register a plugin with optional framework namespace.\"\"\"\n    key = f\"{framework}:{name}\" if framework != 'brainsmith' else name\n    self._plugins[plugin_type][key] = (cls, {**metadata, 'framework': framework})\n</code></pre>"},{"location":"api-reference/plugins/#brainsmith.core.plugins.registry.Registry.get","title":"<code>get(plugin_type: str, name: str) -&gt; Type</code>","text":"<p>Get plugin by name (with or without framework prefix).</p> Source code in <code>brainsmith/core/plugins/registry.py</code> <pre><code>def get(self, plugin_type: str, name: str) -&gt; Type:\n    \"\"\"Get plugin by name (with or without framework prefix).\n    \"\"\"\n    self._load_plugins()\n\n    # Direct lookup first\n    plugins = self._plugins[plugin_type]\n    if name in plugins:\n        return plugins[name][0]\n\n    # If no colon, try with framework prefixes\n    if ':' not in name:\n        # Try common prefixes\n        for prefix in ['brainsmith:', 'finn:', 'qonnx:']:\n            full_name = f'{prefix}{name}'\n            if full_name in plugins:\n                return plugins[full_name][0]\n\n    # Plugin not found - fail fast\n    available = list(self._plugins[plugin_type].keys())\n    raise KeyError(\n        f\"Plugin {plugin_type}:{name} not found. \"\n        f\"Available ({len(available)}): {available[:10] if available else 'none'}\"\n    )\n</code></pre>"},{"location":"api-reference/plugins/#brainsmith.core.plugins.registry.Registry.find","title":"<code>find(plugin_type: str, **criteria) -&gt; List[Type]</code>","text":"<p>Find plugins matching criteria.</p> Source code in <code>brainsmith/core/plugins/registry.py</code> <pre><code>def find(self, plugin_type: str, **criteria) -&gt; List[Type]:\n    \"\"\"Find plugins matching criteria.\"\"\"\n    self._load_plugins()\n    results = []\n    for name, (cls, metadata) in self._plugins[plugin_type].items():\n        if all(metadata.get(k) == v for k, v in criteria.items()):\n            results.append(cls)\n    return results\n</code></pre>"},{"location":"api-reference/plugins/#brainsmith.core.plugins.registry.Registry.all","title":"<code>all(plugin_type: str) -&gt; Dict[str, Type]</code>","text":"<p>Get all plugins of a type.</p> Source code in <code>brainsmith/core/plugins/registry.py</code> <pre><code>def all(self, plugin_type: str) -&gt; Dict[str, Type]:\n    \"\"\"Get all plugins of a type.\"\"\"\n    self._load_plugins()\n    return {name: cls for name, (cls, _) in self._plugins[plugin_type].items()}\n</code></pre>"},{"location":"api-reference/plugins/#brainsmith.core.plugins.registry.Registry.reset","title":"<code>reset() -&gt; None</code>","text":"<p>Reset registry and reload all plugins.</p> <p>This is primarily for testing to ensure a clean state.</p> Source code in <code>brainsmith/core/plugins/registry.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Reset registry and reload all plugins.\n\n    This is primarily for testing to ensure a clean state.\n    \"\"\"\n    # Clear all plugins\n    self._plugins = {\n        'transform': {}, 'kernel': {}, 'backend': {}, 'step': {}\n    }\n\n    # Reset the discovery flag to force reloading\n    if hasattr(self, '_discovered'):\n        delattr(self, '_discovered')\n\n    # Reset framework adapter initialization state\n    try:\n        from . import framework_adapters\n        framework_adapters._initialized = False\n    except ImportError:\n        pass\n\n    self._load_plugins()\n\n    logger.debug(\"Registry reset and plugins reloaded\")\n</code></pre>"},{"location":"api-reference/plugins/#registration-decorators","title":"Registration Decorators","text":""},{"location":"api-reference/plugins/#brainsmith.core.plugins.registry.transform","title":"<code>transform = lambda **kw: plugin('transform', **kw)</code>  <code>module-attribute</code>","text":""},{"location":"api-reference/plugins/#brainsmith.core.plugins.registry.kernel","title":"<code>kernel = lambda **kw: plugin('kernel', **kw)</code>  <code>module-attribute</code>","text":""},{"location":"api-reference/plugins/#brainsmith.core.plugins.registry.backend","title":"<code>backend = lambda **kw: plugin('backend', **kw)</code>  <code>module-attribute</code>","text":""},{"location":"api-reference/plugins/#brainsmith.core.plugins.registry.step","title":"<code>step = lambda **kw: plugin('step', **kw)</code>  <code>module-attribute</code>","text":""},{"location":"api-reference/plugins/#access-functions","title":"Access Functions","text":""},{"location":"api-reference/plugins/#brainsmith.core.plugins.registry.get_transform","title":"<code>get_transform(name: str) -&gt; Type</code>","text":"Source code in <code>brainsmith/core/plugins/registry.py</code> <pre><code>def get_transform(name: str) -&gt; Type:\n    return _registry.get('transform', name)\n</code></pre>"},{"location":"api-reference/plugins/#brainsmith.core.plugins.registry.get_kernel","title":"<code>get_kernel(name: str) -&gt; Type</code>","text":"Source code in <code>brainsmith/core/plugins/registry.py</code> <pre><code>def get_kernel(name: str) -&gt; Type:\n    return _registry.get('kernel', name)\n</code></pre>"},{"location":"api-reference/plugins/#brainsmith.core.plugins.registry.get_backend","title":"<code>get_backend(name: str) -&gt; Type</code>","text":"Source code in <code>brainsmith/core/plugins/registry.py</code> <pre><code>def get_backend(name: str) -&gt; Type:\n    return _registry.get('backend', name)\n</code></pre>"},{"location":"api-reference/plugins/#brainsmith.core.plugins.registry.get_step","title":"<code>get_step(name: str) -&gt; Type</code>","text":"Source code in <code>brainsmith/core/plugins/registry.py</code> <pre><code>def get_step(name: str) -&gt; Type:\n    return _registry.get('step', name)\n</code></pre>"},{"location":"api-reference/plugins/#query-functions","title":"Query Functions","text":""},{"location":"api-reference/plugins/#brainsmith.core.plugins.registry.list_transforms","title":"<code>list_transforms() -&gt; List[str]</code>","text":"<p>List all transform names.</p> Source code in <code>brainsmith/core/plugins/registry.py</code> <pre><code>def list_transforms() -&gt; List[str]:\n    \"\"\"List all transform names.\"\"\"\n    _registry._load_plugins()\n    return list(_registry._plugins['transform'].keys())\n</code></pre>"},{"location":"api-reference/plugins/#brainsmith.core.plugins.registry.list_kernels","title":"<code>list_kernels() -&gt; List[str]</code>","text":"<p>List all kernel names.</p> Source code in <code>brainsmith/core/plugins/registry.py</code> <pre><code>def list_kernels() -&gt; List[str]:\n    \"\"\"List all kernel names.\"\"\"\n    _registry._load_plugins()\n    return list(_registry._plugins['kernel'].keys())\n</code></pre>"},{"location":"api-reference/plugins/#brainsmith.core.plugins.registry.has_transform","title":"<code>has_transform(name: str) -&gt; bool</code>","text":"<p>Check if transform exists.</p> Source code in <code>brainsmith/core/plugins/registry.py</code> <pre><code>def has_transform(name: str) -&gt; bool:\n    \"\"\"Check if transform exists.\"\"\"\n    try:\n        _registry.get('transform', name)\n        return True\n    except KeyError:\n        return False\n</code></pre>"},{"location":"api-reference/plugins/#brainsmith.core.plugins.registry.has_kernel","title":"<code>has_kernel(name: str) -&gt; bool</code>","text":"<p>Check if kernel exists.</p> Source code in <code>brainsmith/core/plugins/registry.py</code> <pre><code>def has_kernel(name: str) -&gt; bool:\n    \"\"\"Check if kernel exists.\"\"\"\n    try:\n        _registry.get('kernel', name)\n        return True\n    except KeyError:\n        return False\n</code></pre>"},{"location":"architecture/overview/","title":"Architecture Overview","text":"<p>Brainsmith's architecture is built around three core concepts: Blueprints, Plugins, and Segment-based DSE.</p>"},{"location":"architecture/overview/#high-level-architecture","title":"High-Level Architecture","text":"<pre><code>graph TB\n    A[Blueprint YAML] --&gt; B[Design Space Parser]\n    B --&gt; C[DSE Tree Builder]\n    C --&gt; D[Exploration Tree]\n    D --&gt; E[Segment Runner]\n    E --&gt; F{Plugin Registry}\n    F --&gt; G[Transforms]\n    F --&gt; H[Kernels]\n    F --&gt; I[Steps]\n    E --&gt; J[Build Artifacts]\n    J --&gt; K[RTL Output]</code></pre>"},{"location":"architecture/overview/#core-components","title":"Core Components","text":""},{"location":"architecture/overview/#1-blueprint-system","title":"1. Blueprint System","text":"<p>Blueprints define design spaces in YAML:</p> <pre><code>name: \"My Accelerator\"\ndesign_space:\n  kernels:\n    - MatMul\n    - Conv2d\n  steps:\n    - cleanup\n    - qonnx_to_finn\n    - step_create_dataflow_partition\n</code></pre> <p>Key Features:</p> <ul> <li>Inheritance support (<code>extends: base.yaml</code>)</li> <li>Dynamic step operations (insert, replace, remove)</li> <li>Parameter sweeps for exploration</li> </ul> <p>Location: <code>brainsmith/core/design/</code></p>"},{"location":"architecture/overview/#2-plugin-registry","title":"2. Plugin Registry","text":"<p>A singleton registry manages all extensible components:</p> <pre><code>from brainsmith.core.plugins import transform, kernel, step\n\n@transform(name=\"MyTransform\")\nclass MyTransform:\n    def apply(self, model):\n        # Transform logic\n        pass\n\n@kernel(name=\"MyKernel\")\nclass MyKernel:\n    # Kernel implementation\n    pass\n</code></pre> <p>Plugin Types:</p> <ul> <li>Transforms - Graph transformations</li> <li>Kernels - Hardware operator implementations</li> <li>Backends - RTL/HLS implementations per kernel</li> <li>Steps - Build pipeline operations</li> </ul> <p>Location: <code>brainsmith/core/plugins/registry.py</code></p>"},{"location":"architecture/overview/#3-segment-based-dse","title":"3. Segment-Based DSE","text":"<p>The exploration tree is divided into segments for efficient computation reuse:</p> <pre><code>graph TD\n    A[Root: Model Input] --&gt; B[Segment 1: Preprocessing]\n    B --&gt; C[Segment 2: Kernel Selection]\n    C --&gt; D[Branch A: RTL Backend]\n    C --&gt; E[Branch B: HLS Backend]\n    D --&gt; F[Segment 3a: RTL Codegen]\n    E --&gt; G[Segment 3b: HLS Codegen]</code></pre> <p>Benefits:</p> <ul> <li>Only changed segments re-execute</li> <li>Shared artifacts cached across branches</li> <li>Parallelizable execution (planned)</li> </ul> <p>Location: <code>brainsmith/core/dse/</code></p>"},{"location":"architecture/overview/#compilation-pipeline","title":"Compilation Pipeline","text":"<p>The standard dataflow compilation follows this pipeline:</p> <pre><code>sequenceDiagram\n    participant U as User\n    participant B as Blueprint Parser\n    participant D as DSE Engine\n    participant P as Plugin Registry\n    participant F as FINN\n\n    U-&gt;&gt;B: Load blueprint\n    B-&gt;&gt;D: Build exploration tree\n    D-&gt;&gt;P: Get transforms/kernels\n    P-&gt;&gt;D: Return plugin instances\n    D-&gt;&gt;F: Execute transforms\n    F-&gt;&gt;D: Return transformed model\n    D-&gt;&gt;U: Generate RTL + reports</code></pre>"},{"location":"architecture/overview/#pipeline-stages","title":"Pipeline Stages","text":"<ol> <li>ONNX \u2192 QONNX - Add quantization metadata</li> <li>QONNX \u2192 FINN - Create dataflow partition</li> <li>Kernel Inference - Replace ops with hardware kernels</li> <li>Specialization - Configure kernel parameters</li> <li>Folding - Apply parallelization strategy</li> <li>Bit Width Minimization - Optimize data types</li> <li>Hardware Codegen - Generate RTL/HLS</li> <li>IP Generation - Create Vivado IP blocks</li> <li>FIFO Sizing - Determine buffer depths</li> <li>Stitched IP - Connect kernels with AXI stream</li> <li>RTL Simulation - Verify correctness</li> </ol>"},{"location":"architecture/overview/#configuration-system","title":"Configuration System","text":"<p>Pydantic-based configuration with layered overrides:</p> <pre><code>graph LR\n    A[Built-in Defaults] --&gt; B[Project Config]\n    B --&gt; C[User Config]\n    C --&gt; D[CLI Args]\n    D --&gt; E[Effective Config]</code></pre> <p>Priority Order:</p> <ol> <li>CLI arguments / environment vars (highest)</li> <li>Explicit <code>--config</code> file</li> <li><code>~/.brainsmith/config.yaml</code> (user)</li> <li><code>.brainsmith/config.yaml</code> (project)</li> <li>Built-in defaults (lowest)</li> </ol> <p>Location: <code>brainsmith/config/</code></p>"},{"location":"architecture/overview/#two-cli-design","title":"Two-CLI Design","text":""},{"location":"architecture/overview/#brainsmith-cli","title":"<code>brainsmith</code> CLI","text":"<p>Application-level commands:</p> <ul> <li><code>config init/show/export</code> - Configuration management</li> <li><code>setup all</code> - Dependency installation</li> <li><code>smith ...</code> - Proxy to smith CLI</li> </ul>"},{"location":"architecture/overview/#smith-cli","title":"<code>smith</code> CLI","text":"<p>Operational commands:</p> <ul> <li><code>dse model.onnx blueprint.yaml</code> - Run DSE</li> <li><code>kernel file.sv</code> - Generate kernel from RTL</li> </ul> <p>Location: <code>brainsmith/interface/cli.py</code></p>"},{"location":"architecture/overview/#key-patterns","title":"Key Patterns","text":""},{"location":"architecture/overview/#decorator-based-registration","title":"Decorator-Based Registration","text":"<pre><code>@transform(name=\"CustomTransform\", framework=\"brainsmith\")\nclass CustomTransform(Transformation):\n    pass\n</code></pre>"},{"location":"architecture/overview/#framework-integration","title":"Framework Integration","text":"<p>External transforms from FINN/QONNX are wrapped:</p> <pre><code># Automatically wrapped\nget_transform(\"finn:Streamline\")  # FINN transform\nget_transform(\"qonnx:InferShapes\")  # QONNX transform\n</code></pre>"},{"location":"architecture/overview/#lazy-plugin-loading","title":"Lazy Plugin Loading","text":"<p>Plugins are discovered on first access:</p> <pre><code># First call triggers discovery\ntransform_cls = get_transform(\"MyTransform\")\n</code></pre>"},{"location":"architecture/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Plugin System - Deep dive into plugins</li> </ul>"},{"location":"architecture/plugin-system/","title":"Plugin System","text":"<p>The plugin system is Brainsmith's core extensibility mechanism, enabling custom kernels, transforms, and build steps.</p>"},{"location":"architecture/plugin-system/#overview","title":"Overview","text":"<p>The plugin registry is a singleton that manages all extensible components with decorator-based registration:</p> <pre><code>from brainsmith.core.plugins import transform, kernel, backend, step\n\n@transform(name=\"MyTransform\")\nclass MyTransform:\n    def apply(self, model):\n        # Your transformation logic\n        return model\n\n@kernel(name=\"MyKernel\")\nclass MyKernel:\n    pass\n\n@backend(kernel=\"MyKernel\", name=\"rtl\", default=True)\nclass MyKernelRTL:\n    pass\n\n@step(name=\"my_step\")\ndef my_custom_step(model, cfg):\n    # Custom build step\n    return model\n</code></pre>"},{"location":"architecture/plugin-system/#plugin-types","title":"Plugin Types","text":""},{"location":"architecture/plugin-system/#transforms","title":"Transforms","text":"<p>Graph transformations that modify the ONNX/QONNX model:</p> <pre><code>@transform(name=\"RemoveIdentity\", framework=\"brainsmith\")\nclass RemoveIdentity(Transformation):\n    def apply(self, model):\n        # Remove identity nodes\n        for node in model.graph.node:\n            if node.op_type == \"Identity\":\n                # Remove node logic\n                pass\n        return model\n</code></pre> <p>Access: <pre><code>from brainsmith.core.plugins import get_transform\n\ntransform_cls = get_transform(\"RemoveIdentity\")\ntransform = transform_cls()\nmodel = transform.apply(model)\n</code></pre></p>"},{"location":"architecture/plugin-system/#kernels","title":"Kernels","text":"<p>Hardware operator implementations:</p> <pre><code>@kernel(name=\"MatMul\")\nclass MatMulKernel:\n    \"\"\"Matrix multiplication kernel.\"\"\"\n\n    def __init__(self, onnx_node):\n        self.onnx_node = onnx_node\n\n    def make_weight_file(self):\n        # Generate weight files\n        pass\n</code></pre> <p>Access: <pre><code>from brainsmith.core.plugins import get_kernel\n\nkernel_cls = get_kernel(\"MatMul\")\nkernel = kernel_cls(onnx_node)\n</code></pre></p>"},{"location":"architecture/plugin-system/#backends","title":"Backends","text":"<p>RTL or HLS implementations per kernel:</p> <pre><code>@backend(kernel=\"MatMul\", name=\"rtl\", default=True)\nclass MatMulRTL:\n    \"\"\"RTL backend for MatMul.\"\"\"\n\n    def generate_hdl(self):\n        # Generate RTL code\n        pass\n\n@backend(kernel=\"MatMul\", name=\"hls\")\nclass MatMulHLS:\n    \"\"\"HLS backend for MatMul.\"\"\"\n\n    def generate_hls(self):\n        # Generate HLS code\n        pass\n</code></pre> <p>Access: <pre><code>from brainsmith.core.plugins import get_backend\n\nbackend_cls = get_backend(\"MatMul\", \"rtl\")\nbackend = backend_cls()\n</code></pre></p>"},{"location":"architecture/plugin-system/#steps","title":"Steps","text":"<p>Build pipeline operations:</p> <pre><code>@step(name=\"my_optimization\")\ndef my_optimization_step(model, cfg):\n    \"\"\"Custom optimization step.\"\"\"\n    # Apply optimizations\n    return model\n</code></pre> <p>Access: <pre><code>from brainsmith.core.plugins import get_step\n\nstep_fn = get_step(\"my_optimization\")\nmodel = step_fn(model, config)\n</code></pre></p>"},{"location":"architecture/plugin-system/#framework-integration","title":"Framework Integration","text":"<p>Brainsmith integrates FINN and QONNX transforms automatically:</p> <pre><code># FINN transforms\nget_transform(\"finn:Streamline\")\nget_transform(\"finn:InferShapes\")\n\n# QONNX transforms\nget_transform(\"qonnx:ConvertSubToAdd\")\nget_transform(\"qonnx:InferDataTypes\")\n</code></pre> <p>These are automatically discovered and wrapped with the <code>framework</code> prefix.</p>"},{"location":"architecture/plugin-system/#registration-api","title":"Registration API","text":""},{"location":"architecture/plugin-system/#manual-registration","title":"Manual Registration","text":"<pre><code>from brainsmith.core.plugins.registry import _registry\n\n_registry.register(\n    plugin_type='transform',\n    name='CustomTransform',\n    cls=CustomTransform,\n    framework='brainsmith',\n    metadata={'version': '1.0'}\n)\n</code></pre>"},{"location":"architecture/plugin-system/#query-functions","title":"Query Functions","text":"<pre><code>from brainsmith.core.plugins import (\n    list_transforms,\n    list_kernels,\n    has_transform,\n    has_kernel\n)\n\n# List all transforms\ntransforms = list_transforms()\n\n# List all kernels\nkernels = list_kernels()\n\n# Check if transform exists\nif has_transform(\"MyTransform\"):\n    ...\n\n# Check if kernel exists\nif has_kernel(\"MatMul\"):\n    ...\n</code></pre>"},{"location":"architecture/plugin-system/#discovery-and-loading","title":"Discovery and Loading","text":"<p>Plugins are lazy-loaded on first access:</p> <ol> <li>Auto-discovery: Registry imports <code>brainsmith.{transforms,kernels,steps,operators}</code></li> <li>Framework adapters: FINN/QONNX plugins loaded on first framework prefix access</li> <li>Decorator registration: <code>@transform</code>, <code>@kernel</code>, etc. register during module import</li> </ol> <pre><code># First access triggers discovery\ntransform_cls = get_transform(\"MyTransform\")  # Auto-discovers all transforms\n\n# Subsequent access uses cache\ntransform_cls2 = get_transform(\"MyTransform\")  # From cache\n</code></pre>"},{"location":"architecture/plugin-system/#blueprint-integration","title":"Blueprint Integration","text":"<p>Reference plugins declaratively in blueprints:</p> <pre><code>design_space:\n  kernels:\n    - name: MatMul\n      backends:\n        - rtl  # References MatMulRTL backend\n        - hls  # References MatMulHLS backend\n\n  steps:\n    - cleanup  # References cleanup step\n    - qonnx_to_finn\n    - my_optimization  # References custom step\n    - finn:Streamline  # References FINN transform\n</code></pre>"},{"location":"architecture/plugin-system/#best-practices","title":"Best Practices","text":""},{"location":"architecture/plugin-system/#naming-conventions","title":"Naming Conventions","text":"<ul> <li>Transforms: PascalCase, descriptive (e.g., <code>RemoveUnusedNodes</code>)</li> <li>Kernels: PascalCase, operator name (e.g., <code>Conv2d</code>, <code>MatMul</code>)</li> <li>Backends: <code>{Kernel}{Backend}</code> (e.g., <code>MatMulRTL</code>, <code>Conv2dHLS</code>)</li> <li>Steps: snake_case, action-oriented (e.g., <code>step_create_partition</code>)</li> </ul>"},{"location":"architecture/plugin-system/#metadata","title":"Metadata","text":"<p>Add metadata for discoverability:</p> <pre><code>@transform(\n    name=\"MyTransform\",\n    framework=\"brainsmith\",\n    version=\"1.0\",\n    description=\"Optimizes graph structure\"\n)\nclass MyTransform:\n    pass\n</code></pre>"},{"location":"architecture/plugin-system/#testing","title":"Testing","text":"<p>Test plugins in isolation:</p> <pre><code>def test_my_transform():\n    from brainsmith.core.plugins import get_transform\n\n    transform_cls = get_transform(\"MyTransform\")\n    transform = transform_cls()\n\n    # Test transformation\n    result = transform.apply(model)\n    assert result is not None\n</code></pre>"},{"location":"architecture/plugin-system/#api-reference","title":"API Reference","text":"<p>For detailed API documentation, see:</p> <ul> <li>Plugin Registry API</li> <li>Core API</li> </ul>"},{"location":"getting-started/configuration/","title":"Configuration","text":"<p>Brainsmith uses a Pydantic-based configuration system with multiple sources for maximum flexibility.</p>"},{"location":"getting-started/configuration/#configuration-sources","title":"Configuration Sources","text":"<p>Configurations are loaded in priority order (highest to lowest):</p> <ol> <li>CLI arguments / environment variables - Override all other sources</li> <li>Explicit config file - Via <code>--config</code> or <code>BRAINSMITH_CONFIG</code></li> <li>User config - <code>~/.brainsmith/config.yaml</code></li> <li>Project config - <code>.brainsmith/config.yaml</code> (in repo)</li> <li>Built-in defaults - Hardcoded fallbacks</li> </ol>"},{"location":"getting-started/configuration/#configuration-file-format","title":"Configuration File Format","text":"<pre><code># Example: ~/.brainsmith/config.yaml\n\n# Xilinx tool paths\nxilinx_path: /opt/Xilinx/Vivado/2024.2\nxilinx_version: 2024.2\n\n# Build directory\nbuild_dir: /tmp/finn_dev_${USER}\n\n# Optional: Override specific settings\nlog_level: INFO\nparallel_builds: 4\n</code></pre>"},{"location":"getting-started/configuration/#key-configuration-options","title":"Key Configuration Options","text":""},{"location":"getting-started/configuration/#xilinx-tools","title":"Xilinx Tools","text":"<pre><code>xilinx_path: /opt/Xilinx/Vivado/2024.2\nxilinx_version: 2024.2\n</code></pre> <p>Required for synthesis and implementation. The path should point to your Vivado installation directory.</p>"},{"location":"getting-started/configuration/#build-directory","title":"Build Directory","text":"<pre><code>build_dir: /tmp/finn_dev_${USER}\n</code></pre> <p>Where temporary build artifacts are stored. Can use environment variables like <code>${USER}</code>.</p>"},{"location":"getting-started/configuration/#logging","title":"Logging","text":"<pre><code>log_level: DEBUG  # DEBUG, INFO, WARNING, ERROR\n</code></pre> <p>Controls verbosity of logging output.</p>"},{"location":"getting-started/configuration/#cli-configuration-commands","title":"CLI Configuration Commands","text":""},{"location":"getting-started/configuration/#initialize-config","title":"Initialize Config","text":"<pre><code># Create default config file\nbrainsmith config init\n</code></pre> <p>Creates <code>~/.brainsmith/config.yaml</code> with default values.</p>"},{"location":"getting-started/configuration/#view-current-config","title":"View Current Config","text":"<pre><code># Show effective configuration (all sources merged)\nbrainsmith config show\n</code></pre>"},{"location":"getting-started/configuration/#export-environment","title":"Export Environment","text":"<pre><code># Export Xilinx environment variables\neval $(brainsmith config export)\n</code></pre> <p>This sets up <code>XILINX_ROOT</code>, <code>XILINX_VERSION</code>, and other environment variables needed by FINN and Vivado.</p>"},{"location":"getting-started/configuration/#cli-overrides","title":"CLI Overrides","text":"<p>Override configuration via command-line:</p> <pre><code># Override build directory\nsmith dse model.onnx blueprint.yaml --build-dir /custom/path\n\n# Use specific config file\nsmith --config /path/to/config.yaml dse model.onnx blueprint.yaml\n\n# Enable debug mode\nsmith --debug dse model.onnx blueprint.yaml\n</code></pre>"},{"location":"getting-started/configuration/#environment-variables","title":"Environment Variables","text":"<p>You can also use environment variables:</p> <pre><code>export BRAINSMITH_CONFIG=/path/to/config.yaml\nexport BRAINSMITH_BUILD_DIR=/custom/build/dir\n</code></pre>"},{"location":"getting-started/configuration/#project-specific-configuration","title":"Project-Specific Configuration","text":"<p>For project-specific settings, create <code>.brainsmith/config.yaml</code> in your project root:</p> <pre><code># .brainsmith/config.yaml (in git repo)\nbuild_dir: ./build\nlog_level: DEBUG\n</code></pre> <p>This allows different projects to have different defaults while keeping user settings separate.</p>"},{"location":"getting-started/configuration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/configuration/#config-not-found","title":"Config not found","text":"<pre><code># Verify config file exists\nls -la ~/.brainsmith/config.yaml\n\n# Re-initialize if needed\nbrainsmith config init\n</code></pre>"},{"location":"getting-started/configuration/#wrong-xilinx-path","title":"Wrong Xilinx path","text":"<pre><code># Check current setting\nbrainsmith config show | grep xilinx\n\n# Update in config file\nnano ~/.brainsmith/config.yaml\n</code></pre>"},{"location":"getting-started/configuration/#environment-not-set","title":"Environment not set","text":"<pre><code># Make sure to run export command\neval $(brainsmith config export)\n\n# Verify environment\necho $XILINX_ROOT\n</code></pre>"},{"location":"getting-started/configuration/#next-steps","title":"Next Steps","text":"<ul> <li>CLI Reference - Full command documentation</li> <li>Architecture - How configuration flows through the system</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This guide will help you set up Brainsmith for development.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Ubuntu 22.04+</li> <li>Python 3.10+</li> <li>Vivado Design Suite 2024.2</li> <li>Poetry</li> </ul>"},{"location":"getting-started/installation/#poetry-environment-setup","title":"Poetry Environment Setup","text":""},{"location":"getting-started/installation/#automated-setup","title":"Automated Setup","text":"<pre><code># Run automated setup script\n./setup-venv.sh\n\n# Activate virtual environment\nsource .venv/bin/activate\n</code></pre>"},{"location":"getting-started/installation/#manual-setup","title":"Manual Setup","text":"<p>If you prefer manual setup:</p> <pre><code># Install Poetry (if not already installed)\ncurl -sSL https://install.python-poetry.org | python3 -\n\n# Install dependencies\npoetry install\n\n# Activate virtual environment\npoetry shell\n</code></pre>"},{"location":"getting-started/installation/#configuration","title":"Configuration","text":""},{"location":"getting-started/installation/#initialize-configuration","title":"Initialize Configuration","text":"<pre><code># Create config file\nbrainsmith config init\n</code></pre> <p>This creates <code>~/.brainsmith/config.yaml</code>.</p>"},{"location":"getting-started/installation/#edit-configuration","title":"Edit Configuration","text":"<p>Edit <code>~/.brainsmith/config.yaml</code> to set your Xilinx paths:</p> <pre><code>xilinx_path: /opt/Xilinx/Vivado/2024.2\nxilinx_version: 2024.2\nbuild_dir: /tmp/finn_dev_${USER}\n</code></pre>"},{"location":"getting-started/installation/#verify-configuration","title":"Verify Configuration","text":"<pre><code># View current configuration\nbrainsmith config show\n\n# Export environment variables\neval $(brainsmith config export)\n</code></pre>"},{"location":"getting-started/installation/#validate-installation","title":"Validate Installation","text":"<p>Run the quick validation test:</p> <pre><code>./examples/bert/quicktest.sh\n</code></pre> <p>This runs a minimal BERT example (single layer) to verify everything works.</p>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start Guide - Run your first DSE</li> <li>Configuration Guide - Learn about configuration options</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>Get started with Brainsmith in 5 minutes.</p>"},{"location":"getting-started/quickstart/#prerequisites","title":"Prerequisites","text":"<p>Make sure you've completed the installation.</p>"},{"location":"getting-started/quickstart/#run-your-first-dse","title":"Run Your First DSE","text":""},{"location":"getting-started/quickstart/#1-prepare-your-model","title":"1. Prepare Your Model","text":"<p>For this quickstart, we'll use the included BERT example:</p> <pre><code>cd examples/bert\n</code></pre>"},{"location":"getting-started/quickstart/#2-run-the-quick-test","title":"2. Run the Quick Test","text":"<pre><code>./quicktest.sh\n</code></pre> <p>This will:</p> <ol> <li>Generate a folding configuration for minimal resources</li> <li>Build a single-layer BERT accelerator</li> <li>Run RTL simulation to verify correctness</li> </ol> <p>Build Time</p> <p>The quicktest takes approximately 30-60 minutes, depending on your system.</p>"},{"location":"getting-started/quickstart/#3-explore-results","title":"3. Explore Results","text":"<p>Results are saved in <code>examples/bert/quicktest/</code>:</p> <pre><code>quicktest/\n\u251c\u2500\u2500 model.onnx              # Quantized ONNX model\n\u251c\u2500\u2500 final_output/           # Generated RTL and reports\n\u2502   \u251c\u2500\u2500 stitched_ip/       # Synthesizable RTL\n\u2502   \u2514\u2500\u2500 report/            # Performance estimates\n\u2514\u2500\u2500 build_dataflow.log     # Build log\n</code></pre>"},{"location":"getting-started/quickstart/#understanding-the-output","title":"Understanding the Output","text":""},{"location":"getting-started/quickstart/#performance-report","title":"Performance Report","text":"<p>Check <code>final_output/report/estimate_reports.json</code>:</p> <pre><code>{\n  \"throughput\": \"1234.5 fps\",\n  \"latency\": \"0.81 ms\",\n  \"resources\": {\n    \"LUT\": 12345,\n    \"FF\": 23456,\n    \"BRAM\": 34,\n    \"DSP\": 56\n  }\n}\n</code></pre>"},{"location":"getting-started/quickstart/#rtl-output","title":"RTL Output","text":"<p>The generated RTL is in <code>final_output/stitched_ip/</code>:</p> <ul> <li><code>finn_design_wrapper.v</code> - Top-level module</li> <li><code>*.v</code> - Individual kernel implementations</li> </ul>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":""},{"location":"getting-started/quickstart/#customize-the-design","title":"Customize the Design","text":"<p>Edit the blueprint to explore different configurations:</p> <pre><code># bert_quicktest.yaml\ndesign_space:\n  kernels:\n    - name: MatMul\n      backends:\n        - matmul_rtl  # Try different backends\n  steps:\n    - step_target_fps_parallelization:\n        target_fps: 100  # Adjust target performance\n</code></pre>"},{"location":"getting-started/quickstart/#run-full-dse","title":"Run Full DSE","text":"<pre><code>smith model.onnx blueprint.yaml --output-dir ./results\n</code></pre>"},{"location":"getting-started/quickstart/#learn-more","title":"Learn More","text":"<ul> <li>Architecture Overview - Understand how Brainsmith works</li> <li>CLI Reference - Explore CLI commands</li> </ul>"}]}