# AutoHWCustomOp Enhanced Implementation Plan: Automatic Tensor Formatting

## Executive Summary

This plan implements the breakthrough discovery that **DataflowModeling mathematics can automatically generate hardware tensor layouts**, eliminating the need for manual `get_hw_compatible_*_tensor` implementations while preserving all performance optimizations.

**Core Innovation**: Replace 200+ lines of manual tensor formatting per operation with mathematical generation from dataflow interface relationships.

## Mathematical Foundation

### The Revolutionary Equivalence

```python
# Manual MVAU approach (current)
def get_hw_compatible_weight_tensor(self, orig_weight_matrix):
    mw, mh = self.get_nodeattr("MW"), self.get_nodeattr("MH")
    pe, simd = self.get_nodeattr("PE"), self.get_nodeattr("SIMD")
    wmem = (mw * mh) // (pe * simd)  # Manual calculation
    # + 190 lines of manual reshaping...

# DataflowModeling automatic approach (target)
weight_interface = DataflowInterface(
    tensor_dims=[mw, mh],     # Level 1: Original tensor
    block_dims=[simd, pe],    # Level 2: Parallelism chunks  
    stream_dims=[simd, pe],   # Level 3: Elements per cycle
)
# wmem = num_blocks[0] * num_blocks[1] = (mw//simd) * (mh//pe) âœ“ IDENTICAL
```

### Mathematical Proof of Correctness

**Key Insight**: `block_dims` encode exactly what manual functions compute:
- `num_blocks = [tensor_dims[i] // block_dims[i]]`
- `wmem_auto = num_blocks[0] * num_blocks[1] = wmem_manual` âœ“
- Hardware layout: `[1, wPar, wmem, iPar]` generated from interface mathematics

## Implementation Architecture

### Phase 1: Core DataflowTensorFormatter

```python
# Location: brainsmith/dataflow/core/tensor_formatter.py
class DataflowTensorFormatter:
    """Universal tensor formatting using dataflow interface mathematics"""
    
    def format_tensor_for_hardware(self, 
                                 tensor: np.ndarray,
                                 interface: DataflowInterface,
                                 operation_hints: Dict[str, Any]) -> np.ndarray:
        """
        Replace manual tensor formatting with mathematical generation.
        
        Args:
            tensor: Original software tensor
            interface: DataflowInterface with mathematical relationships
            operation_hints: Operation-specific formatting flags
            
        Returns:
            Hardware-optimized tensor layout
        """
        
        # Stage 1: Extract mathematical parameters
        tensor_dims = interface.tensor_dims
        block_dims = interface.block_dims
        stream_dims = interface.stream_dims
        num_blocks = interface.get_num_blocks()
        
        # Stage 2: Validate mathematical constraints
        self._validate_tensor_interface_compatibility(tensor, interface)
        
        # Stage 3: Datatype conversion (from interface constraints)
        tensor = self._convert_datatype(tensor, interface.dtype)
        
        # Stage 4: Operation-specific preprocessing (minimal)
        tensor = self._apply_operation_preprocessing(tensor, operation_hints)
        
        # Stage 5: PE distribution (universal pattern)
        tensor = self._distribute_across_pes(tensor, stream_dims)
        
        # Stage 6: Final hardware layout (from interface mathematics)
        tensor = self._generate_hardware_layout(tensor, interface)
        
        # Stage 7: Memory optimization (operation flags)
        tensor = self._apply_memory_optimizations(tensor, operation_hints)
        
        return tensor
    
    def _validate_tensor_interface_compatibility(self, tensor, interface):
        """Ensure tensor dimensions match interface mathematical constraints"""
        expected_shape = interface.tensor_dims
        if list(tensor.shape) != expected_shape:
            raise ValueError(f"Tensor shape {tensor.shape} != interface {expected_shape}")
        
        # Validate parallelism constraints
        for i, (t_dim, b_dim) in enumerate(zip(interface.tensor_dims, interface.block_dims)):
            if t_dim % b_dim != 0:
                raise ValueError(f"tensor_dims[{i}]={t_dim} not divisible by block_dims[{i}]={b_dim}")
    
    def _apply_operation_preprocessing(self, tensor, hints):
        """Apply minimal operation-specific transformations"""
        op_type = hints.get("operation_type", "generic")
        
        if op_type == "matrix_multiplication":
            return tensor.T  # MVAU transpose
        elif op_type == "convolution":
            # VVAU spatial flattening
            channels, height, width = tensor.shape[-3:]
            return tensor.reshape(*tensor.shape[:-3], channels, height * width)
        
        return tensor  # No preprocessing for most operations
    
    def _distribute_across_pes(self, tensor, stream_dims):
        """Universal PE distribution using dataflow mathematics"""
        wPar = stream_dims[1] if len(stream_dims) > 1 else 1
        
        if wPar > 1:
            # Standard PE interleaving (universal pattern)
            return interleave_matrix_outer_dim_from_partitions(tensor, wPar)
        
        return tensor
    
    def _generate_hardware_layout(self, tensor, interface):
        """Generate final hardware layout from interface mathematics"""
        num_blocks = interface.get_num_blocks()
        stream_dims = interface.stream_dims
        
        # Calculate derived quantities from interface mathematics
        if interface.interface_type == InterfaceType.WEIGHT:
            wmem = num_blocks[0] * num_blocks[1]  # Memory depth
            iPar, wPar = stream_dims[0], stream_dims[1]
            final_shape = [1, wPar, wmem, iPar]  # Standard weight layout
        elif interface.interface_type == InterfaceType.OUTPUT:
            tmem = num_blocks[0]  # Threshold memory depth
            pe = stream_dims[0]
            n_steps = interface.tensor_dims[1] if len(interface.tensor_dims) > 1 else 1
            final_shape = [1, pe, tmem, n_steps]  # Standard threshold layout
        else:
            # Generic layout for other interface types
            final_shape = [1] + stream_dims + [num_blocks[0]]
        
        return tensor.reshape(final_shape)
    
    def _apply_memory_optimizations(self, tensor, hints):
        """Apply operation-specific memory optimizations"""
        if hints.get("needs_simd_flip", False):
            tensor = np.flip(tensor, axis=-1)  # SIMD memory optimization
        
        return tensor
```

### Phase 2: Enhanced AutoHWCustomOp Integration

```python
# Location: brainsmith/dataflow/core/auto_hw_custom_op.py (enhancement)

class AutoHWCustomOp(HWCustomOp):
    """Enhanced with automatic tensor formatting capabilities"""
    
    def __init__(self, onnx_node, **kwargs):
        super().__init__(onnx_node, **kwargs)
        self._dataflow_model = self._build_dataflow_model_from_node()
        self._current_parallelism = self._initialize_minimum_parallelism()
        
        # Initialize tensor formatter
        self._tensor_formatter = DataflowTensorFormatter()
    
    def get_hw_compatible_weight_tensor(self, orig_weight_matrix):
        """Automatic weight tensor formatting using dataflow mathematics"""
        
        # Find weight interface from dataflow model
        weight_interfaces = [i for i in self._dataflow_model.interfaces 
                           if i.interface_type == InterfaceType.WEIGHT]
        
        if not weight_interfaces:
            raise ValueError(f"No weight interfaces found in {self.__class__.__name__}")
        
        weight_interface = weight_interfaces[0]  # Primary weight interface
        
        # Update interface with current parallelism settings
        self._update_interface_parallelism(weight_interface, "weight")
        
        # Generate operation hints from interface metadata
        operation_hints = self._extract_operation_hints(weight_interface)
        
        # Use dataflow mathematics to format tensor automatically
        return self._tensor_formatter.format_tensor_for_hardware(
            orig_weight_matrix, weight_interface, operation_hints
        )
    
    def get_hw_compatible_threshold_tensor(self, orig_thres_matrix):
        """Automatic threshold tensor formatting using output interface mathematics"""
        
        # Find output interface (thresholds derive from output characteristics)
        output_interfaces = [i for i in self._dataflow_model.interfaces
                           if i.interface_type == InterfaceType.OUTPUT]
        
        if not output_interfaces:
            raise ValueError(f"No output interfaces found for threshold formatting")
        
        output_interface = output_interfaces[0]
        
        # Create threshold interface based on output interface mathematics
        threshold_interface = self._create_threshold_interface_from_output(output_interface)
        
        # Update with current parallelism
        self._update_interface_parallelism(threshold_interface, "threshold")
        
        # Generate operation hints
        operation_hints = self._extract_operation_hints(threshold_interface)
        operation_hints["operation_type"] = "threshold"
        
        # Format using dataflow mathematics
        return self._tensor_formatter.format_tensor_for_hardware(
            orig_thres_matrix, threshold_interface, operation_hints
        )
    
    def _update_interface_parallelism(self, interface: DataflowInterface, role: str):
        """Update interface stream_dims with current parallelism settings"""
        
        # Get current parallelism from legacy attributes or dataflow model
        if role == "weight":
            iPar = self.get_legacy_attr().get("SIMD", 1)
            wPar = self.get_legacy_attr().get("PE", 1)
        elif role == "threshold":
            iPar = 1  # Thresholds typically don't have input parallelism
            wPar = self.get_legacy_attr().get("PE", 1)
        else:
            iPar = wPar = 1
        
        # Update interface with current parallelism
        interface.update_stream_dims([iPar, wPar])
    
    def _extract_operation_hints(self, interface: DataflowInterface) -> Dict[str, Any]:
        """Extract operation-specific formatting hints from interface metadata"""
        
        hints = {}
        
        # Determine operation type from interface characteristics
        if interface.interface_type == InterfaceType.WEIGHT:
            if len(interface.tensor_dims) == 2:
                hints["operation_type"] = "matrix_multiplication"
            elif len(interface.tensor_dims) >= 3:
                hints["operation_type"] = "convolution"
            else:
                hints["operation_type"] = "generic"
        
        # Extract optimization flags from interface metadata
        if hasattr(interface, 'metadata') and interface.metadata:
            hints.update(interface.metadata.get('optimization_flags', {}))
        
        # Default optimization flags
        hints.setdefault("needs_simd_flip", True)  # Most operations benefit from SIMD flip
        
        return hints
    
    def _create_threshold_interface_from_output(self, output_interface: DataflowInterface) -> DataflowInterface:
        """Create threshold interface based on output interface mathematics"""
        
        # Threshold dimensions derived from output characteristics
        num_channels = output_interface.tensor_dims[0]
        n_thres_steps = self._get_threshold_steps()  # From node attributes
        
        return DataflowInterface(
            name="threshold",
            interface_type=InterfaceType.OUTPUT,  # Thresholds are output-related
            tensor_dims=[num_channels, n_thres_steps],
            block_dims=[output_interface.block_dims[0], 1],  # PE parallelism, no threshold parallelism
            stream_dims=[output_interface.stream_dims[0], 1],
            dtype=self._get_threshold_datatype()
        )
    
    def _get_threshold_steps(self) -> int:
        """Get number of threshold steps from node attributes"""
        # This would be extracted from specific operation requirements
        return getattr(self, '_threshold_steps', 1)
    
    def _get_threshold_datatype(self) -> DataType:
        """Get threshold datatype from node attributes"""
        # Extract from node attributes or use default
        return getattr(self, '_threshold_datatype', DataType["INT8"])
    
    # Enhanced memory calculation methods using dataflow mathematics
    def calc_wmem(self):
        """Auto-calculated WMEM from dataflow interface mathematics"""
        weight_interfaces = [i for i in self._dataflow_model.interfaces 
                           if i.interface_type == InterfaceType.WEIGHT]
        
        if not weight_interfaces:
            return 0
        
        weight_interface = weight_interfaces[0]
        self._update_interface_parallelism(weight_interface, "weight")
        
        # Mathematical calculation from interface relationships
        num_blocks = weight_interface.get_num_blocks()
        return num_blocks[0] * num_blocks[1]  # Automatic WMEM calculation
    
    def calc_tmem(self):
        """Auto-calculated TMEM from dataflow interface mathematics"""
        output_interfaces = [i for i in self._dataflow_model.interfaces
                           if i.interface_type == InterfaceType.OUTPUT]
        
        if not output_interfaces:
            return 0
        
        output_interface = output_interfaces[0]
        self._update_interface_parallelism(output_interface, "threshold")
        
        # Mathematical calculation from interface relationships
        num_blocks = output_interface.get_num_blocks()
        return num_blocks[0]  # Automatic TMEM calculation
```

### Phase 3: Template System Integration

```jinja2
{# Location: brainsmith/tools/hw_kernel_gen/templates/hw_custom_op_phase2.py.j2 #}
{# Enhanced template with automatic tensor formatting #}

class {{ class_name }}(AutoHWCustomOp):
    """Auto-generated HWCustomOp with dataflow-driven tensor formatting"""
    
    @staticmethod  
    def get_interface_metadata() -> List[InterfaceMetadata]:
        """Interface metadata with complete dataflow mathematical relationships"""
        return {{ interface_metadata_with_enhanced_dataflow | repr }}
    
    {% if has_weight_interfaces %}
    def get_hw_compatible_weight_tensor(self, orig_weight_matrix):
        """Automatic weight tensor formatting via dataflow mathematics"""
        # Uses enhanced AutoHWCustomOp automatic formatting
        return super().get_hw_compatible_weight_tensor(orig_weight_matrix)
    {% endif %}
    
    {% if has_threshold_interfaces %}  
    def get_hw_compatible_threshold_tensor(self, orig_thres_matrix):
        """Automatic threshold tensor formatting via dataflow mathematics"""
        # Uses enhanced AutoHWCustomOp automatic formatting
        return super().get_hw_compatible_threshold_tensor(orig_thres_matrix)
    {% endif %}
    
    {% if has_memory_calculations %}
    def calc_wmem(self):
        """Auto-calculated from dataflow interface mathematics"""
        # Mathematical calculation eliminates manual implementation
        return super().calc_wmem()
    
    def calc_tmem(self):
        """Auto-calculated from dataflow interface mathematics"""  
        # Mathematical calculation eliminates manual implementation
        return super().calc_tmem()
    {% endif %}
    
    # Operation-specific attributes derived automatically
    {% for attr_name, attr_value in derived_attributes.items() %}
    def get_{{ attr_name }}(self):
        """Auto-derived from dataflow interface mathematics"""
        return {{ attr_value }}
    {% endfor %}
```

### Phase 4: RTL Parser Enhancement

```python
# Location: brainsmith/tools/hw_kernel_gen/generators/interface_analyzer.py (enhancement)

class EnhancedInterfaceAnalyzer:
    """Enhanced to extract operation-specific formatting hints"""
    
    def analyze_rtl_for_dataflow_interfaces(self, rtl_result) -> List[InterfaceMetadata]:
        """Extract interfaces with operation-specific optimization hints"""
        
        interfaces = []
        
        for interface_info in rtl_result.interfaces:
            # Standard interface creation
            interface_metadata = self._create_base_interface_metadata(interface_info)
            
            # Extract operation-specific hints from RTL analysis
            optimization_hints = self._extract_optimization_hints(interface_info, rtl_result)
            interface_metadata.metadata = {
                'optimization_flags': optimization_hints,
                'operation_type': self._infer_operation_type(interface_info, rtl_result),
                'memory_pattern': self._analyze_memory_access_pattern(interface_info)
            }
            
            interfaces.append(interface_metadata)
        
        return interfaces
    
    def _extract_optimization_hints(self, interface_info, rtl_result) -> Dict[str, Any]:
        """Extract formatting optimization hints from RTL pragmas and analysis"""
        
        hints = {}
        
        # Check for SIMD optimization pragmas
        if self._has_simd_optimization_pragma(interface_info):
            hints["needs_simd_flip"] = True
        
        # Check for transpose requirements
        if self._requires_matrix_transpose(interface_info, rtl_result):
            hints["needs_transpose"] = True
        
        # Check for PE distribution requirements
        if self._requires_pe_distribution(interface_info):
            hints["needs_pe_interleaving"] = True
        
        return hints
    
    def _infer_operation_type(self, interface_info, rtl_result) -> str:
        """Infer operation type from RTL characteristics"""
        
        # Matrix multiplication: 2D weight interfaces with MAC patterns
        if (len(interface_info.tensor_dims) == 2 and 
            interface_info.interface_type == InterfaceType.WEIGHT and
            self._has_mac_pattern(rtl_result)):
            return "matrix_multiplication"
        
        # Convolution: 3D+ weight interfaces with spatial patterns
        if (len(interface_info.tensor_dims) >= 3 and
            interface_info.interface_type == InterfaceType.WEIGHT and
            self._has_spatial_pattern(rtl_result)):
            return "convolution"
        
        # Threshold operations: output interfaces with comparison patterns
        if (interface_info.interface_type == InterfaceType.OUTPUT and
            self._has_threshold_pattern(rtl_result)):
            return "threshold"
        
        return "generic"
```

## Implementation Timeline

### Phase 1: Core Infrastructure (Week 1-2)
1. **DataflowTensorFormatter Implementation**
   - Core tensor formatting algorithms
   - Mathematical validation framework
   - Operation hint processing system
   
2. **Unit Testing Framework**
   - Test mathematical equivalence with manual functions
   - Validate formatting for MVAU/VVAU/Thresholding operations
   - Performance regression testing

### Phase 2: AutoHWCustomOp Enhancement (Week 3-4)
1. **Base Class Enhancement**
   - Integrate DataflowTensorFormatter
   - Add automatic tensor formatting methods
   - Implement parallelism management
   
2. **Memory Calculation Automation**
   - Automatic WMEM/TMEM calculation
   - Interface-based memory planning
   - Legacy attribute mapping

### Phase 3: Template and Generation (Week 5-6)
1. **Template System Update**
   - Enhanced template generation
   - Operation hint extraction
   - Automatic method generation
   
2. **RTL Parser Enhancement**
   - Operation type inference
   - Optimization hint extraction
   - Metadata enhancement

### Phase 4: Validation and Integration (Week 7-8)
1. **Comprehensive Testing**
   - Test against all legacy HWCustomOp implementations
   - Performance validation
   - Correctness verification
   
2. **Documentation and Examples**
   - Usage documentation
   - Migration guide
   - Performance analysis

## Validation Strategy

### Mathematical Correctness Validation

```python
def test_mathematical_equivalence():
    """Validate that automatic formatting produces identical results to manual"""
    
    # Test MVAU equivalence
    manual_mvau = MVAUHWCustomOp(test_node)
    auto_mvau = AutoMVAUHWCustomOp(test_node)  # Generated version
    
    test_weight = np.random.randn(256, 768)
    
    manual_result = manual_mvau.get_hw_compatible_weight_tensor(test_weight)
    auto_result = auto_mvau.get_hw_compatible_weight_tensor(test_weight)
    
    assert np.array_equal(manual_result, auto_result), "MVAU formatting must be identical"
    assert manual_mvau.calc_wmem() == auto_mvau.calc_wmem(), "WMEM calculation must match"
```

### Performance Validation

```python
def test_performance_preservation():
    """Validate that automatic formatting preserves hardware performance"""
    
    # Generate hardware with both manual and automatic formatting
    manual_implementation = generate_manual_implementation()
    auto_implementation = generate_automatic_implementation()
    
    # Simulate hardware execution
    manual_cycles = simulate_hardware_execution(manual_implementation)
    auto_cycles = simulate_hardware_execution(auto_implementation)
    
    assert auto_cycles == manual_cycles, "Performance must be preserved"
```

### Regression Testing Framework

```python
def test_all_legacy_operations():
    """Comprehensive regression testing against all legacy implementations"""
    
    legacy_operations = [
        "MVAU", "VVAU", "Thresholding", "AddStreams", "Streamingdatawidthconverter"
    ]
    
    for op_name in legacy_operations:
        with subtest(operation=op_name):
            validate_operation_equivalence(op_name)
```

## Risk Mitigation

### Technical Risks

1. **Mathematical Errors**: Comprehensive validation against golden references
2. **Performance Degradation**: Extensive performance testing and optimization
3. **Compatibility Issues**: Gradual rollout with fallback to manual implementations
4. **Integration Complexity**: Modular design allows incremental integration

### Mitigation Strategies

1. **Gradual Rollout**: Enable automatic formatting operation-by-operation
2. **Fallback Mechanisms**: Maintain manual implementations during transition
3. **Extensive Testing**: Test against all existing HWCustomOp implementations
4. **Performance Monitoring**: Continuous performance regression testing

## Success Metrics

### Code Reduction Metrics
- **Manual Code Elimination**: Remove 200+ lines per operation
- **Template Simplification**: Reduce template complexity by 60%
- **Maintenance Burden**: Eliminate operation-specific tensor formatting bugs

### Performance Metrics
- **Zero Performance Degradation**: Identical cycle counts to manual implementations
- **Memory Efficiency**: Same or better memory utilization
- **Resource Usage**: Same or better FPGA resource requirements

### Quality Metrics
- **Bug Elimination**: Zero tensor formatting bugs in generated operations
- **Correctness Guarantee**: Mathematical validation prevents invalid configurations
- **Maintainability**: Single system for all current and future operations

## Conclusion

This implementation plan represents a **fundamental breakthrough** in automatic hardware generation. By leveraging the mathematical relationships in DataflowModeling, we can:

1. **Eliminate Manual Implementation**: Replace error-prone manual code with mathematical generation
2. **Preserve Performance**: Maintain all hardware optimizations through mathematical equivalence
3. **Enable Universal Support**: Single system works for all operation types
4. **Guarantee Correctness**: Mathematical constraints prevent invalid configurations

The result is a **revolutionary improvement** in AutoHWCustomOp capabilities, achieving full parity with legacy HWCustomOp implementations while dramatically simplifying the generation process.

**The DataflowModeling system IS the complete solution to tensor formatting generalization!** ðŸš€